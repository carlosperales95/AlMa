PCA on evidences
----------------
----------------

Sentences in cluster n0:
.....................................
	He propose to employ cache-based language and translation model in a phrase-based SMT system for domain909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , page 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .

	Specifically , two type of semantic role feature be propose in this paper : a semantic role reorder feature design to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 page 716724 , Beijing , August 2010 ture design to penalize miss semantic role in the target sentence .

	1com430 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , page 430439 , MIT , Massachusetts , USA , 9-11 October 2010 .

	The key property of the bLSA model be that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , page 520527 , Prague , Czech Republic , June 2007 .

	Results show that our approach significantly reduce the word perplexity on the target language in both case use ASR hypothesis and manual transcript .

	Which approach pro 1 A CWS competition organize by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical Machine Translation , page 216223 , Columbus , Ohio , USA , June 2008 .

	In our approach we introduce equivalence class in order to ignore information not relevant to the translation process .

	The Japanese NER system propose in ( Nakano and Hirai , 2004 which achieve the high F-measure among conventional system , introduce the bunsetsu1 feature in order to consider wide context , but consider only adjacent bunsetsus .

	Tightly integrate joint bilingual name tag into MT training by coordinate tagged604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , page 604614 , Sofia , Bulgaria , August 4-9 2013 .

	For English-to-Arabic translation , we achieve a +1.04 BLEU average improvement by tile our model on top of a large LM .146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , page 146155 , Jeju , Republic of Korea , 8-14 July 2012 .

	The speed improvement discuss in this paper make multiple randomized search per sentence feasible , lead to a faster and good decoder for machine translation with IBM Model 4.6 .

	We expect that a method capable of identify the correct sense of the feature and translate them accordingly could contribute to produce cleaner vector and to extract high quality lexicons.In this paper , we show how source vector can be translate into the target language by a cross-lingual Word Sense Disambiguation ( WSD ) method which exploit the output of data-driven Word Sense Induction ( WSI Apidianaki , 2009 and demonstrate how feature disambiguation enhance the quality of the translation extract from the comparable corpus .

	In Section 5 , we report and discuss the obtained result before conclude and present some direction for future work .1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora , page 110 , Sofia , Bulgaria , August 8 , 2013 .

	On NIST08 Chinese-English translation task , we obtain an improvement of 0.48 BLEU from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , page 13931398 , Seattle , Washington , USA , 18-21 October 2013 .

	While a few study have investigate pattern match approach to compare source and target context ( Fung , 1995 ; Diab and Finch , 2000 ; Yu and Tsujii , 2009 most variant make use of a bilingual lexicon in order to translate the word of the context of a term ( often call seed word Dejean et al 2005 ) instead use a bilingual thesaurus for translate these.Another distinction between approach lie in the way the context be define .

	Bayesian inference , the approach in this paper , have recently be apply to several unsupervised learn problem in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other task in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learn phrase alignment directly ( DeNero et al 2008 Word alignment learn problem be address jointly with segmentation learn in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two work place nonparametric prior ( also know as cache model ) on the parameter and utilize Gibbs sampling .

	An O ( m6 ) greedy search algorithm be develop ( Germann et al 2003 ) whose complexity be re duced far to O ( m2 Germann , 2003 In this paper , we propose an algorithmic framework for solve the decoding problem and show that several efficient decode algorithm can be derive from the technique develop in the framework .

	We start with a mathematical formulation of the decoding problem ( Section 2 We then develop the alternate search paradigm and use it to develop several decode algorithm ( Section 3 Next , we demonstrate the practical utility of our algorithm with the help of result from our initial experiment ( Section 5 ) .

	Decoding be know to belong to a class of computational problem popularly know as NPhard problem ( Knight , 1999 NP-hard problem be know to be computationally hard and have elude polynomial time algorithm ( Garey and Johnson , 1979 The first algorithm for the decoding problem be base on what be know among the speech recognition community as stack-based search ( Jelinek , 1969 The original IBM solution to the decoding problem employ a restrict stack-based search ( Berger et al 1996 This idea be far explore by Wang and Waibel ( Wang and Waibel , 1997 ) who develop a faster stack-based search algorithm .




Sentences in cluster n1:
.....................................
	In our case , by build a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derive subcorpora with probabilistic membership .

	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for good translation .

	We show that incorporate lexical weighting feature condition on soft domain membership directly into our model be an effective strategy for dynamically bias SMT towards relevant translation , as evidence by significant performance gain .

	Depending on the model use to select subcorpora , we can bias our translation toward any arbitrary distinction .

	In this work , we consider the underlying latent topic of the document ( Blei et al 2003 Topic modeling have receive some use in SMT , for instance Bilingual LSA adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which use a bilingual topic model for learn alignment .

	We believe that by perform a rescoring on translation word graph we will obtain a more significant improvement in translation quality .

	Section 3 present our cache-based approach to documentlevel SMT .

	Evaluation show the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .

	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical translation model , take IBM Model 1 as a baseline model , be present under the bilingual topic admixture model formalism .

	Section 4 report experimental result and Section 5 conclude our work.English PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graph .

	Similarly , we can mine Chinese news article to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .

	To highlight the advantage of our propose approach , we compare our result with commercial machine translator Engkoo3 develop in Microsoft Research Asia and Google Translator4 .

	Our evaluation result empirically validate the accuracy of our algorithm over real-life datasets , and show the effectiveness on our propose perspective

	Such engine This work be do when the first two author visit Microsoft Research Asia .

	While high quality entity translation be essential in cross-lingual information access and trans lation , it be non-trivial to achieve , due to the challenge that entity translation , though typically bear pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing effort to address these challenge can be categorize into transliterationand corpusbased approach .

	We will investigate the incorporation of monolingual document for potentially good bilingual LSA modeling

	In this work , we also propose approach to make use of all the Sighan training data regardless of the specification .

	Chinese word segmentation ( CWS ) be a necessary step in Chinese-English statistical machine translation ( SMT ) and its performance have an impact on the result of SMT .

	We furthermore suggest the use of hierarchical lexicon model .

	We will investigate this in the future .

	We also conduct experiment on IREX NE data and an NE-annotated web corpus and conrmed that structural information improve the performance of NER .

	As a consequence , the performance of NER be improve by use structural information and our approach achieve a high F-measure than exist approach

	This need can be address in part by cross-lingual information access task such as entity link ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lie in the performance of Machine Translation ( MT Traditional MT approach focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content word include critical information , especially name .

	The feasibility of speech-to-speech translation be the focus of research at the beginning because each component be difficult to build and their integration seem more difficult .

	After groundbreaking work for two decade , corpus-based speech and language processing technology have recently enable the achievement of speech-to-speech translation that be usable in the real world .

	fem The car go quickly Figure 1 : Ungrammatical Arabic output of Google Translate for the English input The car go quickly .

	The experience obtain in the Verbmobil project , in particular a large-scale end-to-end evaluation , show that the statistical approach result in significantly low error rate than three compete translation approach : the sentence error rate be 29percent in comparison with 52percent to 62percent for the other translation approach .

	Starting with the Bayes decision rule as in speech recognition , we show how the required probability distribution can be structure into three part : the language model , the alignment model and the lexicon model .

	In this paper , we have analyze the complexity of the greedy decode algorithm originally present in Germann et al 2001 ) and presented improvement that drastically reduce the decoder complexity and speed to practically linear time.Experimental data suggest a good correlation betweenG1 decode anddecoding ( with 10 translation per input word consider , a list of 498 candidate for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profile show be cumulative , so that the top curve reflect the total decoding time .

	We present improvement to a greedy decode algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when apply navely ) to practically linear time1 without sacrifice translation quality .

	Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rate of 7.9 percent and 9.3 respectively , for their decoder .

	Operations not include in the figure consume so little time that their plot can not be discern in the graph .

	Och et al . report word error rate of 68.68 percent for optimal search ( base on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combine dynamic program with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translation obtain by a multi-stack decoder and a greedy hill-climbing algorithm against those produce by an optimal integer program decoder that treat decode as a variant of the traveling-salesman problem ( cf . Knight , 1999 Their overall performance metric be the sentence error rate ( SER For decode with IBM Model 3 , they report SERs of about 57 6-word sentence ) and 76 8-word sentence ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which be the focus of this paper .

	We develop a classifier to distinguish temporalentities and our propose method outperform the state-of-the-art approach by 6.1

	To overcome such problem , we propose a new notion of selective temporality ( call this fea 2.3 Step 3 : Reinforcement We reinforce R0 by leverage R and obtain a converged matrix R use the following model : ture ST to distinguish from T ) to automatically distinguish temporal and atemporal entity .

	We expect the disambiguation to have a beneficial impact on the result give that polysemy be a frequent phenomenon in a general , mixed-domain corpus .

	We have show how cross-lingual WSD can be apply to bilingual lexicon extraction from comparable corpus .

	Section 3 present the data use in our experiment and Section 4 provide detail on the approach and the experimental setup .

	To segment each noun phrase , we use nonparametric Bayesian language model ( Goldwater et al 2009 ; Mochihashi et al 2009 Our approach605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , page 605615 , Edinburgh , Scotland , UK , July 2731 , 2011 .

	We apply non-parametric Bayesian language model to segment each noun phrase in these resource accord to the statistical behavior of its supposed constituent in text .

	In this paper , we propose a new task of Japanese noun phrase segmentation .

	When use to compute semantic similarity of phrase pair , bilingual embeddings improve NIST08 end-to-end machine translation result by just below half a BLEU point .

	We believe that our result on the computational complexity of the task in SMT will result in a good understanding of these task from a theoretical perspective .

	In practice , we be never sure that we have find the Viterbi alignment .

	Our result showthat there can not exist a closed form expression ( whose representation be polynomial in the size of the input ) for P ( f | e ) and the count in the EMiterations for Models 3-5 unless P NP

	around the Viterbi alignment , i.e . in determine j = 1 tfj | eajdj : aj I = 0 d ( j | i , m the goodness of the Viterbi alignment in compar ison to the rest of the alignments.Decoding be an integral component of all SMT system ( Wang , Table 1 : IBM Model 3 Here , n ( | e ) be the fertility model , t ( f | e ) be the lexicon model and d ( j | i , m be the distortion model .

	enomial time solution for any of these hardeproblems ( unless P NP and P # P P our result highlight and justify the need for develop polynomial time approximation for these computation .

	In future work , other parameter which influence the performance will be study , among which the use of a terminological extractor to treat complex term ( Daille and Morin , 2005 more contextual window configuration , and the use of syntactic information in combination with lexical information ( Yu and Tsujii , 2009 It would also be interest to compare the projection-based approach to ( Haghighi et al 2008 ) s generative model for bilingual lexicon acquisition from monolingual corpus .

	While the present work do not investigate all the parameter that could potentially impact result , we believe it constitute the most complete and systematic comparison make so far with variant of the context-based projection approach .

	In the remainder of this paper , we describe the projection-based approach to translation spot in Section 2 and detail the parameter that directly influence its performance .

	Problems with the standard EM estimation of IBM Model 1 be point out by Moore and a number of heuristic change to the estimation procedure , such as smooth the parameter estimate , be show to reduce the alignment error rate , but the effect on translation performance be not report .

	We develop a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignment and show that it outperform EM estimation in term of translation BLEU score across several language pair , data size and domain .

	We show that Bayesian inference outperforms EM in all of the tested language pair , domain and data set size , by up to 2.99 BLEU point .

	Recently , Zhao and Gildea propose fertility extension to IBM Model 1 and HMM , but they do not place any prior on the parameter and their inference method be actually stochastic EM ( also know as Monte Carlo EM a ML technique in which sampling be use to fj be associate with a hidden alignment variable aj whose value range over the word position in the corresponding source sentence .

	Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameter to prevent overfitting .

	We develop a Gibbs sampler for alignment under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which be relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 be use in bootstrapping the parameter setting for EM training of high P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment model , and many state-of-the n n ( te , f ) Ne , f n J art SMT system use Model 1 translation probabil ities as feature in their log-linear model .

	We believe that decode algorithms derive from our framework can be of practical significance .

	We have also show that alternate maximization can be employ tocome up with O ( m2 ) decode algorithm .

	We show that the algorithmic handle provide by our framework can be employ to develop a very fast decoding algorithm which find good quality translation .

	We show that both of these problem be easy to solve and provide efficient solution for them .

	In perhaps the first work on the computational complexity of Decoding , Kevin Knight show that the problem be closely relate to the more famous Traveling Salesman problem ( TSP Independently , Christoph Tillman adapt the Held-Karp dynamic programming algorithm for TSP ( Held and Karp , 1962 ) to Decoding ( Tillman , 2001 The original HeldKarp algorithm for TSP be an exponential time dynamic programming algorithm and Tillmans adaptation to Decoding have a prohibitive com plexity of O ( l3m2 2m ) O ( m5 2m ( where m and l be the length of the source and tar get sentence respectively Tillman and Ney show how to improve the complexity of the Held-Karp algorithm for restrict word re order and give a O ( l3m4 ) O ( m7 ) algo rithm for French-English translation ( Tillman and Ney , 2000 An optimal decoder base on the well-known A heuristic be implement and benchmarked in ( Och et al 2001 Since optimal solution can not be compute for practical problem instance in a reasonable amount of time , much of recent work have focus on good quality suboptimal solution .

	At one end of the spectrum be a provably linear time algorithm for compute a suboptimal solution and at the other end be an exponential time algorithm for computingNIST Scores7 Logscoresmada .




