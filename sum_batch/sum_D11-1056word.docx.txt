A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer.

 Word segmentation is the first step of natural language processing for Japanese, Chinese and Thai because they do not delimit words by white-space. Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99 Kudo et al 2004 This success rests on a high-coverage dictionary. Unknown words, or words not covered by the dictionary, are often misidentified. Historically, researchers have devoted extensive human resources to build and maintain high coverage dictionaries (Yokoi, 1995 Since the orthography of Japanese does not specify a standard for segmentation, researchers define their own criteria before constructing lexical resources. For this reason, it is difficult to exploit existing external resources, such as dictionaries and encyclopedias for human readers, where entry words are not segmented according to the criteria. Among them, encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover. Most of these terms are noun phrases and consist of more than one word (morpheme For example, an encyclopedia has an en try(tsuneyama-jou, Tsuneyama Castle According to our segmentation criteria, it consists of two words(tsuneyama) and(jou However, the morphological analyzer wrongly segments it into(tsune) and(yamashiro) because(tsuneyama) is an unknown word. In this paper, we present the first attempt to utilize encyclopedias for word segmentation. We segment each entry noun phrase into words. To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases. For(tsuneyama-jou its constituent(tsune) appears by itself and as constituents of other nouns phrases such as(peak of Tsuneyama) and(Tsuneyama Station) while(yamashiro) does not. To segment each noun phrase, we use nonparametric Bayesian language models (Goldwater et al 2009; Mochihashi et al 2009 Our approach605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605615, Edinburgh, Scotland, UK, July 2731, 2011. Qc 2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling. The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. Type-based sampling (Liang et al 2010) has the ability to directly escape a local optimum, making inference very efficient. However, type-based sampling is not easily applicable to the bigram model owing to sparsity and its dependence on latent assignments. We propose a hybrid type-based sampling procedure, which combines the Metropolis-Hastings algorithm with Gibbs sampling. We circumvent the sparsity problem by joint sampling of unigram-level type. Also, instead of calculating the probability of every possible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer.

In this paper, we proposed a new task of Japanese noun phrase segmentation. We adopted nonparametric Bayesian language models and proposed hybrid type-based sampling that can efficiently correct segmentation given by the morphological analyzer. Although supervised segmentation is very competitive, we showed that it can be supplemented+very important to identify hiragana words correctly. As hiragana is mainly used to write function words and other basic words, segmentation errors concerning hiragana often bring disastrous effects on applications of morphological analysis. For example,the analyzer over-segments(chiritotechiN) into three shorter words among which the second word(tote) is a particle, and this sequence of words is transformed into a terrible parse tree. Most improvements come from correction of over-segmentation because the initial segmentation by the analyzer shows a tendency of oversegmentation. An example of corrected undersegmentation is contra-alto clarinet. The presence of clarinet, alto and contrabass and others in the main text allowed the model to identext to segment noun phrases in it. The proposed method can be applied to other tasks. For example, in unknown word acquisition (Murawaki and Kurohashi, 2008 noun phrases are often acquired from text as single words. We can now segment them into words in a more sophisticated way. In the future we will assign a POS tag to each word in order to use segmented noun phrases in morphological analysis. We assume that the meaning of constituents in a noun phrase rarely depends on outer context. So it would be helpful to augment them with rich semantic information in advance instead of disambiguating their meaning every time we analyze given text 
