 A key factor of high quality word segmenta tion for Japanese is a high-coverage dictio nary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation cri teria. To supplement a morphological dictio nary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For in ference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a lo cal optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological ana lyzer. 

 In this paper, we proposed a new task of Japanese noun phrase segmentation. We adopted nonparametric Bayesian language models and proposed hybrid type-based sampling that can efficiently correct segmentation given by the morphological analyzer. Although supervised segmentation is very competitive, we showed that it can be supplemented  very important to identify hiragana words correctly. As hiragana is mainly used to write function words and other basic words, segmentation errors concerning hiragana often bring disastrous effects on applications of morphological analysis. For example, the analyzer over-segments “ちりとてちん” (chiri totechiN) into three shorter words among which the second word “とて” (tote) is a particle, and this se quence of words is transformed into a terrible parse tree. Most improvements come from correction of over-segmentation because the initial segmentation by the analyzer shows a tendency of oversegmentation. An example of corrected undersegmentation is “contra-alto clarinet.” The presence of “clarinet,” “alto” and “contrabass” and others in the main text allowed the model to iden text to segment noun phrases in it. The proposed method can be applied to other tasks. For example, in unknown word acquisition (Murawaki and Kurohashi, 2008), noun phrases are often acquired from text as single words. We can now segment them into words in a more sophisticated way. In the future we will assign a POS tag to each word in order to use segmented noun phrases in morphological analysis. We assume that the meaning of constituents in a noun phrase rarely depends on outer context. So it would be helpful to augment them with rich semantic information in advance instead of disambiguating their meaning every time we analyze given text. 
