We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic (when applied navely) to practically linear time1 without sacrificing translation quality. We achieve this by integrating hypothesis evaluation into hypothesis creation, tiling improvements over the translation hypothesis at the end of each search iteration, and by imposing restrictions on the amount of word reordering during decoding.

 Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s (Brown et al., 1990, 1993; Berger et al., 1994, 1996). Based on the conventions established in Brown et al. , these models are commonly referred to as the (IBM) Models 1-5. One of the big challenges in building actual MT systems within this framework is that of decoding: finding the translation candidate that maximizes the translation probabilityfor the given input . Knight has shown the problem to be NP-complete. Due to the complexity of the task, practical MT systems usually do not employ optimal decoders (that is, decoders that are guaranteed to find an optimal solution within the constraints of the framework), but rely on approximative algorithms instead. Empirical evidence suggests that such algorithms can perform resonably well. For example, Berger et al. , attribute only 5% of the translation errors of their Candide system, which uses 1 Technically, the complexity is still. However, the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs. a restricted stack search, to search errors. Using the same evaluation metric (but different evaluation data), Wang and Waibel report search error rates of 7.9% and 9.3%, respectively, for their decoders. Och et al. and Germann et al. both implemented optimal decoders and benchmarked approximative algorithms against them. Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000). Germann et al. compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem (cf. Knight, 1999). Their overall performance metric is the sentence error rate (SER). For decoding with IBM Model 3, they report SERs of about 57% (6-word sentences) and 76% (8-word sentences) for optimal decoding, 58% and 75% for stack decoding, and 60% and 75% for greedy decoding, which is the focus of this paper. All these numbers suggest that approximative algorithms are a feasible choice for practical applications. The purpose of this paper is to describe speed improvements to the greedy decoder mentioned above. While acceptably fast for the kind of evaluation used in Germann et al. , namely sentences of up to 20 words, its speed becomes an issue for more realistic applications. Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 (878 segments; ca. 25k tokens) requires, without any of the improvements described in this paper, over 440 CPU hours, using the simpler, faster algorithm (de scribed below). We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality. In the following, we first describe the underlying IBMinitial string: I do not understandthelogicofthesepeople . pick fertilities: I not not understandthelogicofthesepeople . replace words:Jene pas comprends la logiquede cesgens. reorder: Je ne comprends pas la logique de ces gens . insert spurious words: Je ne comprends pas la logique de ces gens -la` . Figure 1: How the IBM models model the translation process. This is a hypothetical example and not taken from any actual training or decoding logs.model(s) of machine translation (Section) and our hillclimbing algorithm (Section 3). In Section 4, we discuss improvements to the algorithm and its implementation, and the effect of restrictions on word reordering.

In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al. and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding (with 10 translations per input word considered, a list of 498 candidates for INSERT, a maximum swap distance of 2 and a maximum swap segment size of 5). The profiles shown are cumulative, so that the top curve reflects the total decoding time. To put the times for decoding in perspective, the dashed line in the lower plot reflects the total decoding time in decoding. Operations not included in the figures consume so little time that their plots cannot be discerned in the graphs. The times shown are averages of 100 sentences each for length10, 20, , 80.IBM Model 4 scores and the BLEU metric. The speed improvements discussed in this paper make multiple randomized searches per sentence feasible, leading to a faster and better decoder for machine translation with IBM Model 4.6 
