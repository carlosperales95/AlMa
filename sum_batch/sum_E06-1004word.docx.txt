In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community. Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT. Our work aims at providing useful insights into the the computational complexity of those problems. We prove that while IBM Models 1-2 are conceptually and computationally simple, computations involving the higher (and more useful) models are hard.Since it is unlikely that there exists a poly-language 1 (Tillman, 2001), (Wang, 1997), (Germann et al., 2003), (Udupa et al., 2004). The models are independent of the language pair and therefore, can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training. Increasingly, parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English, German-English, Arabic-English, Chinese-English, Hindi-English and other language pairs (Brown et al., 1993), (AlOnaizan et al., 1999), (Udupa, 2004). In SMT, every English sentence e is considered as a translation of a given French sentence f withprobability P r (f |e). Therefore, the problem oftranslating f can be viewed as a problem of findingthe most probable translation of f :e = argmax P r(e|f = argmax P r(f |e)P .enomial time solution for any of these hardeproblems (unless P = NP and P#P = P), our results highlight and justify the need for developing polynomial time approximations for these computations. We also discuss some practical ways of dealing with complexity.

 Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic The probability distributions P r(f |e) and P r are known as translation model and lan guage model respectively. In the classic work on SMT, Brown and his colleagues at IBM introduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models (Brown et al., 1993). An alignment between f = f1 . fm and e = e1 . el is a many-to-one mapping a : {1, . , m}{0, . , l}. Thus, an alignment a between f and e associates the french word fj to the English word 2 . The number of words of f mapped to ei by translation (Brown et al., 1993), (Al-Onaizan et al., 1999). The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm (Brown et al., 1993). The models are then used to decode, i.e. translate texts from the source language to the target eaj a is called the fertility of ei and is denoted by i . Since P r(f |e) = a P r(f , a|e), equation 1 can 1 In this paper, we use French and English as the prototypical examples of source and target languages respectively. 2 e0 is a special word called the null word and is used to account for those words in f that are not connected by a to any of the words of e. be rewritten as follows: e = argmax P r(f , a|e)P r.Relaxed Decoding Given the model parameters and a sentence f , e determine the most probable translation and aalignment pair for f . Brown and his colleagues developed a series of 5 translation models which have become to be known in the field of machine translation as IBM models. For a detailed introduction to IBM translation models, please see (Brown et al., 1993). In practice, models 3-5 are known to give good results and models 1-2 are used to seed the EM iterations of the higher models. IBM model 3 is the prototypical translation model and it models P r(f , a|e) as follows: (e,) = argmax P (f , a|e) P(e,a)Viterbi Alignment computation finds applications not only in SMT but also in other areas of Natural Language Processing (Wang, 1998), (Marcu, 2002). Expectation Evaluation is the soul of parameter estimation (Brown et al., 1993), (Al-Onaizan et al., 1999). Conditional Probability computation is important in experimentally studying the concentration of the probability mass P (f , a|e)n(0 | l \l i=1 n(i|ei i! around the Viterbi alignment, i.e. in determining j=1 tfj |eajdj: aj I=0 d(j|i, m,) the goodness of the Viterbi alignment in compar ison to the rest of the alignments.Decoding is an integral component of all SMT systems (Wang, Table 1: IBM Model 3 Here, n(|e) is the fertility model, t(f |e) is the lexicon model and d(j|i, m,) is the distortion model. The computational tasks involving IBM Models are the following:Viterbi Alignment Given the model parameters and a sentence pair (f , e), determine the most probable alignment between f and e. a = argmax P (f , a|e) aExpectation Evaluation This forms the core of model training via the EM algorithm. Please see Section 2.3 for a description of the computational task involved in the EM iterations.Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e). 1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004). Exact Decoding is the original decoding problem as defined in (Brown et al., 1993) and Relaxed Decoding is the relaxation of the decoding problem typically used in practice. While several heuristics have been developed by practitioners of SMT for the computational tasks involving IBM models, not much is known about the computational complexity of these tasks. In their seminal paper on SMT, Brown and his colleagues highlighted the problems we face as we go from IBM Models 1-2 to 3-5(Brown et al., 1993) 3: As we progress from Model 1 to Model 5, evaluating the expectations that gives us counts becomes increasingly difficult. In Models 3 and 4, we must be content with approximate EM iterations because it is not feasible to carry out sums over all possible alignments for these models. In practice, we are never sure that we have found the Viterbi alignment. However, neither their work nor the subsequent P (f |e) = aExact Decoding P (f , a|e) research in SMT studied the computational complexity of these fundamental problems with the exception of the Decoding problem. In (Knight, 1999) it was proved that the Exact Decoding prob Given the model parameters and a sentence f , determine the most probable translation of f . lem is NP-Hard when the language model is a bigram model. e = argmax e P (f , a|e) Pa Our results may be summarized as follows: 3 The emphasis is ours.1. Viterbi Alignment computation is NP-Hard for IBM Models 3, 4, and 5. 

IBM models 3-5 are widely used in SMT. The computational tasks discussed in this work form the backbone of all SMT systems that use IBM models. We believe that our results on the computational complexity of the tasks in SMT will result in a better understanding of these tasks from a theoretical perspective. We also believe that our results may help in the design of effective heuristicsfor some of these tasks. A theoretical analysis of the commonly employed heuristics will also be of interest.An open question in SMT is whether there exists closed form expressions (whose representation is polynomial in the size of the input) for P (f |e)and the counts in the EM iterations for models 3-5 (Brown et al., 1993). For models 1-2, closed formexpressions exist for P (f |e) and the counts in theEM iterations for models 3-5. Our results showthat there cannot exist a closed form expression(whose representation is polynomial in the size of the input) for P (f |e) and the counts in the EMiterations for Models 3-5 unless P = NP. 
