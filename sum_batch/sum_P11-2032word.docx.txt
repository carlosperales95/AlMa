In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs.

 Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006). State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable. However, being a maxi mization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting. In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points. Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008). Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. (2008), Nguyen et al. (2010), and Chung and Gildea (2009). The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010). On the other hand,  182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, Portland, Oregon, June 19-24, 2011. Qc 2011 Association for Computational Linguistics Chung and Gildea (2009) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting. They use variational Bayes for inference, but they do not investigate the effect of Bayesian inference to word alignment in isolation. Recently, Zhao and Gildea (2010) proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters and their inference method is actually stochastic EM (also known as Monte Carlo EM), a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence. The set of alignments for a sentence (corpus) is denoted by a (A). The model parameters consist of a VE × VF ta ble T of word translation probabilities such that te,f = P (f |e). The joint distribution of the Model-1 variables is given by the following generative model3: n approximate the expected counts in the E-step. Even though they report substantial reductions in align P (E, F, A; T) = P (e)P (a|e)P (f |a, e; T) (1) s J ment error rate, the translation BLEU scores do not improve. Our approach in this paper is fully Bayesian in = n	P (e) (I + 1)J s n t j=1  eaj ,fj  (2) which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive, sparse prior. We develop a Gibbs sampler for alignments under IBM Model 1,  In the proposed Bayesian setting, we treat T as a random variable with a prior P (T). To find a suitable prior for T, we re-write (2) as: (e) VE VF which is relevant for the state-of-the-art SMT sys tems since: (1) Model 1 is used in bootstrapping the parameter settings for EM training of higher P (E, F, A|T) = n s P (I + 1)J n n (t e=1 f =1  e,f )ne,f (3) VE VF P (e) order alignment models, and (2) many state-of-the = n n (te,f )Ne,f n J (4) art SMT systems use Model 1 translation probabil ities as features in their log-linear model. We eval  e=1 f =1 (I + 1) s uate the inferred alignments in terms of the end-toend translation performance, where we show the results with a variety of input data to illustrate the general applicability of the proposed technique. To our knowledge, this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance. 2 

We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs, data sizes and domains. As a result of this increase, Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus. 10 The GIZA++ implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4. The proposed method learns a compact, sparse translation distribution, overcoming the wellknown “garbage collection” problem of rare words in EM-estimated current models. 
