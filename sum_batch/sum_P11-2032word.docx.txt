 In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possi ble parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. 

 We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs, data sizes and domains. As a result of this increase, Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM 9 Each target word was aligned to the source candidate that	 	 co-occured the most number of times with that target word in the entire parallel corpus.  10 The GIZA++ implementation of Model 4 artificially limits fertility parameter values to at most nine. Model 4. The proposed method learns a compact, sparse translation distribution, overcoming the wellknown “garbage collection” problem of rare words in EM-estimated current models. 
