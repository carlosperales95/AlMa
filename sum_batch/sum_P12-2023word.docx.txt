 We propose an approach that biases machine translation systems toward relevant transla tions based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation with out any human annotation. We use these topic distributions to compute topic-dependent lex ical weighting probabilities and directly in corporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topic relevant output, resulting in significant im provements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline. 

 Applying SMT to new domains requires techniques to inform our algorithms how best to adapt. This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion. We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations, as evidenced by significant performance gains. This method presents several advantages over existing approaches. We can construct a topic model once on the training data, and use it infer topics on any test set to adapt the translation model. We can also incorporate large quantities of additional data (whether parallel or not) in the source language to infer better topics without relying on collection or genre annotations. Multilingual topic models (Boyd-Graber and Resnik, 2010) would provide a technique to use data from multiple languages to ensure consistent topics. 
