{"document":[{"claim_score":-1.6905329,"evidence_score":-0.47821176,"text":"Abstract In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization ( EM ) ."},{"claim_score":-1.2596719,"evidence_score":-0.19200309,"text":"We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possi ble parameter values in finding the alignment distribution ."},{"claim_score":-0.81366039,"evidence":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .","evidence_score":0.40844196,"text":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."},{"claim_score":0.20322872,"evidence_score":-0.31653466,"claim":"the proposed method effectively addresses the well-known rare word problemsame time induces a much smaller dictionary of bilingual word-pairs","text":"We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs ."},{"claim_score":-1.00159,"evidence_score":-0.56399395,"text":"Introduction Introduction Word alignment is a crucial early step in the training of most statistical machine translation ( SMT ) systems , in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction ( Koehn et al. , 2003 ; Chiang , 2007 ; Galley et al. , 2006 ) ."},{"claim_score":-2.2789076,"evidence_score":-0.33285904,"text":"State-of-the-art word alignment models , such as IBM Models ( Brown et al. , 1993 ) , HMM ( Vogel et al. , 1996 ) , and the jointly-trained symmetric HMM ( Liang et al. , 2006 ) , contain a large number of parameters ( e.g. , word translation probabilities ) that need to be estimated in addition to the desired hidden alignment variables ."},{"claim_score":-0.75320983,"evidence_score":-0.40802502,"text":"The most common method of inference in such models is expectation-maximization ( EM ) ( Dempster et al. , 1977 ) or an approximation to EM when exact EM is intractable ."},{"claim_score":-0.35608143,"evidence":"However , being a maxi mization ( e.g. , maximum likelihood ( ML ) or maximum a posteriori ( MAP ) ) technique , EM is generally prone to local optima and overfitti Conclusion We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .","evidence_score":0.22544708,"text":"However , being a maxi mization ( e.g. , maximum likelihood ( ML ) or maximum a posteriori ( MAP ) ) technique , EM is generally prone to local optima and overfitti Conclusion We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."},{"claim_score":-1.4693852,"evidence":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM 9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .","evidence_score":0.39974283,"text":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM 9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."},{"claim_score":-1.2770063,"evidence":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine .","evidence_score":0.66114691,"text":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine ."},{"claim_score":-2.3216848,"evidence_score":-0.5752008,"text":"Model 4 ."},{"claim_score":-1.2423505,"evidence_score":-0.51189171,"text":"The proposed method learns a compact , sparse translation distribution , overcoming the wellknown `` garbage collection '' problem of rare words in EM-estimated current models ."}]}