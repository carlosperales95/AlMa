{"document":[{"claim_score":0.25467021,"evidence_score":-0.55510014,"claim":"biases machine translation systems toward relevant transla tions based on topic-specific contextswhere topics are induced in an unsupervised way using topiccan be thought of as inducing subcorpora for adaptation with out any human annotation","text":"Abstract We propose an approach that biases machine translation systems toward relevant transla tions based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation with out any human annotation ."},{"claim_score":-1.3305095,"evidence_score":-0.16714951,"text":"We use these topic distributions to compute topic-dependent lex ical weighting probabilities and directly in corporate them into our translation model as features ."},{"claim_score":-0.85253199,"evidence":"Conditioning lexical probabilities on the topic biases translations toward topic relevant output , resulting in significant im provements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .","evidence_score":0.21680934,"text":"Conditioning lexical probabilities on the topic biases translations toward topic relevant output , resulting in significant im provements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."},{"claim_score":-1.3363799,"evidence_score":-0.28609678,"text":"Introduction Introduction The performance of a statistical machine translation ( SMT ) system on a translation task depends largely on the suitability of the available parallel training data ."},{"claim_score":-1.0148455,"evidence_score":-0.83305333,"text":"Domains ( e.g. , newswire vs. blogs ) may vary widely in their lexical choices and stylistic preferences , and what may be preferable in a general setting , or in one domain , is not necessarily preferable in another domain ."},{"claim_score":-0.80832398,"evidence_score":-0.94929886,"text":"Indeed , sometimes the domain can change the meaning of a phrase entirely ."},{"claim_score":-0.71034496,"evidence_score":-0.81868998,"text":"In a food related context , the Chinese sentence `` 粉 丝 很 多 '' ( `` feˇns ¯ i heˇnduo ¯ '' ) would mean `` They have a lot of vermicelli '' ; however , in an informal Internet conversation , this sentence would mean `` They have a lot of fans '' ."},{"claim_score":-0.19489659,"evidence_score":-0.27305627,"text":"Without the broader context , it is impossible to determine the correct translation in otherwise identical sentences ."},{"claim_score":-0.12101778,"evidence_score":-0.34638091,"text":"This problem has led to a substantial amount of recent work in trying to bias , or adapt , the translation m Conclusion Applying SMT to new domains requires techniques to inform our algorithms how best to adapt ."},{"claim_score":-1.6708331,"evidence_score":-0.30713169,"text":"This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion ."},{"claim_score":0.31338205,"evidence":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","evidence_score":0.43800079,"text":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","claim_evidence":"incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamicallysignificant performance gains"},{"claim_score":-1.347971,"evidence_score":-0.9402194,"text":"This method presents several advantages over existing approaches ."},{"claim_score":-1.0121336,"evidence":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .","evidence_score":0.14625465,"text":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model ."},{"claim_score":-0.85764029,"evidence_score":-0.78039501,"text":"We can also incorporate large quantities of additional data ( whether parallel or not ) in the source language to infer better topics without relying on collection or genre annotations ."},{"claim_score":-1.2525156,"evidence_score":-0.62061018,"text":"Multilingual topic models ( Boyd-Graber and Resnik , 2010 ) would provide a technique to use data from multiple languages to ensure consistent topics ."}]}