[
    {
        "claim_score": -0.2921872, 
        "evidence": "We evaluated the adapted LM on SMT and found that the evaluation metrics are crucial to reflect the actual improvement in performance .", 
        "evidence_score": 1.0293831, 
        "text": "We evaluated the adapted LM on SMT and found that the evaluation metrics are crucial to reflect the actual improvement in performance ."
    }, 
    {
        "claim_score": -1.2207436, 
        "evidence": "In Section 3 , we present the effect of LM adaptation on word perplexity , followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3 .", 
        "evidence_score": 0.55832561, 
        "text": "In Section 3 , we present the effect of LM adaptation on word perplexity , followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3 ."
    }, 
    {
        "claim_score": -0.9460509, 
        "evidence": "We organize the paper as follows : In Section 2 , we introduce the bLSA framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .", 
        "evidence_score": 0.44754478, 
        "text": "We organize the paper as follows : In Section 2 , we introduce the bLSA framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation ."
    }, 
    {
        "claim_score": -1.2935557, 
        "evidence": "The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 520527 , Prague , Czech Republic , June 2007 .", 
        "evidence_score": 0.30159118, 
        "text": "The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 520527 , Prague , Czech Republic , June 2007 ."
    }, 
    {
        "claim_score": -0.29264742, 
        "evidence": "Results showed that our approach significantly reduces the word perplexity on the target language in both cases using ASR hypotheses and manual transcripts .", 
        "evidence_score": 0.21702068, 
        "text": "Results showed that our approach significantly reduces the word perplexity on the target language in both cases using ASR hypotheses and manual transcripts ."
    }, 
    {
        "claim_score": -0.99066938, 
        "evidence": "On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .", 
        "evidence_score": 0.05781665, 
        "text": "On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM ."
    }, 
    {
        "claim_score": -0.61055872, 
        "evidence": "We will investigate the incorporation of monolingual documents for potentially better bilingual LSA modeling", 
        "evidence_score": 0.014981935, 
        "text": "We will investigate the incorporation of monolingual documents for potentially better bilingual LSA modeling"
    }
]