[
    {
        "claim_score": -0.84730833, 
        "evidence": "In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .", 
        "evidence_score": 0.79006548, 
        "text": "In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership ."
    }, 
    {
        "claim_score": -0.19184177, 
        "evidence": "Qc 2012 Association for Computational Linguistics data come from ; and even if we do , `` subcorpus '' may not be the most useful notion of domain for better translations .", 
        "evidence_score": 0.6940625, 
        "text": "Qc 2012 Association for Computational Linguistics data come from ; and even if we do , `` subcorpus '' may not be the most useful notion of domain for better translations ."
    }, 
    {
        "claim_evidence": "incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamicallysignificant performance gains", 
        "claim_score": 0.31338205, 
        "evidence": "We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .", 
        "evidence_score": 0.43800079, 
        "text": "We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains ."
    }, 
    {
        "claim_score": -1.0977388, 
        "evidence": "Matsoukas et al. ( 2009 ) introduced assigning a pair of binary features to each training sentence , indicating sentences ' genre and collection as a way to capture domains .", 
        "evidence_score": 0.40336123, 
        "text": "Matsoukas et al. ( 2009 ) introduced assigning a pair of binary features to each training sentence , indicating sentences ' genre and collection as a way to capture domains ."
    }, 
    {
        "claim_score": -0.69978507, 
        "evidence": "Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .", 
        "evidence_score": 0.30877683, 
        "text": "Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."
    }, 
    {
        "claim_score": -0.35286567, 
        "evidence": "Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .", 
        "evidence_score": 0.23639391, 
        "text": "Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction ."
    }, 
    {
        "claim_score": -1.0097031, 
        "evidence": "Chiang et al. ( 2011 ) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , fs ( e | f ) , one for each domain s , which compute a new word translation table ps ( e | f ) esti mated from only those sentences which belong to s : cs ( f , e ) / 2 .", 
        "evidence_score": 0.17165503, 
        "text": "Chiang et al. ( 2011 ) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , fs ( e | f ) , one for each domain s , which compute a new word translation table ps ( e | f ) esti mated from only those sentences which belong to s : cs ( f , e ) / 2 ."
    }, 
    {
        "claim_score": -1.0121336, 
        "evidence": "We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .", 
        "evidence_score": 0.14625465, 
        "text": "We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model ."
    }, 
    {
        "claim_score": -2.053893, 
        "evidence": "115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 115 -- 119 , Jeju , Republic of Korea , 8-14 July 2012 .", 
        "evidence_score": 0.1191586, 
        "text": "115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 115 -- 119 , Jeju , Republic of Korea , 8-14 July 2012 ."
    }, 
    {
        "claim_score": -0.82693176, 
        "evidence": "They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .", 
        "evidence_score": 0.043442885, 
        "text": "They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights ."
    }, 
    {
        "claim_score": -1.0713496, 
        "evidence": "We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model .", 
        "evidence_score": 0.0065037827, 
        "text": "We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model ."
    }
]