[
    {
        "claim_score": -0.85516396, 
        "evidence": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore ( 2004 ) and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .", 
        "evidence_score": 0.71674095, 
        "text": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore ( 2004 ) and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."
    }, 
    {
        "claim_score": -0.75035659, 
        "evidence": "2 We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .", 
        "evidence_score": 0.64012035, 
        "text": "2 We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."
    }, 
    {
        "claim_score": -1.5089481, 
        "evidence": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .", 
        "evidence_score": 0.54509008, 
        "text": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."
    }, 
    {
        "claim_score": -0.89206491, 
        "evidence": "10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .", 
        "evidence_score": 0.43668007, 
        "text": "10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."
    }, 
    {
        "claim_score": -2.363386, 
        "evidence": "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. ( 2008 ) , Nguyen et al. ( 2010 ) , and Chung and Gildea ( 2009 ) .", 
        "evidence_score": 0.43413359, 
        "text": "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. ( 2008 ) , Nguyen et al. ( 2010 ) , and Chung and Gildea ( 2009 ) ."
    }, 
    {
        "claim_score": -0.81366039, 
        "evidence": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .", 
        "evidence_score": 0.40844196, 
        "text": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."
    }, 
    {
        "claim_score": -1.6233231, 
        "evidence": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) .", 
        "evidence_score": 0.40277048, 
        "text": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) ."
    }, 
    {
        "claim_score": -1.3837618, 
        "evidence": "Qc 2011 Association for Computational Linguistics Chung and Gildea ( 2009 ) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .", 
        "evidence_score": 0.28411691, 
        "text": "Qc 2011 Association for Computational Linguistics Chung and Gildea ( 2009 ) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."
    }, 
    {
        "claim_score": -0.71557318, 
        "evidence": "We eval e = 1 f = 1 ( I + 1 ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .", 
        "evidence_score": 0.16370924, 
        "text": "We eval e = 1 f = 1 ( I + 1 ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."
    }, 
    {
        "claim_score": -1.3524749, 
        "evidence": "On the other hand , 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182 -- 187 , Portland , Oregon , June 19-24 , 2011 .", 
        "evidence_score": 0.08320332, 
        "text": "On the other hand , 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182 -- 187 , Portland , Oregon , June 19-24 , 2011 ."
    }, 
    {
        "claim_score": -0.87348658, 
        "evidence": "Recently , Zhao and Gildea ( 2010 ) proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence .", 
        "evidence_score": 0.011154049, 
        "text": "Recently , Zhao and Gildea ( 2010 ) proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence ."
    }
]