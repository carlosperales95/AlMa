Abstract
 The ability of human visual system to detect visual saliency is extraordinarily fast and reliable. However, computational modeling of this basic intelligent behavior still remains a challenge. This paper presents a simple method for the visual saliency detection. Our model is independent of features, categories, or other forms of prior knowledge of the objects. By analyzing the log-spectrum of an input image, we extract the spectral residual of an image in spectral domain, and propose a fast method to construct the corresponding saliency map in spatial domain. We test this model on both natural pictures and artificial images such as psychological patterns. The result indicate fast and robust saliency detection of our method.
Discussion
 We proposed a method for general purpose object detection. This method is based on the log spectra representation of images. Our major contribution is the discovery of spectral residual and its general ability to detect proto-objects.  5.1. The prospect of spectral residual approach One of the advantages of the spectral residual approach is its generality. The prior knowledge required for saliency detection is not necessary in our system. In addition, this all-in-one definition of saliency covers unknown features  such as “curve” in Fig.9. Also, the spectral residual resolves the problem of weighting features from different channels (for example, shape, texture, and orientations). The result of our system, in contrast with its simple implementation, is demonstrated effective. Finally, compared with other detection algorithms, the computational consumption of our method is extremely parsimonious, providing a promising solution to real time systems.  5.2. Further work Is the striking similarities of our results and performance of human visual system, especially, the response to psychological patterns, all comes in a coincidence, or if there is biological implications of the human visual system and the spectral residual? It has been reported that different objects with similar frequency spectra interfere with each other [2]. More recent studies also indicate that a visual target takes more time to be identified when the spectrum of background is carefully tuned to mask the spectrum of the foreground [28]. More work is required to discover the spectral properties of early vision. In this paper, our discussion is limited to static images. Although it is possible to compute the saliency map for each  Figure 8. The result of our method in comparison with Itti’s method and the result of human labelers. In each group, we present 1) the input image, 2) saliency map generated by spectral residual, 3) saliency map Q generated by Itti’s method, and 4) labeled map of the four labelers. In the labeled map, the white region represents the hit map, where Ok (x) = 1; the black region represents the false alarm map, where Q (1 − Ok (x)) = 0; and the gray region is selected by some labelers but rejected by others.  frames of a video sequence without considering their continuity, incorporating motion features will greatly extend the application of our method. Due to the particularity of motion features, a unified model of features has not yet been proposed. Yet, we are glad to see that efforts have been made in incorporating motion into a general framework of features [16]. Another potential work is to cooperate our method with segmentation techniques. Segmentation is an independent area of research whose primary goal is to separate borders. In comparison, our method overlooked the spatial homogeneity of an object. For instance, in the last example of Fig.8, the poloists and their horses are separated. In order to achieve the general purpose object detection, further efforts should be done to delimit a clear border of an object.  6. Acknowledgement The work was the National High-Tech Research Program of China (Grant No.252006AA01Z125) and supported by  the National Basic Research Program of China (Grant No. 2005CB724301). The first author would like to thank Deli Zhao, Dirk Walther, and Yuandong Tian for their valuable discussions.  References
