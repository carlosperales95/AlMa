<?xml version="1.0" encoding="UTF-8"?><document><sentence><claim_score>-1.7734402</claim_score><evidence_score>0.18517543</evidence_score><text>Abstract Figure 1 .</text>
<evidence> Abstract Figure 1 .</evidence>
</sentence>
<sentence><claim_score>-0.67077204</claim_score><evidence_score>-0.47596579</evidence_score><text>Our context-aware saliency results ( bottom ) comply with the descriptions that people provided ( samples in the second row ) for the input images ( top ) .</text>
<not_argumentative> Our context-aware saliency results ( bottom ) comply with the descriptions that people provided ( samples in the second row ) for the input images ( top ) .</not_argumentative>
</sentence>
<sentence><claim_score>-0.072520315</claim_score><evidence_score>-0.3943998</evidence_score><text>People tend to describe the scene rather than the dominant object .</text>
<not_argumentative> People tend to describe the scene rather than the dominant object .</not_argumentative>
</sentence>
<sentence><claim_score>-0.5668044</claim_score><evidence_score>-0.4688651</evidence_score><text>Classical saliency extraction algorithms aim at the third row , which might miss the essence of the scene .</text>
<not_argumentative> Classical saliency extraction algorithms aim at the third row , which might miss the essence of the scene .</not_argumentative>
</sentence>
<sentence><claim_score>-0.93611362</claim_score><evidence_score>-0.20903151</evidence_score><text>Conversely , we maintain all the essential regions of the image .</text>
<not_argumentative> Conversely , we maintain all the essential regions of the image .</not_argumentative>
</sentence>
<sentence><claim_score>-1.7458124</claim_score><evidence_score>-0.5737821</evidence_score><text>methods aim to extract the `` girl '' , the `` figure '' , and the `` athlete `` ( third row ) .</text>
<not_argumentative> methods aim to extract the `` girl '' , the `` figure '' , and the `` athlete `` ( third row ) .</not_argumentative>
</sentence>
<sentence><claim_score>-1.2428977</claim_score><evidence_score>-0.68615948</evidence_score><text>This type of saliency is useful for several high-level tasks , such as object recognition [ 20 ] or segmentation [ 18 ] .</text>
<not_argumentative> This type of saliency is useful for several high-level tasks , such as object recognition [ 20 ] or segmentation [ 18 ] .</not_argumentative>
</sentence>
<sentence><claim_score>-0.60604462</claim_score><evidence_score>-1.4750497</evidence_score><text>There are , however , applications where the context of the dominant objects is just as essential as the objects themselves .</text>
<not_argumentative> There are , however , applications where the context of the dominant objects is just as essential as the objects themselves .</not_argumentative>
</sentence>
<sentence><claim_score>-1.5032111</claim_score><evidence_score>0.061829182</evidence_score><text>Examples include image classification [ 14 ] , summarization of a photo collection [ 17 ] , thumbnailing [ 21 ] , and retargeting [ 19 ] .</text>
<evidence> Examples include image classification [ 14 ] , summarization of a photo collection [ 17 ] , thumbnailing [ 21 ] , and retargeting [ 19 ] .</evidence>
</sentence>
<sentence><claim_score>-0.7856359</claim_score><evidence_score>0.12202054</evidence_score><text>For these applications , the detected regions in Figure 1 should correspond to the titles you gave above .</text>
<evidence> For these applications , the detected regions in Figure 1 should correspond to the titles you gave above .</evidence>
</sentence>
<sentence><claim_score>-1.1853371</claim_score><evidence_score>0.041457515</evidence_score><text>The regions on the bottom row of Figure 1 match these titles better than the regions on the third row .</text>
<evidence> The regions on the bottom row of Figure 1 match these titles better than the regions on the third row .</evidence>
</sentence>
<sentence><claim_score>-2.7430432</claim_score><evidence_score>-1.0162205</evidence_score><text>( a ) Input ( b ) Local [ 24 ] ( c ) Global [ 7 ] ( d ) Local-global [ 13 ] ( e ) Our context-aware Figure 2 .</text>
<not_argumentative> ( a ) Input ( b ) Local [ 24 ] ( c ) Global [ 7 ] ( d ) Local-global [ 13 ] ( e ) Our context-aware Figure 2 .</not_argumentative>
</sentence>
<sentence><claim_score>-0.68165966</claim_score><evidence_score>-0.64980371</evidence_score><text>Comparing different approaches to saliency This calls for introducing a new type of saliency -- contextaware saliency .</text>
<not_argumentative> Comparing different approaches to saliency This calls for introducing a new type of saliency -- contextaware saliency .</not_argumentative>
</sentence>
<sentence><claim_score>-1.26993</claim_score><evidence_score>-0.79857289</evidence_score><text>Here , the goal is to identify the pixels that correspond to the bottom row ( and to the titles ) .</text>
<not_argumentative> Here , the goal is to identify the pixels that correspond to the bottom row ( and to the titles ) .</not_argumentative>
</sentence>
<sentence><claim_score>-0.67632821</claim_score><evidence_score>-0.049186419</evidence_score><text>According to this concept , the salient regions should contain not only the prominent objects but also the parts of the background that convey the context .</text>
<not_argumentative> According to this concept , the salient regions should contain not only the prominent objects but also the parts of the background that convey the context .</not_argumentative>
</sentence>
<sentence><claim_score>-1.856439</claim_score><evidence_score>0.53695231</evidence_score><text>We differentiate between three types of images , as illustrated in Figure 1 .</text>
<evidence> We differentiate between three types of images , as illustrated in Figure 1 .</evidence>
</sentence>
<sentence><claim_score>-0.58676307</claim_score><evidence_score>-0.091611027</evidence_score><text>In the girl 's case , the background is not interesting , hence , we expect the extracted salient region to coincide with the salient object .</text>
<not_argumentative> In the girl 's case , the background is not interesting , hence , we expect the extracted salient region to coincide with the salient object .</not_argumentative>
</sentence>
<sentence><claim_score>-0.64893484</claim_score><evidence_score>-0.11824179</evidence_score><text>In the flower-field 's case , the texture of the flowers is essential for understanding the content .</text>
<not_argumentative> In the flower-field 's case , the texture of the flowers is essential for understanding the content .</not_argumentative>
</sentence>
<sentence><claim_score>-1.0659151</claim_score><evidence_score>-0.75683921</evidence_score><text>However , only a small portion of it -- the portion surrounding the figure -- suffices .</text>
<not_argumentative> However , only a small portion of it -- the portion surrounding the figure -- suffices .</not_argumentative>
</sentence>
<sentence><claim_score>-0.76181632</claim_score><evidence_score>-0.41382387</evidence_score><text>In the weight lifter 's case , some of the contextual background is vital for conveying the scene .</text>
<not_argumentative> In the weight lifter 's case , some of the contextual background is vital for conveying the scene .</not_argumentative>
</sentence>
<sentence><claim_score>-1.4515011</claim_score><evidence_score>-0.72142429</evidence_score><text>This is not necessarily the portion surrounding the athlete , but rather a unique part of the background ( the weights and the olympic logo ) .</text>
<not_argumentative> This is not necessarily the portion surrounding the athlete , but rather a unique part of the background ( the weights and the olympic logo ) .</not_argumentative>
</sentence>
<sentence><claim_score>-0.56485432</claim_score><evidence_score>0.12268264</evidence_score><text>Therefore , detecting the prominent object together with naive addition of its immediate surrounding will not suffice .</text>
<evidence> Therefore , detecting the prominent object together with naive addition of its immediate surrounding will not suffice .</evidence>
</sentence>
<sentence><claim_score>-1.2880401</claim_score><evidence_score>-0.78075919</evidence_score><text>This paper proposes a novel algorithm for context-aware saliency detection .</text>
<not_argumentative> This paper proposes a novel algorithm for context-aware saliency detection .</not_argumentative>
</sentence>
<sentence><claim_score>0.98866573</claim_score><evidence_score>-0.83933551</evidence_score><text>The underlying idea is that salient regions are distinctive with respect to both their local and global surroundings .</text>
<claim> salient regions are distinctive with respect to both their local and global surroundings</claim>
</sentence>
<sentence><claim_score>-1.4959458</claim_score><evidence_score>-0.35922468</evidence_score><text>Hence , the unique parts of the background , and not only the dominant objects , would be marked salient by our algorithm ( e.g. , the Olympics logo in Figure 1 ) .</text>
<not_argumentative> Hence , the unique parts of the background , and not only the dominant objects , would be marked salient by our algorithm ( e.g. , the Olympics logo in Figure 1 ) .</not_argumentative>
</sentence>
<sentence><claim_score>-0.59884429</claim_score><evidence_score>-0.44405725</evidence_score><text>Moreover , to comply with the Gestalt laws , we prioritize regions close to the foci of attention .</text>
<not_argumentative> Moreover , to comply with the Gestalt laws , we prioritize regions close to the foci of attention .</not_argumentative>
</sentence>
<sentence><claim_score>-0.67658404</claim_score><evidence_score>0.086674474</evidence_score><text>This maintains the background texture , when it is interesting , such as in the case of the flower field in Figure 1 .</text>
<evidence> This maintains the background texture , when it is interesting , such as in the case of the flower field in Figure 1 .</evidence>
</sentence>
<sentence><claim_score>-1.8560882</claim_score><evidence_score>-0.11705509</evidence_score><text>We demonstrate the utility of our context-aware saliency in two applications .</text>
<not_argumentative> We demonstrate the utility of our context-aware saliency in two applications .</not_argumentative>
</sentence>
<sentence><claim_score>0.31006652</claim_score><evidence_score>0.70181497</evidence_score><text>The first is retargeting [ 1 , 19 , 16 ] , where we show that our saliency can successfully mark the regions that should be kept untouched .</text>
<claim_evidence> our saliency can successfully mark the regions that should be kept untouched</claim_evidence>
<evidence> The first is retargeting [ 1 , 19 , 16 ] , where we show that our saliency can successfully mark the regions that should be kept untouched .</evidence>
</sentence>
<sentence><claim_score>-0.60933279</claim_score><evidence_score>-0.0051448862</evidence_score><text>The second is summarization [ 17 , 25 , 2 , 15 , 4 ] , where we demonstrate that saliencybased collages are informative , compact , and eye-pleasing .</text>
<not_argumentative> The second is summarization [ 17 , 25 , 2 , 15 , 4 ] , where we demonstrate that saliencybased collages are informative , compact , and eye-pleasing .</not_argumentative>
</sentence>
<sentence><claim_score>-1.2659643</claim_score><evidence_score>-0.87475326</evidence_score><text>The contribution of this paper is hence threefold .</text>
<not_argumentative> The contribution of this paper is hence threefold .</not_argumentative>
</sentence>
<sentence><claim_score>-1.182998</claim_score><evidence_score>-0.70931438</evidence_score><text>First , we introduce principles for context-aware saliency ( Section 2 ) .</text>
<not_argumentative> First , we introduce principles for context-aware saliency ( Section 2 ) .</not_argumentative>
</sentence>
<sentence><claim_score>-1.7941937</claim_score><evidence_score>-0.98372721</evidence_score><text>Second , we propose an algorithm that detects this saliency ( Section 3 ) and present results on images of various types ( Section 4 ) .</text>
<not_argumentative> Second , we propose an algorithm that detects this saliency ( Section 3 ) and present results on images of various types ( Section 4 ) .</not_argumentative>
</sentence>
<sentence><claim_score>-2.0885999</claim_score><evidence_score>-0.39715394</evidence_score><text>Last but not least , we demonstrate the applicability of our saliency ( Section 5 ) .</text>
<not_argumentative> Last but not least , we demonstrate the applicability of our saliency ( Section 5 ) .</not_argumentative>
</sentence>
<sentence><claim_score>-0.66428338</claim_score><evidence_score>-1.1105429</evidence_score><text>Conclusion This paper proposes a new type of saliency -- contextaware saliency -- which detects the important parts of the scene .</text>
<not_argumentative> Conclusion This paper proposes a new type of saliency -- contextaware saliency -- which detects the important parts of the scene .</not_argumentative>
</sentence>
<sentence><claim_score>-1.6261782</claim_score><evidence_score>-0.94764979</evidence_score><text>This saliency is based on four principles observed in the psychological literature : local low-level considerations , global considerations , visual organizational rules , and highlevel factors .</text>
<not_argumentative> This saliency is based on four principles observed in the psychological literature : local low-level considerations , global considerations , visual organizational rules , and highlevel factors .</not_argumentative>
</sentence>
<sentence><claim_score>-1.3588689</claim_score><evidence_score>-0.83324585</evidence_score><text>The paper further presents an algorithm for computing this saliency .</text>
<not_argumentative> The paper further presents an algorithm for computing this saliency .</not_argumentative>
</sentence>
<sentence><claim_score>-0.35112885</claim_score><evidence_score>-1.3530681</evidence_score><text>There exists a variety of applications where the context of the dominant objects is just as essential as the objects themselves .</text>
<not_argumentative> There exists a variety of applications where the context of the dominant objects is just as essential as the objects themselves .</not_argumentative>
</sentence>
<sentence><claim_score>-1.5323209</claim_score><evidence_score>-1.2972056</evidence_score><text>This paper evaluated the contribution of context - ( a ) The collage summarization ( b ) The saliency maps of the input images Figure 11 .</text>
<not_argumentative> This paper evaluated the contribution of context - ( a ) The collage summarization ( b ) The saliency maps of the input images Figure 11 .</not_argumentative>
</sentence>
<sentence><claim_score>-1.6682111</claim_score><evidence_score>0.042122844</evidence_score><text>Summarization of a trip to LA using 14 images .</text>
<evidence> Summarization of a trip to LA using 14 images .</evidence>
</sentence>
<sentence><claim_score>-2.4728364</claim_score><evidence_score>-0.73593129</evidence_score><text>aware saliency in two such applications -- retargeting and summarization .</text>
<not_argumentative> aware saliency in two such applications -- retargeting and summarization .</not_argumentative>
</sentence>
<sentence><claim_score>-1.0508389</claim_score><evidence_score>-0.086152553</evidence_score><text>In the future we intend to learn the benefits of this saliency in more applications , such as image classification and thumbnailing .</text>
<not_argumentative> In the future we intend to learn the benefits of this saliency in more applications , such as image classification and thumbnailing .</not_argumentative>
</sentence>
<sentence><claim_score>-1.3568412</claim_score><evidence_score>-0.23530467</evidence_score><text>Acknowledgements : This work was supported by the Fund for the Promotion of Research at the Technion and by the Ollendorff Foundation .</text>
<not_argumentative> Acknowledgements : This work was supported by the Fund for the Promotion of Research at the Technion and by the Ollendorff Foundation .</not_argumentative>
</sentence>
</document>