{"document":[{"claim_score":0.26609839,"evidence_score":-0.53007972,"claim":"topics are induced in an unsupervised way using topiccan be thought of as inducing subcorpora for adaptation without any human annotation","text":"We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation ."},{"claim_score":-1.0340334,"evidence_score":-0.15807633,"text":"We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features ."},{"claim_score":-0.69978507,"evidence":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .","evidence_score":0.30877683,"text":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."},{"claim_score":-1.2202343,"evidence_score":-0.52307109,"text":"Applying SMT to new domains requires techniques to inform our algorithms how best to adapt ."},{"claim_score":-1.6708331,"evidence_score":-0.30713169,"text":"This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion ."},{"claim_score":0.31338205,"evidence":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","evidence_score":0.43800079,"text":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","claim_evidence":"incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamicallysignificant performance gains"},{"claim_score":-1.347971,"evidence_score":-0.9402194,"text":"This method presents several advantages over existing approaches ."},{"claim_score":-1.0121336,"evidence":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .","evidence_score":0.14625465,"text":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model ."},{"claim_score":-0.85764029,"evidence_score":-0.78039501,"text":"We can also incorporate large quantities of additional data ( whether parallel or not ) in the source language to infer better topics without relying on collection or genre annotations ."},{"claim_score":-1.2525156,"evidence_score":-0.62061018,"text":"Multilingual topic models ( Boyd-Graber and Resnik , 2010 ) would provide a technique to use data from multiple languages to ensure consistent topics ."}]}