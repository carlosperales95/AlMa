{"document":[{"claim_score":-1.6209413,"evidence_score":-0.55875084,"text":"In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization ( EM ) ."},{"claim_score":-1.2878261,"evidence_score":-0.20315603,"text":"We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possible parameter values in finding the alignment distribution ."},{"claim_score":-0.81366039,"evidence":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .","evidence_score":0.40844196,"text":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."},{"claim_score":0.20322872,"evidence_score":-0.31653466,"claim":"the proposed method effectively addresses the well-known rare word problemsame time induces a much smaller dictionary of bilingual word-pairs","text":"We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs ."},{"claim_score":-0.75892448,"evidence_score":-0.61946898,"text":"Word alignment is a crucial early step in the training of most statistical machine translation ( SMT ) systems , in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction ( Koehn et al. , 2003 ; Chiang , 2007 ; Galley et al. , 2006 ) ."},{"claim_score":-2.2789076,"evidence_score":-0.33285904,"text":"State-of-the-art word alignment models , such as IBM Models ( Brown et al. , 1993 ) , HMM ( Vogel et al. , 1996 ) , and the jointly-trained symmetric HMM ( Liang et al. , 2006 ) , contain a large number of parameters ( e.g. , word translation probabilities ) that need to be estimated in addition to the desired hidden alignment variables ."},{"claim_score":-0.75320983,"evidence_score":-0.40802502,"text":"The most common method of inference in such models is expectation-maximization ( EM ) ( Dempster et al. , 1977 ) or an approximation to EM when exact EM is intractable ."},{"claim_score":-0.75963717,"evidence_score":-0.57615294,"text":"However , being a maxi mization ( e.g. , maximum likelihood ( ML ) or maximum a posteriori ( MAP ) ) technique , EM is generally prone to local optima and overfitting ."},{"claim_score":-1.1219886,"evidence_score":-0.32047616,"text":"In essence , the alignment distribution obtained via EM takes into account only the most likely point in the parameter space , but does not consider contributions from other points ."},{"claim_score":-0.85516396,"evidence":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore ( 2004 ) and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .","evidence_score":0.71674095,"text":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore ( 2004 ) and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."},{"claim_score":-0.35705892,"evidence_score":-0.3295088,"text":"Zhao and Xing ( 2006 ) note that the parameter estimation ( for which they use variational EM ) suffers from data sparsity and use symmetric Dirichlet priors , but they find the MAP solution ."},{"claim_score":-1.6233231,"evidence":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) .","evidence_score":0.40277048,"text":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) ."},{"claim_score":-2.363386,"evidence":"Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. ( 2008 ) , Nguyen et al. ( 2010 ) , and Chung and Gildea ( 2009 ) .","evidence_score":0.43413359,"text":"Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. ( 2008 ) , Nguyen et al. ( 2010 ) , and Chung and Gildea ( 2009 ) ."},{"claim_score":-1.9193709,"evidence_score":-1.1383175,"text":"The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling ."},{"claim_score":-1.2249446,"evidence_score":-0.69792162,"text":"However , alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA + + ( Xu et al. , 2008 ) or by local maximization ( Nguyen et al. , 2010 ) ."},{"claim_score":-1.3524749,"evidence":"On the other hand , 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182 -- 187 , Portland , Oregon , June 19-24 , 2011 .","evidence_score":0.08320332,"text":"On the other hand , 182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182 -- 187 , Portland , Oregon , June 19-24 , 2011 ."},{"claim_score":-1.3837618,"evidence":"Qc 2011 Association for Computational Linguistics Chung and Gildea ( 2009 ) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .","evidence_score":0.28411691,"text":"Qc 2011 Association for Computational Linguistics Chung and Gildea ( 2009 ) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."},{"claim_score":-0.164774,"evidence_score":-0.4332809,"text":"They use variational Bayes for inference , but they do not investigate the effect of Bayesian inference to word alignment in isolation ."},{"claim_score":-0.87348658,"evidence":"Recently , Zhao and Gildea ( 2010 ) proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence .","evidence_score":0.011154049,"text":"Recently , Zhao and Gildea ( 2010 ) proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence ."},{"claim_score":-1.3041212,"evidence_score":-1.2009664,"text":"The set of alignments for a sentence ( corpus ) is denoted by a ( A ) ."},{"claim_score":-1.6374377,"evidence_score":-0.17309855,"text":"The model parameters consist of a VE Ã— VF ta ble T of word translation probabilities such that te , f = P ( f | e ) ."},{"claim_score":-0.75993804,"evidence_score":-0.28980835,"text":"The joint distribution of the Model-1 variables is given by the following generative model3 : n approximate the expected counts in the E-step ."},{"claim_score":-1.8506118,"evidence_score":-0.54522807,"text":"Even though they report substantial reductions in align P ( E , F , A ; T ) = P ( e ) P ( a | e ) P ( f | a , e ; T ) ( 1 ) s J ment error rate , the translation BLEU scores do not improve ."},{"claim_score":-1.0106464,"evidence_score":-0.12060934,"text":"Our approach in this paper is fully Bayesian in = n P ( e ) ( I + 1 ) J s n t j = 1 eaj , fj ( 2 ) which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive , sparse prior ."},{"claim_score":-1.330747,"evidence_score":-0.14379568,"text":"We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T ) ."},{"claim_score":-1.0429087,"evidence_score":-0.35369441,"text":"To find a suitable prior for T , we re-write ( 2 ) as : ( e ) VE VF which is relevant for the state-of-the-art SMT sys tems since : ( 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T ) = n s P ( I + 1 ) J n n ( t e = 1 f = 1 e , f ) ne , f ( 3 ) VE VF P ( e ) order alignment models , and ( 2 ) many state-of-the = n n ( te , f ) Ne , f n J ( 4 ) art SMT systems use Model 1 translation probabil ities as features in their log-linear model ."},{"claim_score":-0.71557318,"evidence":"We eval e = 1 f = 1 ( I + 1 ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .","evidence_score":0.16370924,"text":"We eval e = 1 f = 1 ( I + 1 ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."},{"claim_score":-0.69109501,"evidence_score":-0.20694197,"text":"To our knowledge , this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance ."},{"claim_score":-0.75035659,"evidence":"2 We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .","evidence_score":0.64012035,"text":"2 We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."},{"claim_score":-1.5089481,"evidence":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .","evidence_score":0.54509008,"text":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."},{"claim_score":-0.89206491,"evidence":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .","evidence_score":0.43668007,"text":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."},{"claim_score":-1.2423505,"evidence_score":-0.51189171,"text":"The proposed method learns a compact , sparse translation distribution , overcoming the wellknown `` garbage collection '' problem of rare words in EM-estimated current models ."}]}