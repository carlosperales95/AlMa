{"document":[{"claim_score":0.093371718,"evidence_score":-0.86405517,"claim":"Abstract The problem of identifyingtext is typically tackled using specifically trained machine learning clas sifiers","text":"Abstract The problem of identifying and correcting confusibles , i.e. context-sensitive spelling errors , in text is typically tackled using specifically trained machine learning clas sifiers ."},{"claim_score":-0.93414242,"evidence_score":-0.50393437,"text":"For each different set of con fusibles , a specific classifier is trained and tuned ."},{"claim_score":-1.3565263,"evidence_score":-0.017581461,"text":"In this research , we investigate a more generic approach to context-sensitive con fusible correction ."},{"claim_score":-1.0393786,"evidence_score":-0.6329722,"text":"Instead of using spe cific classifiers , we use one generic clas sifier based on a language model ."},{"claim_score":-0.92507581,"evidence_score":-0.87472973,"text":"This measures the likelihood of sentences with different possible solutions of a confusible in place ."},{"claim_score":-0.030048242,"evidence_score":-1.0620949,"text":"The advantage of this approach is that all confusible sets are handled by a single model ."},{"claim_score":0.7967242,"evidence":"Preliminary results show that the performance of the generic clas sifier approach is only slightly worse that that of the specific classifier approach .","evidence_score":0.56684891,"text":"Preliminary results show that the performance of the generic clas sifier approach is only slightly worse that that of the specific classifier approach .","claim_evidence":"Preliminary results show that the performance of the generic clas sifier approach is only"},{"claim_score":-0.56529392,"evidence_score":-0.84444312,"text":"Conclusion and future work Confusibles are spelling errors that can only be detected within their sentential context ."},{"claim_score":-0.52259344,"evidence_score":-1.0610897,"text":"This kind of errors requires a completely different approach compared to non-word errors ( errors that can be identified out of context , i.e. sequences of characters that do not belong to the language ) ."},{"claim_score":-0.41002407,"evidence_score":-0.6683452,"text":"In practice , most confusible disambiguation systems are based on machine learning classification techniques , where for each type of confusible , a new classifier is trained and tuned ."},{"claim_score":-0.85462477,"evidence":"In this article , we investigate the use of language models in the context of confusible disambiguation .","evidence_score":0.0060642564,"text":"In this article , we investigate the use of language models in the context of confusible disambiguation ."},{"claim_score":-0.61382479,"evidence_score":-0.37006466,"text":"This approach works by selecting the word in the set of confusibles that has the highest probability in the sentential context according to the language model ."},{"claim_score":-1.3740021,"evidence_score":-1.2548383,"text":"Any kind of language model can be used in this approach ."},{"claim_score":0.42199581,"evidence_score":-0.35124,"claim":"it is easy to add new sets of confusibles without retraining","text":"The main advantage of using language models as generic classifiers is that it is easy to add new sets of confusibles without retraining or adding additional classifiers ."},{"claim_score":1.3514896,"evidence":"The entire language is mod eled , which means that all the information on words in their context is inherently present .","evidence_score":0.041877401,"text":"The entire language is mod eled , which means that all the information on words in their context is inherently present .","claim_evidence":"The entire language is mod eledall the information on words in their context is inherently present"},{"claim_score":-0.28873711,"evidence_score":-0.30677342,"text":"The experiments show that using generic classifiers based on simple n-gram language models yield slightly worse results compared to the specific classifier approach , where each classifier is specifically trained on one confusible set ."},{"claim_score":-0.089743141,"evidence_score":-0.6779058,"text":"However , the advantage of the generic classifier approach is that only one system has to be trained , compared to different systems for each confusible in the specific classifier case ."},{"claim_score":-0.6822368,"evidence":"Also , the exact computation of the probabilities using the n-grams , in particular the means of backing-off , has a large impact on the results .","evidence_score":0.10108005,"text":"Also , the exact computation of the probabilities using the n-grams , in particular the means of backing-off , has a large impact on the results ."},{"claim_score":-0.86150876,"evidence_score":-0.75956126,"text":"As future work , we would like to investigate the accuracy of more complex language models used as classifiers ."},{"claim_score":-0.83826242,"evidence_score":-0.49069909,"text":"The n-gram language models described here are relatively simple , but more complex language models could improve performance ."},{"claim_score":-0.69747851,"evidence":"In particular , instead of back-off , smoothing techniques could be investigated to reduce the impact of zero probability problems ( Chen and Goodman , 1996 ) .","evidence_score":0.17803104,"text":"In particular , instead of back-off , smoothing techniques could be investigated to reduce the impact of zero probability problems ( Chen and Goodman , 1996 ) ."},{"claim_score":-0.67061124,"evidence":"This assumes that the training data we are currently working with is not enough to properly describe the language .","evidence_score":0.0097337978,"text":"This assumes that the training data we are currently working with is not enough to properly describe the language ."},{"claim_score":-1.5663813,"evidence_score":-0.81369167,"text":"Additionally , language models that concentrate on more structural descriptions of the language , for instance , using grammatical inference techniques ( de la Higuera , 2005 ) , or models that explicitly take long distance dependencies into account ( Griffiths et al. , 2005 ) can be investigated ."},{"claim_score":-0.70896763,"evidence_score":-1.1273899,"text":"This leads to much richer language models that could , for example , check whether there is already a verb in the sentence ( which helps in cases such as -LCB- its , it 's -RCB- ) ."},{"claim_score":-1.0650985,"evidence_score":-0.58450188,"text":"A different route which we would also like to investigate is the usage of a specific classifier , such as TiMBL 's IGTree , as a language model ."},{"claim_score":-0.53670231,"evidence_score":-0.38470327,"text":"If a classifier is trained to predict the next word in the sentence or to predict the word at a given position with both left and right context as features , it can be used to estimate the probability of the words in a confusible set , just like the language models we have looked at so far ."},{"claim_score":-0.97178371,"evidence_score":-1.5019347,"text":"Another type of classifier might estimate the perplexity at a position , or provide some other measure of `` surprisedness '' ."},{"claim_score":-2.0190612,"evidence_score":-0.84088526,"text":"Effectively , these approaches all take a model of the entire language ( as described in the training data ) into account ."}]}