{"document":[{"claim_score":0.15168031,"evidence_score":-0.095410581,"claim":"Mixture modelling is a standard technique for density estimationhas just started to be explored","text":"Mixture modelling is a standard technique for density estimation , but its use in statistical machine translation ( SMT ) has just started to be explored ."},{"claim_score":-0.3359755,"evidence_score":-0.3788221,"text":"One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset ."},{"claim_score":-0.98071091,"evidence_score":-0.74854958,"text":"This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears ."},{"claim_score":-0.82714935,"evidence_score":-0.11503343,"text":"In this paper , we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system ."},{"claim_score":-0.79226691,"evidence_score":-0.076933032,"text":"Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling ."},{"claim_score":-0.34054976,"evidence_score":-0.50451688,"text":"Mixture modelling is a popular approach for density estimation in many scientific areas ( G. J. McLachlan and D. Peel , 2000 One of the most interesting properties of mixture modelling is its capability to model multimodal datasets by defining soft partitions on these datasets , and learning specific probability distributions for each partition , that better explains the general data generation process.Work supported by the EC ( FEDER ) and the Spanish MEC under grant TIN2006-15694-CO2-01 , the Consellera dEmpresa , Universitat i Ciencia Generalitat Valenciana under contract GV06the Universidad Politecnica de Valencia with ILETA project and Ministerio de Educacio n y Ciencia.In Machine Translation ( MT it is common to encounter large parallel corpora devoted to heterogeneous topics ."},{"claim_score":-0.90925185,"evidence_score":-0.67358534,"text":"These topics usually define sets of topic-specific lexicons that need to be translated taking into the semantic context in which they are found ."},{"claim_score":-1.5341517,"evidence_score":-0.82812146,"text":"This semantic dependency problem could be overcome by learning topic-dependent translation models that capture together the semantic context and the translation process ."},{"claim_score":0.62391379,"evidence_score":-0.3638559,"claim":"the application of mixture modelling in SMT has received increasing attention","text":"However , there have not been until very recently that the application of mixture modelling in SMT has received increasing attention ."},{"claim_score":-1.6139092,"evidence":"In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical translation models , taking IBM Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism .","evidence_score":0.13722302,"text":"In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical translation models , taking IBM Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism ."},{"claim_score":-0.77653957,"evidence_score":-0.18589451,"text":"These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence ."},{"claim_score":-1.258917,"evidence_score":-0.19050371,"text":"The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an English-Chinese task ."},{"claim_score":-1.8620428,"evidence":"In ( Civera and Juan , 2006 a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed .","evidence_score":0.18314891,"text":"In ( Civera and Juan , 2006 a mixture extension of IBM model 2 along with a specific dynamicprogramming decoding algorithm were proposed ."},{"claim_score":-0.79539057,"evidence_score":-0.30704684,"text":"This IBM-2 mixture model offers a significant gain in translation quality over the conventional IBM model 2 on a semi-synthetic task ."},{"claim_score":-1.3981268,"evidence_score":-0.052857339,"text":"In this work , we present a mixture extension of the well-known HMM alignment model first proposed in ( Vogel and others , 1996 ) and refined in ( Och and Ney , 2003 This model possesses appealing properties among which are worth mentioning , the simplicity of the first-order word alignment distribution that can be made independent of absolute positions while177 Proceedings of the Second Workshop on Statistical Machine Translation , pages 177180 , Prague , June 2007 ."},{"claim_score":-0.39372964,"evidence":"Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages , and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach .","evidence_score":0.22720386,"text":"Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of word alignment in European languages , and the efficient and exact computation of the E-step and Viterbi alignment by using a dynamic-programming approach ."},{"claim_score":-2.2213806,"evidence_score":-0.237714,"text":"These properties have made this model suitable for extensions ( Toutanova et al 2002 ) and integration in a phrase-based model ( Deng and Byrne , 2005 ) in the past ."},{"claim_score":-0.49616586,"evidence":"3 Mixture of HMM alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .","evidence_score":0.1109651,"text":"3 Mixture of HMM alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) ."},{"claim_score":-2.140465,"evidence_score":-0.53904825,"text":"In this work , a novel mixture version of the HMM alignment model was introduced ."},{"claim_score":-2.0299145,"evidence_score":-0.33166456,"text":"This model was employed to generate topic-dependent Viterbi alignments that were input into a state-of-the-art phrasebased system ."},{"claim_score":-0.18041335,"evidence_score":-0.15594032,"text":"The preliminary results reported on the English-Spanish partitions of the Europarl and News-Commentary corpora may raise some doubts about the applicability of mixture modelling to SMT , nonetheless in the advent of larger open-domain corpora , the idea behind topic-specific translation models seem to be more than appropriate , necessary ."},{"claim_score":-0.3692379,"evidence_score":-0.45617198,"text":"On the other hand , we are fully aware that indirectly assessing the quality of a model through a phrasebased system is a difficult task because of the different factors involved ( Ayan and Dorr , 2006 Finally , the main problem in mixture modelling is the linear growth of the set of parameters as the number of components increases ."},{"claim_score":-0.940821,"evidence_score":-0.52136545,"text":"In the HMM , and also in IBM models , this problem is aggravated because of the use of statistical dictionary entailing a large number of parameters ."},{"claim_score":-0.94870006,"evidence_score":-0.047796095,"text":"A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events ( Och and Ney , 2003 ; Zhao and Xing , 2006"}]}