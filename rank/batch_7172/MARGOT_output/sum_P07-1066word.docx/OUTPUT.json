{"document":[{"claim_score":-0.78596025,"evidence_score":-0.31042426,"text":"We propose a novel approach to crosslingual language model ( LM ) adaptation based on bilingual Latent Semantic Analysis ( bLSA A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training ."},{"claim_score":-0.71158585,"evidence_score":-0.044184657,"text":"Using the proposed bLSA framework crosslingual LM adaptation can be performed by , first , inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LM via marginal adaptation ."},{"claim_score":-1.0968483,"evidence_score":-0.66019767,"text":"The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language ."},{"claim_score":-0.99066938,"evidence":"On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .","evidence_score":0.05781665,"text":"On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM ."},{"claim_score":-0.87357473,"evidence_score":-0.26982336,"text":"Furthermore , the proposed approach consistently improved machine translation quality on both speech and text based adaptation ."},{"claim_score":-0.02745423,"evidence_score":-0.17979974,"text":"Language model adaptation is crucial to numerous speech and translation tasks as it enables higherlevel contextual information to be effectively incorporated into a background LM improving recognition or translation performance ."},{"claim_score":-0.97132592,"evidence_score":-0.4307601,"text":"One approach is 520 to employ Latent Semantic Analysis ( LSA ) to capture in-domain word unigram distributions which are then integrated into the background N-gram LM ."},{"claim_score":-1.3477196,"evidence_score":-0.26580008,"text":"This approach has been successfully applied in automatic speech recognition ( ASR Tam and Schultz , 2006 ) using the Latent Dirichlet Allocation ( LDA Blei et al 2003 The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution ."},{"claim_score":-0.81213378,"evidence_score":-0.48239132,"text":"For LM adaptation , the topic mixture weights are estimated based on in-domain adaptation text ( e.g. ASR hypotheses The adapted mixture weights are then used to interpolate a topicdependent unigram LM , which is finally integrated into the background N-gram LM using marginal adaptation ( Kneser et al 1997 ) In this paper , we propose a framework to perform LM adaptation across languages , enabling the adaptation of a LM from one language based on the adaptation text of another language ."},{"claim_score":-1.0908277,"evidence_score":-0.39215654,"text":"In statistical machine translation ( SMT one approach is to apply LM adaptation on the target language based on an initial translation of input references ( Kim and Khudanpur , 2003 ; Paulik et al 2005 This scheme is limited by the coverage of the translation model , and overall by the quality of translation ."},{"claim_score":-0.20070448,"evidence_score":-1.3869901,"text":"Since this approach only allows to apply LM adaptation after translation , available knowledge can not be applied to extend the coverage ."},{"claim_score":-1.4663989,"evidence_score":-0.42322678,"text":"We propose a bilingual LSA model ( bLSA ) for crosslingual LM adaptation that can be applied before translation ."},{"claim_score":-1.3155969,"evidence_score":-0.19768121,"text":"The bLSA model consists of two LSA models : one for each side of the language trained on parallel document corpora ."},{"claim_score":-1.2935557,"evidence":"The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 520527 , Prague , Czech Republic , June 2007 .","evidence_score":0.30159118,"text":"The key property of the bLSA model is that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 520527 , Prague , Czech Republic , June 2007 ."},{"claim_score":-0.37491931,"evidence_score":-0.12597744,"text":"Qc 2007 Association for Computational Linguistics the latent topic of the source and target LSA models can be assumed to be a one-to-one correspondence and thus share a common latent topic space since the training corpora consist of bilingual parallel data ."},{"claim_score":-1.0640513,"evidence_score":-0.53564283,"text":"For instance , say topic 10 of the Chinese LSA model is about politics ."},{"claim_score":-1.7806738,"evidence_score":-0.71335222,"text":"Then topic 10 of the English LSA model is set to also correspond to politics and so forth ."},{"claim_score":-1.0282735,"evidence_score":-0.048896538,"text":"During LM adaptation , we first infer the topic mixture weights from the source text using the source LSA model ."},{"claim_score":-1.2822669,"evidence_score":-0.30755995,"text":"Then we transfer the inferred mixture weights to the target LSA model and thus obtain the target LSA marginals ."},{"claim_score":-1.1789212,"evidence_score":-0.64619819,"text":"The challenge is to enforce the one-to-one topic correspon dence ."},{"claim_score":-1.3718826,"evidence_score":-0.61358207,"text":"Our proposal is to share common variationalASR hypoChinese ASRChinese > English SMT Chinese Ngram LMEnglish Ngram LM AdaptAdapt Topic distribution Chinese LSAEnglish LSAChinese textEnglish text ChineseEnglishParallel document corpusMT hypo Dirichlet posteriors over the topic mixture weights of a document pair in the LDA-style model ."},{"claim_score":0.575909,"evidence_score":-0.28339293,"claim":"the model searches for a common latent topic space in an unsupervised fashion , rather than to require manual interaction","text":"The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion , rather than to require manual interaction ."},{"claim_score":-0.60833503,"evidence_score":-0.89247231,"text":"Since the topic space is language independent , our approach supports topic transfer in multiple language pairs in O ( N ) where N is the number of languages ."},{"claim_score":-1.5739219,"evidence_score":-0.44484426,"text":"Related work includes the Bilingual Topic Admixture Model ( BiTAM ) for word alignment proposed by ( Zhao and Xing , 2006 Basically , the BiTAM model consists of topic-dependent transla tion lexicons modeling P r ( c | e where c , e and k denotes the source Chinese word , target English word and the topic index respectively ."},{"claim_score":-1.4589413,"evidence_score":-0.62936375,"text":"On the other hand , the bLSA framework models P r ( c | k ) and P r ( e | k ) which is different from the BiTAM model ."},{"claim_score":-0.96729987,"evidence_score":-0.60361246,"text":"By their different modeling nature , the bLSA model usually supports more topics than the BiTAM model ."},{"claim_score":-1.3376287,"evidence_score":-0.21629212,"text":"Another work by ( Kim and Khudanpur , 2004 ) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection ."},{"claim_score":-0.9460509,"evidence":"We organize the paper as follows : In Section 2 , we introduce the bLSA framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .","evidence_score":0.44754478,"text":"We organize the paper as follows : In Section 2 , we introduce the bLSA framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation ."},{"claim_score":-1.2207436,"evidence":"In Section 3 , we present the effect of LM adaptation on word perplexity , followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3 .","evidence_score":0.55832561,"text":"In Section 3 , we present the effect of LM adaptation on word perplexity , followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3 ."},{"claim_score":-0.69007674,"evidence_score":-0.12351756,"text":"Section 4 describes conclusions and fu Figure 1 : Topic transfer in bilingual LSA model ."},{"claim_score":-0.94324706,"evidence_score":-0.87398475,"text":"ture works ."},{"claim_score":-2.1392389,"evidence_score":-0.13049218,"text":"We proposed a bilingual latent semantic model for crosslingual LM adaptation in spoken language translation ."},{"claim_score":-1.313335,"evidence_score":-0.7352864,"text":"The bLSA model consists of a set of monolingual LSA models in which a one-to-one topic correspondence is enforced between the LSA models through the sharing of variational Dirichlet posteriors ."},{"claim_score":-1.6263513,"evidence_score":-0.55686064,"text":"Bootstrapping a LSA model for a new language can be performed rapidly with topic transfer from a well-trained LSA model of another language ."},{"claim_score":-1.2376217,"evidence_score":-0.20342721,"text":"We transfer the inferred topic distribution from the input source text to the target language effectively to obtain an in-domain target LSA marginals for LM adaptation ."},{"claim_score":-0.29264742,"evidence":"Results showed that our approach significantly reduces the word perplexity on the target language in both cases using ASR hypotheses and manual transcripts .","evidence_score":0.21702068,"text":"Results showed that our approach significantly reduces the word perplexity on the target language in both cases using ASR hypotheses and manual transcripts ."},{"claim_score":-1.4051631,"evidence_score":-0.77033294,"text":"Interestingly , the adaptation performance is not much affected when ASR hypotheses were used ."},{"claim_score":-0.2921872,"evidence":"We evaluated the adapted LM on SMT and found that the evaluation metrics are crucial to reflect the actual improvement in performance .","evidence_score":1.0293831,"text":"We evaluated the adapted LM on SMT and found that the evaluation metrics are crucial to reflect the actual improvement in performance ."},{"claim_score":-0.42368855,"evidence_score":-0.50295058,"text":"Future directions include the exploration of story-dependent LM adaptation with automatic story segmentation instead of show-dependent adaptation due to the possibility of multiple stories within a show ."},{"claim_score":-0.61055872,"evidence":"We will investigate the incorporation of monolingual documents for potentially better bilingual LSA modeling","evidence_score":0.014981935,"text":"We will investigate the incorporation of monolingual documents for potentially better bilingual LSA modeling"}]}