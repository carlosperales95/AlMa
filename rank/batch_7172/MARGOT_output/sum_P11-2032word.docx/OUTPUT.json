{"document":[{"claim_score":-1.2034712,"evidence_score":-0.19941727,"text":"In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization ( EM We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possible parameter values in finding the alignment distribution ."},{"claim_score":-0.81366039,"evidence":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .","evidence_score":0.40844196,"text":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."},{"claim_score":0.20322872,"evidence_score":-0.31653466,"claim":"the proposed method effectively addresses the well-known rare word problemsame time induces a much smaller dictionary of bilingual word-pairs","text":"We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs ."},{"claim_score":-1.0621005,"evidence_score":-0.46588064,"text":"Word alignment is a crucial early step in the training of most statistical machine translation ( SMT ) systems , in which the estimated alignments are used for constraining the set of candidates in phraseextraction ( Koehn et al 2003 ; Chiang , 2007 ; Galley et al 2006 State-of-the-art word alignment models , such as IBM Models ( Brown et al 1993 HMM ( Vogel et al 1996 and the jointly-trained symmetric HMM ( Liang et al 2006 contain a large number of parameters ( e.g word translation probabilities ) that need to be estimated in addition to the desired hidden alignment variables ."},{"claim_score":-0.43850076,"evidence_score":-0.20833264,"text":"The most common method of inference in such models is expectation-maximization ( EM Dempster et al 1977 ) or an approximation to EM when exact EM is intractable ."},{"claim_score":-0.71469571,"evidence_score":-0.68090275,"text":"However , being a maxi mization ( e.g maximum likelihood ( ML ) or maximum a posteriori ( MAP technique , EM is generally prone to local optima and overfitting ."},{"claim_score":-1.1219886,"evidence_score":-0.32047616,"text":"In essence , the alignment distribution obtained via EM takes into account only the most likely point in the parameter space , but does not consider contributions from other points ."},{"claim_score":-0.654889,"evidence":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .","evidence_score":0.68134977,"text":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."},{"claim_score":-0.026144042,"evidence_score":-0.34876644,"text":"Zhao and Xing note that the parameter estimation ( for which they use variational EM ) suffers from data sparsity and use symmetric Dirichlet priors , but they find the MAP solution ."},{"claim_score":-1.4220576,"evidence":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learning phrase alignments directly ( DeNero et al 2008 Word alignment learning problem was addressed jointly with segmentation learning in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling .","evidence_score":0.055574715,"text":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learning phrase alignments directly ( DeNero et al 2008 Word alignment learning problem was addressed jointly with segmentation learning in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling ."},{"claim_score":-0.84559131,"evidence_score":-0.64029255,"text":"However , alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA Xu et al 2008 ) or by local maximization ( Nguyen et al 2010 On the other hand ,182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182187 , Portland , Oregon , June 19-24 , 2011 ."},{"claim_score":-0.93610418,"evidence":"Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .","evidence_score":0.060755699,"text":"Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."},{"claim_score":-0.164774,"evidence_score":-0.4332809,"text":"They use variational Bayes for inference , but they do not investigate the effect of Bayesian inference to word alignment in isolation ."},{"claim_score":-0.13144222,"evidence":"Recently , Zhao and Gildea proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence .","evidence_score":0.1845619,"text":"Recently , Zhao and Gildea proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence ."},{"claim_score":-0.88610805,"evidence_score":-0.62815829,"text":"The set of alignments for a sentence ( corpus ) is denoted by a ( A The model parameters consist of a VEVF ta ble T of word translation probabilities such that te , f P ( f | e The joint distribution of the Model-1 variables is given by the following generative model3 : n approximate the expected counts in the E-step ."},{"claim_score":-1.3217117,"evidence_score":-0.091449643,"text":"Even though they report substantial reductions in align P ( E , F , A ; T P P ( a | e ) P ( f | a , e ; T 1 ) s J ment error rate , the translation BLEU scores do not improve ."},{"claim_score":-0.64987617,"evidence_score":-0.32694117,"text":"Our approach in this paper is fully Bayesian in n P ( I 1 ) J s n t j = 1eaj , fj which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive , sparse prior ."},{"claim_score":-0.93366416,"evidence":"We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which is relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment models , and many state-of-the n n ( te , f ) Ne , f n J art SMT systems use Model 1 translation probabil ities as features in their log-linear model .","evidence_score":0.0075819532,"text":"We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which is relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment models , and many state-of-the n n ( te , f ) Ne , f n J art SMT systems use Model 1 translation probabil ities as features in their log-linear model ."},{"claim_score":-1.2354252,"evidence":"We evale = 1 f = 1 ( I s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .","evidence_score":0.090446258,"text":"We evale = 1 f = 1 ( I s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."},{"claim_score":-0.69109501,"evidence_score":-0.20694197,"text":"To our knowledge , this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance ."},{"claim_score":-0.7686924,"evidence":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .","evidence_score":0.6315724,"text":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."},{"claim_score":-1.5089481,"evidence":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .","evidence_score":0.54509008,"text":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."},{"claim_score":-1.1853427,"evidence":"10 The GIZA implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .","evidence_score":0.46842484,"text":"10 The GIZA implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."},{"claim_score":-1.0888879,"evidence_score":-0.62062823,"text":"The proposed method learns a compact , sparse translation distribution , overcoming the wellknown garbage collection problem of rare words in EM-estimated current models"}]}