{"document":[{"claim_score":-1.5808891,"evidence_score":-0.30355806,"text":"In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation ( SMT ) but which have not been addressed satisfactorily by the SMT research community ."},{"claim_score":-0.89385607,"evidence_score":-0.62520956,"text":"Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT ."},{"claim_score":-0.73367962,"evidence_score":-0.35895497,"text":"Our work aims at providing useful insights into the the computational complexity of those problems ."},{"claim_score":-1.0975129,"evidence_score":-0.056857273,"text":"We prove that while IBM Models 1-2 are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 ) , ( Wang , 1997 ) , ( Germann et al. , 2003 ) , ( Udupa et al. , 2004 ) ."},{"claim_score":-0.14389738,"evidence_score":-0.7301704,"text":"The models are independent of the language pair and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training ."},{"claim_score":-2.0079087,"evidence_score":-0.3387704,"text":"Increasingly , parallel corpora are becoming available for many language pairs and SMT systems have been built for French-English , German-English , Arabic-English , Chinese-English , Hindi-English and other language pairs ( Brown et al. , 1993 ) , ( AlOnaizan et al. , 1999 ) , ( Udupa , 2004 ) ."},{"claim_score":-2.2761935,"evidence_score":-1.0809524,"text":"In SMT , every English sentence e is considered as a translation of a given French sentence f withprobability P r ( f | e ) ."},{"claim_score":-1.3243752,"evidence_score":-0.25558585,"text":"Therefore , the problem oftranslating f can be viewed as a problem of findingthe most probable translation of f : e ∗ = argmax P r ( e | f ) = argmax P r ( f | e ) P ( e ) ."},{"claim_score":-1.6519052,"evidence_score":-0.14046727,"text":"enomial time solution for any of these harde ( 1 ) problems ( unless P = NP and P #P = P ) , our results highlight and justify the need for developing polynomial time approximations for these computations ."},{"claim_score":-1.2225852,"evidence_score":-1.0679218,"text":"We also discuss some practical ways of dealing with complexity ."},{"claim_score":-1.1140531,"evidence_score":-0.99336032,"text":"Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic The probability distributions P r ( f | e ) and P r ( e ) are known as translation model and lan guage model respectively ."},{"claim_score":-1.2868691,"evidence":"In the classic work on SMT , Brown and his colleagues at IBM introduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models ( Brown et al. , 1993 ) .","evidence_score":0.29920355,"text":"In the classic work on SMT , Brown and his colleagues at IBM introduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models ( Brown et al. , 1993 ) ."},{"claim_score":-0.46056378,"evidence":"An alignment between f = f1 ... fm and e = e1 ... el is a many-to-one mapping a : -LCB- 1 , ... , m -RCB- → -LCB- 0 , ... , l -RCB- .","evidence_score":0.28176396,"text":"An alignment between f = f1 ... fm and e = e1 ... el is a many-to-one mapping a : -LCB- 1 , ... , m -RCB- → -LCB- 0 , ... , l -RCB- ."},{"claim_score":-1.4515265,"evidence_score":-0.46768352,"text":"Thus , an alignment a between f and e associates the french word fj to the English word 2 ."},{"claim_score":-1.9366988,"evidence_score":-0.21723923,"text":"The number of words of f mapped to ei by translation ( Brown et al. , 1993 ) , ( Al-Onaizan et al. , 1999 ) ."},{"claim_score":-1.7611735,"evidence_score":-0.43082159,"text":"The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm ( Brown et al. , 1993 ) ."},{"claim_score":-0.46576203,"evidence_score":-1.0253844,"text":"The models are then used to decode , i.e. translate texts from the source language to the target eaj a is called the fertility of ei and is denoted by φi ."},{"claim_score":-2.299841,"evidence_score":-0.56298497,"text":"Since P r ( f | e ) = a P r ( f , a | e ) , equation 1 can 1 In this paper , we use French and English as the prototypical examples of source and target languages respectively ."},{"claim_score":-0.97953102,"evidence_score":-0.79700008,"text":"2 e0 is a special word called the null word and is used to account for those words in f that are not connected by a to any of the words of e. be rewritten as follows : e ∗ = argmax ) P r ( f , a | e ) P r ( e ) ."},{"claim_score":-0.96577592,"evidence":"( 2 ) \u2022 Relaxed Decoding Given the model parameters and a sentence f , e determine the most probable translation and a alignment pair for f .","evidence_score":0.14243488,"text":"( 2 ) \u2022 Relaxed Decoding Given the model parameters and a sentence f , e determine the most probable translation and a alignment pair for f ."},{"claim_score":-1.0701216,"evidence_score":-0.23646279,"text":"Brown and his colleagues developed a series of 5 translation models which have become to be known in the field of machine translation as IBM models ."},{"claim_score":-2.2799438,"evidence_score":-0.37563434,"text":"For a detailed introduction to IBM translation models , please see ( Brown et al. , 1993 ) ."},{"claim_score":-0.80689216,"evidence_score":-0.59766997,"text":"In practice , models 3-5 are known to give good results and models 1-2 are used to seed the EM iterations of the higher models ."},{"claim_score":-2.221709,"evidence_score":-0.73066362,"text":"IBM model 3 is the prototypical translation model and it models P r ( f , a | e ) as follows : ( e ∗ , a ∗ ) = argmax P ( f , a | e ) P ( e ) ( e , a ) Viterbi Alignment computation finds applications not only in SMT but also in other areas of Natural Language Processing ( Wang , 1998 ) , ( Marcu , 2002 ) ."},{"claim_score":-2.1708242,"evidence_score":-0.28981315,"text":"Expectation Evaluation is the soul of parameter estimation ( Brown et al. , 1993 ) , ( Al-Onaizan et al. , 1999 ) ."},{"claim_score":-0.23574928,"evidence_score":-0.68954077,"text":"Conditional Probability computation is important in experimentally studying the concentration of the probability mass P ( f , a | e ) ≡ n ( φ0 | l \\ l i = 1 n ( φi | ei ) φi !"},{"claim_score":-1.3591113,"evidence_score":-0.050940492,"text":"around the Viterbi alignment , i.e. in determining j = 1 t fj | eaj × dj : aj I = 0 d ( j | i , m , l ) the goodness of the Viterbi alignment in compar ison to the rest of the alignments ."},{"claim_score":-1.0517705,"evidence_score":-0.23773234,"text":"Decoding is an integral component of all SMT systems ( Wang , Table 1 : IBM Model 3 Here , n ( φ | e ) is the fertility model , t ( f | e ) is the lexicon model and d ( j | i , m , l ) is the distortion model ."},{"claim_score":-0.77360048,"evidence_score":-0.23379796,"text":"The computational tasks involving IBM Models are the following : \u2022 Viterbi Alignment Given the model parameters and a sentence pair ( f , e ) , determine the most probable alignment between f and e. a ∗ = argmax P ( f , a | e ) a \u2022 Expectation Evaluation This forms the core of model training via the EM algorithm ."},{"claim_score":-1.4871855,"evidence_score":-0.23304703,"text":"Please see Section 2.3 for a description of the computational task involved in the EM iterations ."},{"claim_score":-2.2995588,"evidence_score":-0.37288436,"text":"\u2022 Conditional Probability Given the model parameters and a sentence pair ( f , e ) , compute P ( f | e ) ."},{"claim_score":-2.9486972,"evidence_score":-0.10728971,"text":"1997 ) , ( Tillman , 2000 ) , ( Och et al. , 2001 ) , ( Germann et al. , 2003 ) , ( Udupa et al. , 2004 ) ."},{"claim_score":-1.2374597,"evidence_score":-0.76078348,"text":"Exact Decoding is the original decoding problem as defined in ( Brown et al. , 1993 ) and Relaxed Decoding is the relaxation of the decoding problem typically used in practice ."},{"claim_score":-1.4431775,"evidence_score":-0.7992054,"text":"While several heuristics have been developed by practitioners of SMT for the computational tasks involving IBM models , not much is known about the computational complexity of these tasks ."},{"claim_score":-0.79898271,"evidence":"In their seminal paper on SMT , Brown and his colleagues highlighted the problems we face as we go from IBM Models 1-2 to 3-5 ( Brown et al. , 1993 ) 3 : `` As we progress from Model 1 to Model 5 , evaluating the expectations that gives us counts becomes increasingly difficult .","evidence_score":1.2114669,"text":"In their seminal paper on SMT , Brown and his colleagues highlighted the problems we face as we go from IBM Models 1-2 to 3-5 ( Brown et al. , 1993 ) 3 : `` As we progress from Model 1 to Model 5 , evaluating the expectations that gives us counts becomes increasingly difficult ."},{"claim_score":-0.81818718,"evidence_score":-0.51720914,"text":"In Models 3 and 4 , we must be content with approximate EM iterations because it is not feasible to carry out sums over all possible alignments for these models ."},{"claim_score":-0.4600355,"evidence":"In practice , we are never sure that we have found the Viterbi alignment '' .","evidence_score":0.59593324,"text":"In practice , we are never sure that we have found the Viterbi alignment '' ."},{"claim_score":-1.5802003,"evidence_score":-0.80180483,"text":"However , neither their work nor the subsequent P ( f | e ) = ) a \u2022 Exact Decoding P ( f , a | e ) research in SMT studied the computational complexity of these fundamental problems with the exception of the Decoding problem ."},{"claim_score":-1.0585506,"evidence":"In ( Knight , 1999 ) it was proved that the Exact Decoding prob Given the model parameters and a sentence f , determine the most probable translation of f .","evidence_score":0.58625873,"text":"In ( Knight , 1999 ) it was proved that the Exact Decoding prob Given the model parameters and a sentence f , determine the most probable translation of f ."},{"claim_score":-0.54187897,"evidence_score":-0.69801261,"text":"lem is NP-Hard when the language model is a bigram model ."},{"claim_score":-0.66581755,"evidence_score":-0.95725219,"text":"e ∗ = argmax e ) P ( f , a | e ) P ( e ) a Our results may be summarized as follows : 3 The emphasis is ours ."},{"claim_score":-2.4760022,"evidence":"1 .","evidence_score":1.0404366,"text":"1 ."},{"claim_score":-1.5128324,"evidence_score":-0.27383914,"text":"Viterbi Alignment computation is NP-Hard for IBM Models 3 , 4 , and 5 ."},{"claim_score":-2.2546666,"evidence":"2 .","evidence_score":0.88063411,"text":"2 ."},{"claim_score":-1.7462391,"evidence_score":-1.1788717,"text":"IBM models 3-5 are widely used in SMT ."},{"claim_score":-1.974557,"evidence_score":-1.3813783,"text":"The computational tasks discussed in this work form the backbone of all SMT systems that use IBM models ."},{"claim_score":-0.37130169,"evidence":"We believe that our results on the computational complexity of the tasks in SMT will result in a better understanding of these tasks from a theoretical perspective .","evidence_score":0.34797446,"text":"We believe that our results on the computational complexity of the tasks in SMT will result in a better understanding of these tasks from a theoretical perspective ."},{"claim_score":0.66234789,"evidence_score":-0.12081358,"claim":"our results may help in the design of effective heuristicsfor","text":"We also believe that our results may help in the design of effective heuristicsfor some of these tasks ."},{"claim_score":-0.75068457,"evidence_score":-0.55887478,"text":"A theoretical analysis of the commonly employed heuristics will also be of interest.An open question in SMT is whether there ex-ists closed form expressions ( whose representation is polynomial in the size of the input ) for P ( f | e ) and the counts in the EM iterations for models 3-5 ( Brown et al. , 1993 ) ."},{"claim_score":-1.5896861,"evidence":"For models 1-2 , closed formexpressions exist for P ( f | e ) and the counts in theEM iterations for models 3-5 .","evidence_score":0.10244697,"text":"For models 1-2 , closed formexpressions exist for P ( f | e ) and the counts in theEM iterations for models 3-5 ."},{"claim_score":-0.50333227,"evidence":"Our results showthat there can not exist a closed form expression ( whose representation is polynomial in the size of the input ) for P ( f | e ) and the counts in the EMiterations for Models 3-5 unless P = NP .","evidence_score":0.1216174,"text":"Our results showthat there can not exist a closed form expression ( whose representation is polynomial in the size of the input ) for P ( f | e ) and the counts in the EMiterations for Models 3-5 unless P = NP ."}]}