{"document":[{"claim_score":0.26609839,"evidence_score":-0.53007972,"claim":"topics are induced in an unsupervised way using topiccan be thought of as inducing subcorpora for adaptation without any human annotation","text":"We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation ."},{"claim_score":-1.0340334,"evidence_score":-0.15807633,"text":"We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features ."},{"claim_score":-0.69978507,"evidence":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .","evidence_score":0.30877683,"text":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."},{"claim_score":-1.1621767,"evidence_score":-0.46456932,"text":"The performance of a statistical machine translation ( SMT ) system on a translation task depends largely on the suitability of the available parallel training data ."},{"claim_score":-1.0148455,"evidence_score":-0.83305333,"text":"Domains ( e.g. , newswire vs. blogs ) may vary widely in their lexical choices and stylistic preferences , and what may be preferable in a general setting , or in one domain , is not necessarily preferable in another domain ."},{"claim_score":-0.80832398,"evidence_score":-0.94929886,"text":"Indeed , sometimes the domain can change the meaning of a phrase entirely ."},{"claim_score":-0.71034496,"evidence_score":-0.81868998,"text":"In a food related context , the Chinese sentence `` 粉 丝 很 多 '' ( `` feˇns ¯ i heˇnduo ¯ '' ) would mean `` They have a lot of vermicelli '' ; however , in an informal Internet conversation , this sentence would mean `` They have a lot of fans '' ."},{"claim_score":-0.19489659,"evidence_score":-0.27305627,"text":"Without the broader context , it is impossible to determine the correct translation in otherwise identical sentences ."},{"claim_score":-0.62694181,"evidence_score":-0.3056707,"text":"This problem has led to a substantial amount of recent work in trying to bias , or adapt , the translation model ( TM ) toward particular domains of interest ( Axelrod et al. , 2011 ; Foster et al. , 2010 ; Snover et al. , 2008 ) .1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation ."},{"claim_score":-1.0977388,"evidence":"Matsoukas et al. ( 2009 ) introduced assigning a pair of binary features to each training sentence , indicating sentences ' genre and collection as a way to capture domains .","evidence_score":0.40336123,"text":"Matsoukas et al. ( 2009 ) introduced assigning a pair of binary features to each training sentence , indicating sentences ' genre and collection as a way to capture domains ."},{"claim_score":-0.82693176,"evidence":"They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .","evidence_score":0.043442885,"text":"They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights ."},{"claim_score":-0.61676855,"evidence_score":-0.30269613,"text":"As sentence weights were found to be most beneficial for lexical weighting , Chiang et al. ( 2011 ) extends the same notion of conditioning on provenance ( i.e. , the origin of the text ) by removing the separate mapping step , directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature , estimated from only those sentences that comprise that genre or collection ."},{"claim_score":-0.41959379,"evidence_score":-0.83175055,"text":"The common thread throughout prior work is the concept of a domain ."},{"claim_score":-1.0467189,"evidence_score":-0.8338643,"text":"A domain is typically a hard constraint that is externally imposed and hand labeled , such as genre or corpus collection ."},{"claim_score":-1.5850947,"evidence_score":-1.1731192,"text":"For example , a sentence either comes from newswire , or weblog , but not both ."},{"claim_score":-1.3763009,"evidence_score":-1.3165343,"text":"However , this poses several problems ."},{"claim_score":-0.32865475,"evidence_score":-0.53727239,"text":"First , since a sentence contributes its counts only to the translation table for the source it came from , many word pairs will be unobserved for a given table ."},{"claim_score":-2.3109214,"evidence_score":-0.98758026,"text":"This sparsity requires smoothing ."},{"claim_score":-0.97288738,"evidence_score":-0.53155596,"text":"Second , we may not know the ( sub ) corpora our training 1 Language model adaptation is also prevalent but is not the focus of this work ."},{"claim_score":-2.053893,"evidence":"115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 115 -- 119 , Jeju , Republic of Korea , 8-14 July 2012 .","evidence_score":0.1191586,"text":"115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 115 -- 119 , Jeju , Republic of Korea , 8-14 July 2012 ."},{"claim_score":-0.19184177,"evidence":"Qc 2012 Association for Computational Linguistics data come from ; and even if we do , `` subcorpus '' may not be the most useful notion of domain for better translations .","evidence_score":0.6940625,"text":"Qc 2012 Association for Computational Linguistics data come from ; and even if we do , `` subcorpus '' may not be the most useful notion of domain for better translations ."},{"claim_score":-0.64077998,"evidence_score":-0.3900616,"text":"We take a finer-grained , flexible , unsupervised approach for lexical weighting by domain ."},{"claim_score":-1.0713496,"evidence":"We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model .","evidence_score":0.0065037827,"text":"We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model ."},{"claim_score":-0.22798492,"evidence_score":-0.09417789,"text":"Unsupervised modeling of the training data produces naturally occurring subcorpora , generalizing beyond corpus and genre ."},{"claim_score":-0.35286567,"evidence":"Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .","evidence_score":0.23639391,"text":"Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction ."},{"claim_score":-0.18766583,"evidence_score":-0.44190682,"text":"This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation ."},{"claim_score":-2.4045808,"evidence_score":-0.17946205,"text":"In this work , we consider the underlying latent topics of the documents ( Blei et al. , 2003 ) ."},{"claim_score":-1.8305649,"evidence_score":-0.12034235,"text":"Topic modeling has received some use in SMT , for instance Bilingual LSA adaptation ( Tam et al. , 2007 ) , and the BiTAM model ( Zhao and Xing , 2006 ) , which uses a bilingual topic model for learning alignment ."},{"claim_score":-0.84730833,"evidence":"In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .","evidence_score":0.79006548,"text":"In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership ."},{"claim_score":-1.4845616,"evidence_score":-0.29000254,"text":"This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics ."},{"claim_score":-2.2780109,"evidence_score":-0.56306342,"text":"We accomplish this by introducing topic dependent lexical probabilities directly as c ( f , e ) / 2 ."},{"claim_score":-1.6341447,"evidence_score":-0.79290859,"text":"e c ( f , e ) ."},{"claim_score":-2.3638223,"evidence_score":-0.53853617,"text":"Phrase pair probabilities p ( e | f ) are computed from these as described in Koehn et al. ( 2003 ) ."},{"claim_score":-1.0097031,"evidence":"Chiang et al. ( 2011 ) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , fs ( e | f ) , one for each domain s , which compute a new word translation table ps ( e | f ) esti mated from only those sentences which belong to s : cs ( f , e ) / 2 .","evidence_score":0.17165503,"text":"Chiang et al. ( 2011 ) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , fs ( e | f ) , one for each domain s , which compute a new word translation table ps ( e | f ) esti mated from only those sentences which belong to s : cs ( f , e ) / 2 ."},{"claim_score":-0.32561888,"evidence_score":-0.14060835,"text":"e cs ( f , e ) , where cs ( · ) is the number of occurrences of the word pair in s. Topic Modeling for MT We extend provenance to cover a set of automatically generated topics zn ."},{"claim_score":-1.7740956,"evidence_score":-0.2874362,"text":"Given a parallel training corpus T composed of documents di , we build a source side topic model over T , which provides a topic distribution p ( zn | di ) for zn = -LCB- 1 , ... , K -RCB- over each document , using Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) ."},{"claim_score":-0.71827651,"evidence_score":-0.42391615,"text":"Then , we assign p ( zn | di ) to be the topic distribution for every sentence xj ∈ di , thus enforcing topic sharing across sentence pairs in the same document instead of treating them as unrelated ."},{"claim_score":-1.08924,"evidence_score":-0.24643596,"text":"Computing the topic distribution over a document and assigning it to the sentences serves to tie the sentences together in the document context ."},{"claim_score":-0.98926877,"evidence_score":-0.45956136,"text":"To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f ) of a word pair under topic zn : features in the translation model , and interpolating them log-linearly with our other features , thus allow ezn ( e , f ) = p ( zn | di ) cj ( e , f ) ( 1 ) ing us to discriminatively optimize their weights on di ∈ T xj ∈ di an arbitrary objective function ."},{"claim_score":-0.66541181,"evidence_score":-0.28826658,"text":"Incorporating these features into our hierarchical phrase-based translation system significantly improved translation per where cj ( · ) denotes the number of occurrences of the word pair in sentence xj , and then compute : ezn ( e , f ) formance , by up to 1 BLEU and 3 TER over a strong Chinese to English baseline ."},{"claim_score":-2.2698718,"evidence_score":-0.26238183,"text":"pzn ( e | f ) = 2 ."},{"claim_score":-1.3113453,"evidence_score":-0.45805069,"text":"e ezn ( e , f ) ( 2 ) 2 Applying SMT to new domains requires techniques to inform our algorithms how best to adapt ."},{"claim_score":-1.6708331,"evidence_score":-0.30713169,"text":"This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion ."},{"claim_score":0.31338205,"evidence":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","evidence_score":0.43800079,"text":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","claim_evidence":"incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamicallysignificant performance gains"},{"claim_score":-1.347971,"evidence_score":-0.9402194,"text":"This method presents several advantages over existing approaches ."},{"claim_score":-1.0121336,"evidence":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .","evidence_score":0.14625465,"text":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model ."},{"claim_score":-0.85764029,"evidence_score":-0.78039501,"text":"We can also incorporate large quantities of additional data ( whether parallel or not ) in the source language to infer better topics without relying on collection or genre annotations ."},{"claim_score":-1.2525156,"evidence_score":-0.62061018,"text":"Multilingual topic models ( Boyd-Graber and Resnik , 2010 ) would provide a technique to use data from multiple languages to ensure consistent topics ."}]}