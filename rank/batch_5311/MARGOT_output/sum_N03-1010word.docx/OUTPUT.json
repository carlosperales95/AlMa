{"document":[{"claim_score":-0.9227228,"evidence":"We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied na ¨ ıvely ) to practically linear time1 without sacrificing translation quality .","evidence_score":0.38153842,"text":"We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied na ¨ ıvely ) to practically linear time1 without sacrificing translation quality ."},{"claim_score":-0.27441496,"evidence":"We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding .","evidence_score":0.0030233184,"text":"We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding ."},{"claim_score":-1.7246429,"evidence_score":-0.4313121,"text":"Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s ( Brown et al. , 1990 , 1993 ; Berger et al. , 1994 , 1996 ) ."},{"claim_score":-2.0799365,"evidence_score":-1.1117309,"text":"Based on the conventions established in Brown et al. ( 1993 ) , these models are commonly referred to as the ( IBM ) Models 1-5 ."},{"claim_score":-1.0369586,"evidence_score":-0.5354165,"text":"One of the big challenges in building actual MT systems within this framework is that of decoding : finding the translation candidate that maximizes the translation probability for the given input ."},{"claim_score":-0.85693753,"evidence_score":-0.47388615,"text":"Knight ( 1999 ) has shown the problem to be NP-complete ."},{"claim_score":-0.93067743,"evidence_score":-0.62513992,"text":"Due to the complexity of the task , practical MT systems usually do not employ optimal decoders ( that is , decoders that are guaranteed to find an optimal solution within the constraints of the framework ) , but rely on approximative algorithms instead ."},{"claim_score":0.81781931,"evidence_score":-0.55596426,"claim":"such algorithms can perform resonably well","text":"Empirical evidence suggests that such algorithms can perform resonably well ."},{"claim_score":-0.78191471,"evidence_score":-0.58023972,"text":"For example , Berger et al. ( 1994 ) , attribute only 5percent of the translation errors of their Candide system , which uses 1 Technically , the complexity is still ."},{"claim_score":0.26198559,"evidence_score":-0.49825098,"claim":"it does not have any noticable effect on the translation speed for all reasonable inputs","text":"However , the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs ."},{"claim_score":-1.3147273,"evidence_score":-0.3812575,"text":"a restricted stack search , to search errors ."},{"claim_score":-1.5208885,"evidence":"Using the same evaluation metric ( but different evaluation data ) , Wang and Waibel ( 1997 ) report search error rates of 7.9 percent and 9.3 percent , respectively , for their decoders .","evidence_score":0.051201343,"text":"Using the same evaluation metric ( but different evaluation data ) , Wang and Waibel ( 1997 ) report search error rates of 7.9 percent and 9.3 percent , respectively , for their decoders ."},{"claim_score":-2.1896027,"evidence":"Och et al. ( 2001 ) and Germann et al. ( 2001 ) both implemented optimal decoders and benchmarked approximative algorithms against them .","evidence_score":0.38175034,"text":"Och et al. ( 2001 ) and Germann et al. ( 2001 ) both implemented optimal decoders and benchmarked approximative algorithms against them ."},{"claim_score":-1.1188922,"evidence":"Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A * algorithm ) , and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 ) .","evidence_score":0.18043398,"text":"Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A * algorithm ) , and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 ) ."},{"claim_score":-1.3192116,"evidence_score":-0.070430496,"text":"Germann et al. ( 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 ) ."},{"claim_score":-1.1131428,"evidence_score":-0.84138611,"text":"Their overall performance metric is the sentence error rate ( SER ) ."},{"claim_score":-0.43532464,"evidence_score":-0.65796992,"text":"For decoding with IBM Model 3 , they report SERs of about 57percent ( 6-word sentences ) and 76percent ( 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper ."},{"claim_score":0.55157748,"evidence_score":-0.34424761,"claim":"approximative algorithms are a feasible choice for practical applications","text":"All these numbers suggest that approximative algorithms are a feasible choice for practical applications ."},{"claim_score":-1.2990004,"evidence_score":-0.81529911,"text":"The purpose of this paper is to describe speed improvements to the greedy decoder mentioned above ."},{"claim_score":-1.3375866,"evidence_score":-0.6859698,"text":"While acceptably fast for the kind of evaluation used in Germann et al. ( 2001 ) , namely sentences of up to 20 words , its speed becomes an issue for more realistic applications ."},{"claim_score":-1.2696475,"evidence_score":-0.2410196,"text":"Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , `` faster '' algorithm ( de scribed below ) ."},{"claim_score":0.011659802,"evidence":"We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .","evidence_score":0.064890397,"text":"We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .","claim_evidence":"this time can be reduced to ca. 40 minutes without sacrificing translation quality"},{"claim_score":-1.0353955,"evidence":"In the following , we first describe the underlying IBM initial string : I do not understand the logic of these people .","evidence_score":0.081752644,"text":"In the following , we first describe the underlying IBM initial string : I do not understand the logic of these people ."},{"claim_score":-1.2238454,"evidence_score":-0.43006213,"text":"pick fertilities : I not not understand the logic of these people ."},{"claim_score":-1.0564962,"evidence_score":-0.83589024,"text":"replace words : Je ne pas comprends la logique de ces gens ."},{"claim_score":-1.4448858,"evidence_score":-0.45113389,"text":"reorder : Je ne comprends pas la logique de ces gens ."},{"claim_score":-0.65207577,"evidence_score":-0.27435565,"text":"insert spurious words : Je ne comprends pas la logique de ces gens - la ` ."},{"claim_score":-1.6573258,"evidence_score":-0.065945444,"text":"Figure 1 : How the IBM models model the translation process ."},{"claim_score":-1.8423715,"evidence_score":-1.1288355,"text":"This is a hypothetical example and not taken from any actual training or decoding logs ."},{"claim_score":-2.1398406,"evidence_score":-0.96112872,"text":"model ( s ) of machine translation ( Section 2 ) and our hillclimbing algorithm ( Section 3 ) ."},{"claim_score":-0.65621977,"evidence":"In Section 4 , we discuss improvements to the algorithm and its implementation , and the effect of restrictions on word reordering .","evidence_score":0.24699716,"text":"In Section 4 , we discuss improvements to the algorithm and its implementation , and the effect of restrictions on word reordering ."},{"claim_score":-1.2167558,"evidence":"2 In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al. ( 2001 ) and presented improvements that drastically reduce the decoder 's complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word con-sidered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 ) .","evidence_score":0.11594093,"text":"2 In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al. ( 2001 ) and presented improvements that drastically reduce the decoder 's complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word con-sidered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 ) ."},{"claim_score":-0.87272239,"evidence_score":-0.058522963,"text":"The profiles shown are cumulative , so that the top curve reflects the total decoding time ."},{"claim_score":-1.00688,"evidence_score":-0.44508261,"text":"To put the times for decoding in perspective , the dashed line in the lower plot reflects the total decoding time in decoding ."},{"claim_score":-0.56878603,"evidence":"Operations not included in the figures consume so little time that their plots can not be discerned in the graphs .","evidence_score":0.1326544,"text":"Operations not included in the figures consume so little time that their plots can not be discerned in the graphs ."},{"claim_score":-2.0714592,"evidence":"The times shown are averages of 100 sentences each for length10 , 20 , , 80 .","evidence_score":0.31740742,"text":"The times shown are averages of 100 sentences each for length10 , 20 , , 80 ."},{"claim_score":-2.4967784,"evidence":"IBM Model 4 scores and the BLEU metric .","evidence_score":0.21729648,"text":"IBM Model 4 scores and the BLEU metric ."},{"claim_score":-0.60220886,"evidence":"The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6","evidence_score":0.026806951,"text":"The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6"}]}