{"document":[{"claim_score":0.26609839,"evidence_score":-0.53007972,"claim":"topics are induced in an unsupervised way using topiccan be thought of as inducing subcorpora for adaptation without any human annotation","text":"We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation ."},{"claim_score":-1.0340334,"evidence_score":-0.15807633,"text":"We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features ."},{"claim_score":-0.69978507,"evidence":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .","evidence_score":0.30877683,"text":"Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."},{"claim_score":-1.1621767,"evidence_score":-0.46456932,"text":"The performance of a statistical machine translation ( SMT ) system on a translation task depends largely on the suitability of the available parallel training data ."},{"claim_score":-1.0148455,"evidence_score":-0.83305333,"text":"Domains ( e.g. , newswire vs. blogs ) may vary widely in their lexical choices and stylistic preferences , and what may be preferable in a general setting , or in one domain , is not necessarily preferable in another domain ."},{"claim_score":-0.80832398,"evidence_score":-0.94929886,"text":"Indeed , sometimes the domain can change the meaning of a phrase entirely ."},{"claim_score":-0.45981166,"evidence_score":-0.72097514,"text":"In a food related context , the Chinese sentence ( fensi henduo would mean They have a lot of vermicelli ; however , in an informal Internet conversation , this sentence would mean They have a lot of fans ."},{"claim_score":-0.11262571,"evidence_score":-0.16417906,"text":"Without the broader context , it is impossible to determine the correct translation in otherwise identical sentences.This problem has led to a substantial amount of recent work in trying to bias , or adapt , the translation model ( TM ) toward particular domains of interest ( Axelrod et al. , 2011 ; Foster et al. , 2010 ; Snover et al. , 2008 ) .1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation ."},{"claim_score":-0.52855765,"evidence":"Matsoukas et al. introduced assigning a pair of binary features to each training sentence , indicating sentences genre and collection as a way to capture domains .","evidence_score":0.26555092,"text":"Matsoukas et al. introduced assigning a pair of binary features to each training sentence , indicating sentences genre and collection as a way to capture domains ."},{"claim_score":-0.82693176,"evidence":"They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .","evidence_score":0.043442885,"text":"They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights ."},{"claim_score":-0.49690474,"evidence_score":-0.32923798,"text":"As sentence weights were found to be most beneficial for lexical weighting , Chiang et al. extends the same notion of conditioning on provenance ( i.e. , the origin of the text ) by removing the separate mapping step , directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature , estimated from only those sentences that comprise that genre or collection ."},{"claim_score":-0.41959379,"evidence_score":-0.83175055,"text":"The common thread throughout prior work is the concept of a domain ."},{"claim_score":-1.0467189,"evidence_score":-0.8338643,"text":"A domain is typically a hard constraint that is externally imposed and hand labeled , such as genre or corpus collection ."},{"claim_score":-1.5850947,"evidence_score":-1.1731192,"text":"For example , a sentence either comes from newswire , or weblog , but not both ."},{"claim_score":-1.3763009,"evidence_score":-1.3165343,"text":"However , this poses several problems ."},{"claim_score":-0.32865475,"evidence_score":-0.53727239,"text":"First , since a sentence contributes its counts only to the translation table for the source it came from , many word pairs will be unobserved for a given table ."},{"claim_score":-2.3109214,"evidence_score":-0.98758026,"text":"This sparsity requires smoothing ."},{"claim_score":-1.3563647,"evidence_score":-0.049534323,"text":"Second , we may not know the ( sub ) corpora our training 1 Language model adaptation is also prevalent but is not the focus of this work .115 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 115119 , Jeju , Republic of Korea , 8-14 July 2012 ."},{"claim_score":-0.29039079,"evidence":"Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations .","evidence_score":0.7609634,"text":"Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations ."},{"claim_score":-0.64077998,"evidence_score":-0.3900616,"text":"We take a finer-grained , flexible , unsupervised approach for lexical weighting by domain ."},{"claim_score":-1.0713496,"evidence":"We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model .","evidence_score":0.0065037827,"text":"We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model ."},{"claim_score":-0.22798492,"evidence_score":-0.09417789,"text":"Unsupervised modeling of the training data produces naturally occurring subcorpora , generalizing beyond corpus and genre ."},{"claim_score":-0.35286567,"evidence":"Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .","evidence_score":0.23639391,"text":"Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction ."},{"claim_score":-0.18766583,"evidence_score":-0.44190682,"text":"This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation ."},{"claim_score":-2.4045808,"evidence_score":-0.17946205,"text":"In this work , we consider the underlying latent topics of the documents ( Blei et al. , 2003 ) ."},{"claim_score":-1.8305649,"evidence_score":-0.12034235,"text":"Topic modeling has received some use in SMT , for instance Bilingual LSA adaptation ( Tam et al. , 2007 ) , and the BiTAM model ( Zhao and Xing , 2006 ) , which uses a bilingual topic model for learning alignment ."},{"claim_score":-0.84730833,"evidence":"In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .","evidence_score":0.79006548,"text":"In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership ."},{"claim_score":-1.4845616,"evidence_score":-0.29000254,"text":"This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics ."},{"claim_score":-2.1257436,"evidence_score":-0.94575255,"text":"We accomplish this by introducing topic dependent lexical probabilities directly as c ( f , e ) c ( f , ) ."},{"claim_score":-1.6098527,"evidence_score":-0.25841033,"text":"Phrase pair probabilities p ( e | f are computed from these as described in Koehn et al. ."},{"claim_score":-0.36418566,"evidence_score":-0.20441853,"text":"Chiang et al. showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , fs ( e | f ) , one for each domain s , which compute a new word translation table ps ( e | f esti mated from only those sentences which belong to s : cs ( f , e ) cs ( f , ) , where cs ( ) is the number of occurrences of the word pair in s. Topic Modeling for MT We extend provenance to cover a set of automatically generated topics zn ."},{"claim_score":-1.2596356,"evidence_score":-0.37416584,"text":"Given a parallel training corpus T composed of documents di , we build a source side topic model over T , which provides a topic distribution p ( zn | di ) for zn = -LCB- 1 , ."},{"claim_score":-2.2358731,"evidence_score":-0.11810091,"text":", K -RCB- over each document , using Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) ."},{"claim_score":-0.62727848,"evidence_score":-0.39774304,"text":"Then , we assign p ( zn | di ) to be the topic distribution for every sentence xjdi , thus enforcing topic sharing across sentence pairs in the same document instead of treating them as unrelated ."},{"claim_score":-1.08924,"evidence_score":-0.24643596,"text":"Computing the topic distribution over a document and assigning it to the sentences serves to tie the sentences together in the document context ."},{"claim_score":-0.87992168,"evidence":"To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a word pair under topic zn : features in the translation model , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function .","evidence_score":0.1159573,"text":"To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a word pair under topic zn : features in the translation model , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function ."},{"claim_score":-0.82702238,"evidence_score":-0.36163114,"text":"Incorporating these features into our hierarchical phrase-based translation system significantly improved translation per where cj ( ) denotes the number of occurrences of the word pair in sentence xj , and then compute : ezn ( e , f formance , by up to 1 BLEU and 3 TER over a strong Chinese to English baseline ."},{"claim_score":-2.0004813,"evidence_score":-0.36192484,"text":"pzn ( e | f = 2 ."},{"claim_score":-1.330574,"evidence_score":-0.36202624,"text":"e ezn ( e , f Applying SMT to new domains requires techniques to inform our algorithms how best to adapt ."},{"claim_score":-1.6708331,"evidence_score":-0.30713169,"text":"This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion ."},{"claim_score":0.31338205,"evidence":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","evidence_score":0.43800079,"text":"We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .","claim_evidence":"incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamicallysignificant performance gains"},{"claim_score":-1.347971,"evidence_score":-0.9402194,"text":"This method presents several advantages over existing approaches ."},{"claim_score":-1.0121336,"evidence":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .","evidence_score":0.14625465,"text":"We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model ."},{"claim_score":-0.85764029,"evidence_score":-0.78039501,"text":"We can also incorporate large quantities of additional data ( whether parallel or not ) in the source language to infer better topics without relying on collection or genre annotations ."},{"claim_score":-1.2525156,"evidence_score":-0.62061018,"text":"Multilingual topic models ( Boyd-Graber and Resnik , 2010 ) would provide a technique to use data from multiple languages to ensure consistent topics ."}]}