{"document":[{"claim_score":-1.6209413,"evidence_score":-0.55875084,"text":"In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization ( EM ) ."},{"claim_score":-1.2878261,"evidence_score":-0.20315603,"text":"We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possible parameter values in finding the alignment distribution ."},{"claim_score":-0.81366039,"evidence":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .","evidence_score":0.40844196,"text":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."},{"claim_score":0.20322872,"evidence_score":-0.31653466,"claim":"the proposed method effectively addresses the well-known rare word problemsame time induces a much smaller dictionary of bilingual word-pairs","text":"We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs ."},{"claim_score":-0.75289774,"evidence_score":-0.65726568,"text":"Word alignment is a crucial early step in the training of most statistical machine translation ( SMT ) systems , in which the estimated alignments are used for constraining the set of candidates in phraseextraction ( Koehn et al. , 2003 ; Chiang , 2007 ; Galley et al. , 2006 ) ."},{"claim_score":-2.2789076,"evidence_score":-0.33285904,"text":"State-of-the-art word alignment models , such as IBM Models ( Brown et al. , 1993 ) , HMM ( Vogel et al. , 1996 ) , and the jointly-trained symmetric HMM ( Liang et al. , 2006 ) , contain a large number of parameters ( e.g. , word translation probabilities ) that need to be estimated in addition to the desired hidden alignment variables ."},{"claim_score":-0.75320983,"evidence_score":-0.40802502,"text":"The most common method of inference in such models is expectation-maximization ( EM ) ( Dempster et al. , 1977 ) or an approximation to EM when exact EM is intractable ."},{"claim_score":-0.75963717,"evidence_score":-0.57615294,"text":"However , being a maxi mization ( e.g. , maximum likelihood ( ML ) or maximum a posteriori ( MAP ) ) technique , EM is generally prone to local optima and overfitting ."},{"claim_score":-1.1219886,"evidence_score":-0.32047616,"text":"In essence , the alignment distribution obtained via EM takes into account only the most likely point in the parameter space , but does not consider contributions from other points ."},{"claim_score":-0.654889,"evidence":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .","evidence_score":0.68134977,"text":"Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."},{"claim_score":-0.026144042,"evidence_score":-0.34876644,"text":"Zhao and Xing note that the parameter estimation ( for which they use variational EM ) suffers from data sparsity and use symmetric Dirichlet priors , but they find the MAP solution ."},{"claim_score":-1.6233231,"evidence":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) .","evidence_score":0.40277048,"text":"Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) ."},{"claim_score":-1.6424992,"evidence":"Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. , Nguyen et al. , and Chung and Gildea .","evidence_score":0.2830717,"text":"Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. , Nguyen et al. , and Chung and Gildea ."},{"claim_score":-1.9193709,"evidence_score":-1.1383175,"text":"The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling ."},{"claim_score":-1.2249446,"evidence_score":-0.69792162,"text":"However , alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA + + ( Xu et al. , 2008 ) or by local maximization ( Nguyen et al. , 2010 ) ."},{"claim_score":-1.5533011,"evidence":"On the other hand ,182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182187 , Portland , Oregon , June 19-24 , 2011 .","evidence_score":0.42884064,"text":"On the other hand ,182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182187 , Portland , Oregon , June 19-24 , 2011 ."},{"claim_score":-0.93610418,"evidence":"Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .","evidence_score":0.060755699,"text":"Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."},{"claim_score":-0.164774,"evidence_score":-0.4332809,"text":"They use variational Bayes for inference , but they do not investigate the effect of Bayesian inference to word alignment in isolation ."},{"claim_score":-0.72318132,"evidence_score":-0.04973867,"text":"Recently , Zhao and Gildea proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM ) , a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence ."},{"claim_score":-1.3041212,"evidence_score":-1.2009664,"text":"The set of alignments for a sentence ( corpus ) is denoted by a ( A ) ."},{"claim_score":-1.5825537,"evidence":"The model parameters consist of a VEVF ta ble T of word translation probabilities such that te , f = P ( f | e ) .","evidence_score":0.0033278648,"text":"The model parameters consist of a VEVF ta ble T of word translation probabilities such that te , f = P ( f | e ) ."},{"claim_score":-0.75993804,"evidence_score":-0.28980835,"text":"The joint distribution of the Model-1 variables is given by the following generative model3 : n approximate the expected counts in the E-step ."},{"claim_score":-1.2528844,"evidence_score":-0.56516552,"text":"Even though they report substantial reductions in align P ( E , F , A ; T ) = P P ( a | e ) P ( f | a , e ; T ) s J ment error rate , the translation BLEU scores do not improve ."},{"claim_score":-0.6239522,"evidence_score":-0.32467746,"text":"Our approach in this paper is fully Bayesian in = n P ( I + 1 ) J s n t j = 1eaj , fj which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive , sparse prior ."},{"claim_score":-1.330747,"evidence_score":-0.14379568,"text":"We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T ) ."},{"claim_score":-1.3296877,"evidence_score":-0.32466322,"text":"To find a suitable prior for T , we re-write as : VE VF which is relevant for the state-of-the-art SMT sys tems since : Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T ) = n s P ( I + 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment models , and many state-of-the = n n ( te , f ) Ne , f n J art SMT systems use Model 1 translation probabil ities as features in their log-linear model ."},{"claim_score":-0.75895227,"evidence":"We evale = 1 f = 1 ( I + ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .","evidence_score":0.15996833,"text":"We evale = 1 f = 1 ( I + ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."},{"claim_score":-0.69109501,"evidence_score":-0.20694197,"text":"To our knowledge , this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance ."},{"claim_score":-0.7686924,"evidence":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .","evidence_score":0.6315724,"text":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."},{"claim_score":-1.5089481,"evidence":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .","evidence_score":0.54509008,"text":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."},{"claim_score":-0.89206491,"evidence":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .","evidence_score":0.43668007,"text":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."},{"claim_score":-1.3661963,"evidence_score":-0.59401457,"text":"The proposed method learns a compact , sparse translation distribution , overcoming the wellknown garbage collection problem of rare words in EM-estimated current models ."}]}