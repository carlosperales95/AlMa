[
    {
        "claim_score": -0.654889, 
        "evidence": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .", 
        "evidence_score": 0.68134977, 
        "text": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."
    }, 
    {
        "claim_score": -0.7686924, 
        "evidence": "We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .", 
        "evidence_score": 0.6315724, 
        "text": "We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."
    }, 
    {
        "claim_score": -1.5089481, 
        "evidence": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .", 
        "evidence_score": 0.54509008, 
        "text": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."
    }, 
    {
        "claim_score": -0.89206491, 
        "evidence": "10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .", 
        "evidence_score": 0.43668007, 
        "text": "10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."
    }, 
    {
        "claim_score": -1.5533011, 
        "evidence": "On the other hand ,182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182187 , Portland , Oregon , June 19-24 , 2011 .", 
        "evidence_score": 0.42884064, 
        "text": "On the other hand ,182 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : shortpapers , pages 182187 , Portland , Oregon , June 19-24 , 2011 ."
    }, 
    {
        "claim_score": -0.81366039, 
        "evidence": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .", 
        "evidence_score": 0.40844196, 
        "text": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."
    }, 
    {
        "claim_score": -1.6233231, 
        "evidence": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) .", 
        "evidence_score": 0.40277048, 
        "text": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al. , 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al. , 2009 ) and learning phrase alignments directly ( DeNero et al. , 2008 ) ."
    }, 
    {
        "claim_score": -1.6424992, 
        "evidence": "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. , Nguyen et al. , and Chung and Gildea .", 
        "evidence_score": 0.2830717, 
        "text": "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. , Nguyen et al. , and Chung and Gildea ."
    }, 
    {
        "claim_score": -0.75895227, 
        "evidence": "We evale = 1 f = 1 ( I + ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .", 
        "evidence_score": 0.15996833, 
        "text": "We evale = 1 f = 1 ( I + ) s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."
    }, 
    {
        "claim_score": -0.93610418, 
        "evidence": "Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .", 
        "evidence_score": 0.060755699, 
        "text": "Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."
    }, 
    {
        "claim_score": -1.5825537, 
        "evidence": "The model parameters consist of a VEVF ta ble T of word translation probabilities such that te , f = P ( f | e ) .", 
        "evidence_score": 0.0033278648, 
        "text": "The model parameters consist of a VEVF ta ble T of word translation probabilities such that te , f = P ( f | e ) ."
    }
]