{"document":[{"claim_score":-1.3309239,"evidence_score":-1.1230015,"text":"In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model , where each word type is constrained to belong to a single class ."},{"claim_score":0.049843344,"evidence_score":-0.056955279,"claim":"able to easily add multiple kinds of featuresthe latter from parallel corpora Using only context features , our system yields results comparable to state-of-the art , far better than a similar model without the one-class-per-type constraint","text":"By using a mixture model rather than a sequence model ( e.g HMM we are able to easily add multiple kinds of features , including those at both the type level ( morphology features ) and token level ( context and alignment features , the latter from parallel corpora Using only context features , our system yields results comparable to state-of-the art , far better than a similar model without the one-class-per-type constraint ."},{"claim_score":-0.12625004,"evidence_score":-0.062025776,"text":"Using the additional features provides added benefit , and our final system outperforms the best published results on most of the 25 corpora tested ."},{"claim_score":-1.589716,"evidence_score":-0.48165686,"text":"We have presented a Bayesian model for syntactic class induction that has two important properties ."},{"claim_score":-1.3291802,"evidence_score":-0.63279094,"text":"First , it is type-based , assigning the same class to every token of a word type ."},{"claim_score":-1.4475105,"evidence":"We have shown by the gold standard number of classes shown in morphology .","evidence_score":0.011385013,"text":"We have shown by the gold standard number of classes shown in morphology ."},{"claim_score":-0.96041141,"evidence_score":-0.73950789,"text":"ALIGNMENTS adds alignment features , reporting the average score across all possible choices of paired language and the scores under the best performing paired language ( in parens alone or with morphology features ."},{"claim_score":-1.8627855,"evidence_score":-0.77157085,"text":"standard tags in all cases ."},{"claim_score":-0.96114122,"evidence":"k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus dueits size .","evidence_score":0.040647957,"text":"k-means and SVD2 models could not produce a clustering in the Czech CoNLL corpus dueits size ."},{"claim_score":-0.82479274,"evidence":"Best published results are from Christodoulopoulos et al 2010 Berg-Kirkpatrick et al 2010 ) and Leeet al 2010 The latter two papers do not report VM scores .","evidence_score":0.83628947,"text":"Best published results are from Christodoulopoulos et al 2010 Berg-Kirkpatrick et al 2010 ) and Leeet al 2010 The latter two papers do not report VM scores ."},{"claim_score":0.90602821,"evidence_score":-0.21535653,"claim":"No best published results are shown for theclark performs bestthis restriction is very helpful","text":"No best published results are shown for the MULTEXT languages ; Christodoulopoulos et al 2010 ) report results based on 45 tags suggesting that clark performs best on these corpora.comparison with a token-based version of the model that this restriction is very helpful ."},{"claim_score":-0.75194645,"evidence_score":-0.96077103,"text":"Second , it is a clustering model rather than a sequence model ."},{"claim_score":-1.3489687,"evidence_score":-0.69405976,"text":"This property makes it easy to incorporate multiple kinds of features into the model at either the token or the type level ."},{"claim_score":-0.19692637,"evidence":"Here , we experimented with token-level context features and alignment features and type-level morphology features , showing that morphology features are helpful in nearly all cases , and alignment features can be helpful if the aligned language is properly chosen .","evidence_score":0.073811948,"text":"Here , we experimented with token-level context features and alignment features and type-level morphology features , showing that morphology features are helpful in nearly all cases , and alignment features can be helpful if the aligned language is properly chosen ."},{"claim_score":-0.76110043,"evidence_score":-0.15967238,"text":"Our results even without these extra features are competitive with stateof-the-art ; with the additional features we achieve the best published results in the majority of the 25 corpora tested ."},{"claim_score":-0.49208112,"evidence_score":-0.75611752,"text":"Since it is so easy to add extra features to our model , one direction for future work is to explore other possible features ."},{"claim_score":-1.018586,"evidence_score":-0.64217373,"text":"For example , it could be useful to add dependency features from an unsupervised dependency parser ."},{"claim_score":-1.0148902,"evidence_score":-1.2594485,"text":"We are also interested in improving our morphology features , either by considering other ways to extract features during preprocessing ( for example , including prefixes or not concatenating together all suffixes or by developing a joint model for inducing both morphology and syntactic classes simultaneously ."},{"claim_score":-1.0479712,"evidence_score":-0.80895428,"text":"Finally , our model could be extended by replacing the standard mixture model with an infinite mixture model ( Rasmussen ,2000 ) in order to induce the number of syntactic classes automatically"}]}