{"document":[{"claim_score":0.69166641,"evidence_score":-0.25163295,"claim":"web counts can be used to approximate bigram frequencies , and thus should be useful for a wide variety of NLP tasks","text":"Previous work demonstrated that web counts can be used to approximate bigram frequencies , and thus should be useful for a wide variety of NLP tasks ."},{"claim_score":-1.1796882,"evidence_score":-0.49072134,"text":"So far , only two generation tasks ( candidate selection for machine translation and confusion-set disambiguation ) have been tested using web-scale data sets ."},{"claim_score":-0.88890861,"evidence_score":-0.39770519,"text":"The present paper investigates if these results generalize to tasks covering both syntax and semantics , both generation and analysis , and a larger range of n-grams ."},{"claim_score":0.28068486,"evidence":"For the majority of tasks , we fi nd that simple , unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus .","evidence_score":0.061330178,"text":"For the majority of tasks , we fi nd that simple , unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus .","claim_evidence":"models perform better when n-gram frequencies are obtained from the web rather than from a large corpus"},{"claim_score":-0.97650367,"evidence_score":-0.20386548,"text":"However , in most cases , web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora ."},{"claim_score":0.1234567,"evidence":"We argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard models . .","evidence_score":0.21147363,"text":"We argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard models . .","claim_evidence":"web-based models should therefore be used as a baseline for"},{"claim_score":-0.23322097,"evidence":"We showed that simple , unsupervised models using web counts can be devised for a variety of NLP tasks .","evidence_score":0.5544492,"text":"We showed that simple , unsupervised models using web counts can be devised for a variety of NLP tasks ."},{"claim_score":-1.1583892,"evidence":"The tasks were selected so that they cover both syntax and semantics , both generation and analysis , and a wider range of n-grams than have been previously used .","evidence_score":0.048048528,"text":"The tasks were selected so that they cover both syntax and semantics , both generation and analysis , and a wider range of n-grams than have been previously used ."},{"claim_score":-0.53840179,"evidence":"For all but two tasks ( candidate selection for MT and noun countability detection ) we found that simple , unsupervised models perform signifi cantly better when ngram frequencies are obtained from the web rather than from a standard large corpus .","evidence_score":0.15322821,"text":"For all but two tasks ( candidate selection for MT and noun countability detection ) we found that simple , unsupervised models perform signifi cantly better when ngram frequencies are obtained from the web rather than from a standard large corpus ."},{"claim_score":-0.21512841,"evidence":"This result is consistent with Keller and Lapatas fi ndings that the web yields better counts than the BNC .","evidence_score":0.087395549,"text":"This result is consistent with Keller and Lapatas fi ndings that the web yields better counts than the BNC ."},{"claim_score":-0.28003794,"evidence_score":-0.2020712,"text":"The reason for this seems to be that the web is much larger than the BNC ( about 1000 times the size seems to compensate for the fact that simple heuristics were used to obtain web counts , and for the noise inherent in web data ."},{"claim_score":-1.3565709,"evidence":"Our results were less encouraging when it comes to comparisons with state-of-the-art models .","evidence_score":0.18108433,"text":"Our results were less encouraging when it comes to comparisons with state-of-the-art models ."},{"claim_score":-0.30724423,"evidence":"We found that in all but one case , web-based models fail to signifi cantly outperform the state of the art .","evidence_score":0.78273428,"text":"We found that in all but one case , web-based models fail to signifi cantly outperform the state of the art ."},{"claim_score":-2.1260144,"evidence_score":-0.19274161,"text":"The exception was compound noun interpretation , for which the Altavista model was signifi cantly better than the Lauers model ."},{"claim_score":-0.51459584,"evidence":"For three tasks ( candidate selection for MT , adjective ordering , and compound noun bracketing we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature .","evidence_score":0.17266691,"text":"For three tasks ( candidate selection for MT , adjective ordering , and compound noun bracketing we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature ."},{"claim_score":0.63798411,"evidence":"Note that for all the tasks we investigated , the best performance in the literature was obtained by supervised models that have access not only to simple bigram or trigram frequencies , but also to linguistic information such as part-of-speech tags , semantic restrictions , or context ( or a thesaurus , in the case of Lauers models When unsupervised web-based models are compared against supervised methods that employ a wide variety of features , we observe that having access to linguistic information makes up for the lack of vast amounts of data .","evidence_score":7.6697411E-4,"text":"Note that for all the tasks we investigated , the best performance in the literature was obtained by supervised models that have access not only to simple bigram or trigram frequencies , but also to linguistic information such as part-of-speech tags , semantic restrictions , or context ( or a thesaurus , in the case of Lauers models When unsupervised web-based models are compared against supervised methods that employ a wide variety of features , we observe that having access to linguistic information makes up for the lack of vast amounts of data .","claim_evidence":"Note that for all the tasksthe best performance in the literature was obtained by supervised models that have access not only to simple bigramunsupervised web-based models are compared against supervised methods that employ a wide variety of featureshaving access to linguistic information makes up for the lack of vast amounts of data"},{"claim_score":0.49158791,"evidence_score":-0.34645707,"claim":"Our results therefore indicate that large data setsthey are claimed to beweb-based models should be used as a new baseline for NLP tasks","text":"Our results therefore indicate that large data sets such as those obtained from the web are not the panacea that they are claimed to be ( at least implicitly ) by authors such as Grefenstette and Keller and Lapata ( 2003 Rather , in our opinion , web-based models should be used as a new baseline for NLP tasks ."},{"claim_score":-0.80798837,"evidence_score":-0.76513024,"text":"The web baseline indicates how much can be achieved with a simple , unsupervised model based on n-grams with access to a huge data set ."},{"claim_score":-0.31730037,"evidence_score":-0.35821123,"text":"This baseline is more realistic than baselines obtained from standard corpora ; it is generally harder to beat , as our comparisons with the BNC baseline throughout this paper have shown ."},{"claim_score":0.78874812,"evidence_score":-0.12960608,"claim":"training data can be avoided","text":"Note that for certain tasks , the performance of a web baseline model might actually be suffi cient , so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided ."},{"claim_score":-0.54521652,"evidence_score":-0.91351358,"text":"Another possibility that needs further investigation is the combination of web-based models with supervised methods ."},{"claim_score":-0.56555057,"evidence_score":-1.3050608,"text":"This can be done with ensemble learning methods or simply by using web-based frequencies ( or probabilities ) as features ( in addition to linguistically motivated features ) to train supervised classifi ers"}]}