{"document":[{"claim_score":-1.1554081,"evidence_score":-0.77503866,"text":"We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation : word-based models and character-based models ."},{"claim_score":-0.52888721,"evidence_score":-0.28324883,"text":"We show that , in spite of similar performance overall , the two models produce different distribution of segmentation errors , in a way that can be explained by theoretical properties of the two models ."},{"claim_score":-0.64344334,"evidence_score":-0.44831901,"text":"The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter ."},{"claim_score":-1.9460045,"evidence_score":-0.74462834,"text":"A Bootstrap Aggregating model is proposed ."},{"claim_score":-1.2988032,"evidence_score":-0.45430293,"text":"By letting multiple segmenters vote , our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff ."},{"claim_score":-0.29460694,"evidence":"The experiments highlight the fundamental difference between word-based and character-based models , which enlighten us to design new models .","evidence_score":0.18109728,"text":"The experiments highlight the fundamental difference between word-based and character-based models , which enlighten us to design new models ."},{"claim_score":0.42869565,"evidence_score":-0.48253263,"claim":"the theoretical differences cause different error distribution.The two approaches are either based on a particular view of segmentation","text":"The above analysis indicates that the theoretical differences cause different error distribution.The two approaches are either based on a particular view of segmentation ."},{"claim_score":-1.4741504,"evidence":"Our analysis points out several drawbacks of each one .","evidence_score":0.089615536,"text":"Our analysis points out several drawbacks of each one ."},{"claim_score":-0.78180997,"evidence_score":-0.87284834,"text":"It may be helpful for both models to overcome their shortcomings ."},{"claim_score":-0.5793839,"evidence_score":-0.96700327,"text":"For example , one weakness of word-based model is its word induction ability which is partially caused by its neglect of internal structure of words ."},{"claim_score":0.0085443935,"evidence_score":-1.1133216,"claim":"A word-based model may be improvedthere is still space for improvement","text":"A word-based model may be improved by solving this problem .5 System CombinationThe error analysis also suggests that there is still space for improvement , just by combining the two existing models ."},{"claim_score":-0.63827357,"evidence_score":-0.081123223,"text":"Here , we introduce a classifier ensemble method for system combination .5.1 Upper Bound of System CombinationTo get an upper bound of the improvement that can be obtained by combining the strengths of each model , we have performed an oracle experiment ."},{"claim_score":-0.39923456,"evidence":"We think the optimal combination system should choose the right prediction when the two segmenters do not agree with each other .","evidence_score":0.27618391,"text":"We think the optimal combination system should choose the right prediction when the two segmenters do not agree with each other ."},{"claim_score":-0.59702637,"evidence_score":-0.72606939,"text":"There is a gold segmenter that generates gold-standard segmentation results ."},{"claim_score":-1.2939323,"evidence":"In the oracle experiment , we let the three segmenters , i.e. baseline segmenters and the gold segmenter , vote .","evidence_score":0.28897863,"text":"In the oracle experiment , we let the three segmenters , i.e. baseline segmenters and the gold segmenter , vote ."},{"claim_score":-1.2616444,"evidence_score":-0.13899402,"text":"The three segmenters output three segmentation results , which are further transformed into IOB2 representation ( Ramshaw and Marcus , 1995 Namely , each character has three B or I labels ."},{"claim_score":-2.2004957,"evidence_score":-0.34561966,"text":"We assign each character an oracle label which is chosn by at least two segmenters ."},{"claim_score":0.34160078,"evidence_score":-0.14076853,"claim":"When the baseline segmenters are agreethe gold segmenter can not change the segmentation whether it is right or wrong","text":"When the baseline segmenters are agree with each other , the gold segmenter can not change the segmentation whether it is right or wrong ."},{"claim_score":-0.19053733,"evidence_score":-0.23500965,"text":"In the situation that the two baseline segmenters disagree , the vote given by the gold segmenter will decide the right prediction ."},{"claim_score":-1.5927553,"evidence_score":-0.49231588,"text":"This kind of optimal performance is presented in Tab .4 ."},{"claim_score":-1.5974945,"evidence":"Compared these results with Tab .","evidence_score":0.280862,"text":"Compared these results with Tab ."},{"claim_score":-0.29907564,"evidence":"1 , we see a significant increase in accuracy for the four data sets .","evidence_score":0.36088033,"text":"1 , we see a significant increase in accuracy for the four data sets ."},{"claim_score":0.41170815,"evidence_score":-0.37750559,"claim":"The upper bound of error reduction with system combination is overis a machine learning ensemble meta-algorithm to improve classificationrate is a comparison between the F-score","text":"The upper bound of error reduction with system combination is over 30percent .5.2 Our ModelBootstrap aggregating ( Bagging ) is a machine learning ensemble meta-algorithm to improve classification and regression models in terms of reduction ( ER ) rate is a comparison between the F-score produced by the oracle combination system and the character-based system ( see Tab ."},{"claim_score":-2.2163114,"evidence_score":-0.12749769,"text":"1 ) ."},{"claim_score":-0.46888572,"evidence_score":-0.16590358,"text":"stability and classification accuracy ( Breiman ,1996 It also reduces variance and helps to avoid overfitting ."},{"claim_score":-0.43466515,"evidence_score":-0.55260552,"text":"Given a training set D of size n , Bagging generates m new training sets Di of sizentn , by sampling examples from D uniformly.The m models are fitted using the above m bootstrap samples and combined by voting ( for classification ) or averaging the output ( for regression We propose a Bagging model to combine multiple segmentation systems ."},{"claim_score":-0.57774612,"evidence":"In the training phase , given a training set D of size n , our model generates m new training sets Di of size 63.2 percentn bysampling examples from D without replacement.Namely no example will be repeated in each Di Each Di is separately used to train a word-based segmenter and a character-based segmenter .","evidence_score":0.039569578,"text":"In the training phase , given a training set D of size n , our model generates m new training sets Di of size 63.2 percentn bysampling examples from D without replacement.Namely no example will be repeated in each Di Each Di is separately used to train a word-based segmenter and a character-based segmenter ."},{"claim_score":-0.71811884,"evidence_score":-0.32485587,"text":"Using this strategy , we can get 2m weak segmenters ."},{"claim_score":-0.037757305,"evidence_score":-0.19930823,"text":"Note that the sampling strategy is different from the standard one ."},{"claim_score":0.63352247,"evidence_score":-0.24840754,"claim":"there is no significant difference between the two sampling strategies in terms of accuracy","text":"Our experiment shows that there is no significant difference between the two sampling strategies in terms of accuracy ."},{"claim_score":-0.92629967,"evidence_score":-0.87804165,"text":"However , the non-placement strategy is more efficient ."},{"claim_score":-0.55625555,"evidence_score":-0.54180868,"text":"In the segmentation phase , the 2m models outputs2m segmentation results , which are further transformed into IOB2 representation ."},{"claim_score":-0.9448528,"evidence_score":-1.0780343,"text":"In other words , each character has 2m B or I labels ."},{"claim_score":-1.3404813,"evidence_score":-1.0604056,"text":"The final segmentation is the voting result of these 2m labels ."},{"claim_score":0.95544713,"evidence_score":-0.86772652,"claim":"since 2m is an even numberthere may be equal number","text":"Note that since 2m is an even number , there may be equal number of B and I labels ."},{"claim_score":-1.4907756,"evidence":"In this case , our system prefer B to reduce error propagation .5.3 ResultsFig .","evidence_score":0.25444839,"text":"In this case , our system prefer B to reduce error propagation .5.3 ResultsFig ."},{"claim_score":-0.83241375,"evidence_score":-0.36841985,"text":"4 shows the influence of m in the bagging algorithm ."},{"claim_score":-0.31073333,"evidence_score":-0.30549688,"text":"Because each new data set Di in bagging algorithm is generated by a random procedure , the performance of all bagging experiments are not the same ."},{"claim_score":-1.2830987,"evidence_score":-0.11252584,"text":"To give a more stable evaluation , we repeat 5 experiments for each m and show the previous work and of our combination model.Tab ."},{"claim_score":-1.9823077,"evidence_score":-0.41092478,"text":"5 summarizes the performance of our final ent numbers of sampling data sets ."},{"claim_score":-0.1971349,"evidence_score":-0.19041836,"text":"Characterbagging means that the bagging system built on the single character-based segmenter ."},{"claim_score":-1.5560638,"evidence_score":-1.1943939,"text":"Wordbagging is named in the same way ."},{"claim_score":-1.0520292,"evidence":"Fig. 3 shows the precision , recall , F-score of the two baseline systems and our final system for which we generate m 15 new data sets for bagging .","evidence_score":0.34181578,"text":"Fig. 3 shows the precision , recall , F-score of the two baseline systems and our final system for which we generate m 15 new data sets for bagging ."},{"claim_score":-0.98875821,"evidence_score":-0.4822118,"text":"We can see significant improvements on the four datasets in terms of the balanced Fscore ."},{"claim_score":-0.15299148,"evidence_score":-0.21426283,"text":"The improvement of precision and recall are not consistent ."},{"claim_score":-0.50256447,"evidence_score":-0.3157904,"text":"The improvement of AS and CU datasets is from the recall improvement ; the improvement of PKU datasets is from the precision improvement ."},{"claim_score":0.35939295,"evidence":"We think the different performance is mainly because the four datasets are annotated by using different standards.We have presented a thorough study of the difference between word-based and character-based segmentation approaches for Chinese .","evidence_score":0.3195523,"text":"We think the different performance is mainly because the four datasets are annotated by using different standards.We have presented a thorough study of the difference between word-based and character-based segmentation approaches for Chinese .","claim_evidence":"We think the different performance is mainlydatasets are annotated by using different standards.We have presented a thorough study of the difference between word-based and character-based segmentation approaches for Chinese"},{"claim_score":-0.76094865,"evidence_score":-0.75372427,"text":"The theoretical and empirical analysis provides insights leading to better models ."},{"claim_score":-1.9772285,"evidence_score":-1.0585255,"text":"The strengths and weaknesses of the two methods are not exactly the same ."},{"claim_score":-0.77712132,"evidence_score":-0.59339506,"text":"To exploit their complementary strengths , we propose a Bagging model for system combination ."},{"claim_score":1.5064693,"evidence":"Experiments show that the combination strategy is helpful","evidence_score":0.29365296,"text":"Experiments show that the combination strategy is helpful","claim_evidence":"the combination strategy is helpful"}]}