[
    {
        "claim_evidence": "The entire language is modeledall the information on words in their context is inherently present", 
        "claim_score": 0.96823402, 
        "evidence": "The entire language is modeled , which means that all the information on words in their context is inherently present .", 
        "evidence_score": 0.17344572, 
        "text": "The entire language is modeled , which means that all the information on words in their context is inherently present ."
    }, 
    {
        "claim_evidence": "Preliminary results show that the performance of the generic classifier approach is only", 
        "claim_score": 0.78632363, 
        "evidence": "Preliminary results show that the performance of the generic classifier approach is only slightly worse that that of the specific classifier approach .", 
        "evidence_score": 0.47946708, 
        "text": "Preliminary results show that the performance of the generic classifier approach is only slightly worse that that of the specific classifier approach ."
    }, 
    {
        "claim": "it is easy to add new sets of confusibles without retraining", 
        "claim_score": 0.42199581, 
        "evidence_score": -0.35124, 
        "text": "The main advantage of using language models as generic classifiers is that it is easy to add new sets of confusibles without retraining or adding additional classifiers ."
    }, 
    {
        "claim_evidence": "smoothing techniques could be investigated to reduce the impact of zero probabilitythe training data we are currently working with is not enough to properly describe the language", 
        "claim_score": 0.19788894, 
        "evidence": "In particular , instead of back-off , smoothing techniques could be investigated to reduce the impact of zero probability problems ( Chen and Goodman ,1996 This assumes that the training data we are currently working with is not enough to properly describe the language .", 
        "evidence_score": 0.6361502, 
        "text": "In particular , instead of back-off , smoothing techniques could be investigated to reduce the impact of zero probability problems ( Chen and Goodman ,1996 This assumes that the training data we are currently working with is not enough to properly describe the language ."
    }, 
    {
        "claim": "text is typically tackled using specifically trained machine learning classifiers", 
        "claim_score": 0.14948248, 
        "evidence_score": -0.67114948, 
        "text": "The problem of identifying and correcting confusibles , i.e. context-sensitive spelling errors , in text is typically tackled using specifically trained machine learning classifiers ."
    }
]