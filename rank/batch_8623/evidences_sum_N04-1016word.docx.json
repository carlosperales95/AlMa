[
    {
        "claim_score": -0.30724423, 
        "evidence": "We found that in all but one case , web-based models fail to signifi cantly outperform the state of the art .", 
        "evidence_score": 0.78273428, 
        "text": "We found that in all but one case , web-based models fail to signifi cantly outperform the state of the art ."
    }, 
    {
        "claim_score": -0.23322097, 
        "evidence": "We showed that simple , unsupervised models using web counts can be devised for a variety of NLP tasks .", 
        "evidence_score": 0.5544492, 
        "text": "We showed that simple , unsupervised models using web counts can be devised for a variety of NLP tasks ."
    }, 
    {
        "claim_evidence": "web-based models should therefore be used as a baseline for", 
        "claim_score": 0.1234567, 
        "evidence": "We argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard models . .", 
        "evidence_score": 0.21147363, 
        "text": "We argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard models . ."
    }, 
    {
        "claim_score": -1.3565709, 
        "evidence": "Our results were less encouraging when it comes to comparisons with state-of-the-art models .", 
        "evidence_score": 0.18108433, 
        "text": "Our results were less encouraging when it comes to comparisons with state-of-the-art models ."
    }, 
    {
        "claim_score": -0.51459584, 
        "evidence": "For three tasks ( candidate selection for MT , adjective ordering , and compound noun bracketing we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature .", 
        "evidence_score": 0.17266691, 
        "text": "For three tasks ( candidate selection for MT , adjective ordering , and compound noun bracketing we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature ."
    }, 
    {
        "claim_score": -0.53840179, 
        "evidence": "For all but two tasks ( candidate selection for MT and noun countability detection ) we found that simple , unsupervised models perform signifi cantly better when ngram frequencies are obtained from the web rather than from a standard large corpus .", 
        "evidence_score": 0.15322821, 
        "text": "For all but two tasks ( candidate selection for MT and noun countability detection ) we found that simple , unsupervised models perform signifi cantly better when ngram frequencies are obtained from the web rather than from a standard large corpus ."
    }, 
    {
        "claim_score": -0.21512841, 
        "evidence": "This result is consistent with Keller and Lapatas fi ndings that the web yields better counts than the BNC .", 
        "evidence_score": 0.087395549, 
        "text": "This result is consistent with Keller and Lapatas fi ndings that the web yields better counts than the BNC ."
    }, 
    {
        "claim_evidence": "models perform better when n-gram frequencies are obtained from the web rather than from a large corpus", 
        "claim_score": 0.28068486, 
        "evidence": "For the majority of tasks , we fi nd that simple , unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus .", 
        "evidence_score": 0.061330178, 
        "text": "For the majority of tasks , we fi nd that simple , unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus ."
    }, 
    {
        "claim_score": -1.1583892, 
        "evidence": "The tasks were selected so that they cover both syntax and semantics , both generation and analysis , and a wider range of n-grams than have been previously used .", 
        "evidence_score": 0.048048528, 
        "text": "The tasks were selected so that they cover both syntax and semantics , both generation and analysis , and a wider range of n-grams than have been previously used ."
    }, 
    {
        "claim_evidence": "Note that for all the tasksthe best performance in the literature was obtained by supervised models that have access not only to simple bigramunsupervised web-based models are compared against supervised methods that employ a wide variety of featureshaving access to linguistic information makes up for the lack of vast amounts of data", 
        "claim_score": 0.63798411, 
        "evidence": "Note that for all the tasks we investigated , the best performance in the literature was obtained by supervised models that have access not only to simple bigram or trigram frequencies , but also to linguistic information such as part-of-speech tags , semantic restrictions , or context ( or a thesaurus , in the case of Lauers models When unsupervised web-based models are compared against supervised methods that employ a wide variety of features , we observe that having access to linguistic information makes up for the lack of vast amounts of data .", 
        "evidence_score": 0.00076697411, 
        "text": "Note that for all the tasks we investigated , the best performance in the literature was obtained by supervised models that have access not only to simple bigram or trigram frequencies , but also to linguistic information such as part-of-speech tags , semantic restrictions , or context ( or a thesaurus , in the case of Lauers models When unsupervised web-based models are compared against supervised methods that employ a wide variety of features , we observe that having access to linguistic information makes up for the lack of vast amounts of data ."
    }
]