{"document":[{"claim_score":-1.6209413,"evidence_score":-0.55875084,"text":"In this work , we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization ( EM ) ."},{"claim_score":-1.2878261,"evidence_score":-0.20315603,"text":"We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1 , integrating over all possible parameter values in finding the alignment distribution ."},{"claim_score":-0.81366039,"evidence":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .","evidence_score":0.40844196,"text":"We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."},{"claim_score":0.20322872,"evidence_score":-0.31653466,"claim":"the proposed method effectively addresses the well-known rare word problemsame time induces a much smaller dictionary of bilingual word-pairs","text":"We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs ."},{"claim_score":-0.7686924,"evidence":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .","evidence_score":0.6315724,"text":"We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."},{"claim_score":-1.5089481,"evidence":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .","evidence_score":0.54509008,"text":"As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."},{"claim_score":-0.89206491,"evidence":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .","evidence_score":0.43668007,"text":"10 The GIZA + + implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."},{"claim_score":-1.2423505,"evidence_score":-0.51189171,"text":"The proposed method learns a compact , sparse translation distribution , overcoming the wellknown `` garbage collection '' problem of rare words in EM-estimated current models ."}]}