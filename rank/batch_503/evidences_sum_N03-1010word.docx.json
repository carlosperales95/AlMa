[
    {
        "claim_score": -0.63627755, 
        "evidence": "In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time .", 
        "evidence_score": 0.3384229, 
        "text": "In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time ."
    }, 
    {
        "claim_score": -0.21901931, 
        "evidence": "Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .", 
        "evidence_score": 0.32595825, 
        "text": "Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality ."
    }, 
    {
        "claim_score": -0.86431589, 
        "evidence": "We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality .", 
        "evidence_score": 0.29107837, 
        "text": "We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality ."
    }, 
    {
        "claim_score": -1.724494, 
        "evidence": "Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rates of 7.9 percent and 9.3 respectively , for their decoders .", 
        "evidence_score": 0.28861677, 
        "text": "Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rates of 7.9 percent and 9.3 respectively , for their decoders ."
    }, 
    {
        "claim_score": -2.4967784, 
        "evidence": "IBM Model 4 scores and the BLEU metric .", 
        "evidence_score": 0.21729648, 
        "text": "IBM Model 4 scores and the BLEU metric ."
    }, 
    {
        "claim_score": -2.1868477, 
        "evidence": "The times shown are averages of 100 sentences each for length10 , 20 80 .", 
        "evidence_score": 0.1711314, 
        "text": "The times shown are averages of 100 sentences each for length10 , 20 80 ."
    }, 
    {
        "claim_score": -0.56878603, 
        "evidence": "Operations not included in the figures consume so little time that their plots can not be discerned in the graphs .", 
        "evidence_score": 0.1326544, 
        "text": "Operations not included in the figures consume so little time that their plots can not be discerned in the graphs ."
    }, 
    {
        "claim_score": -0.83057472, 
        "evidence": "The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6 .", 
        "evidence_score": 0.030558678, 
        "text": "The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6 ."
    }, 
    {
        "claim_score": -0.27441496, 
        "evidence": "We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding .", 
        "evidence_score": 0.0030233184, 
        "text": "We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding ."
    }, 
    {
        "claim_score": -0.13681385, 
        "evidence": "Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with IBM Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper .", 
        "evidence_score": 0.00179677, 
        "text": "Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with IBM Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper ."
    }
]