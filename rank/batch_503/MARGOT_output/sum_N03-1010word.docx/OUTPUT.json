{"document":[{"claim_score":-0.86431589,"evidence":"We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality .","evidence_score":0.29107837,"text":"We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality ."},{"claim_score":-0.27441496,"evidence":"We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding .","evidence_score":0.0030233184,"text":"We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding ."},{"claim_score":-1.6226858,"evidence_score":-0.42763039,"text":"Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s ( Brown et al 1990 , 1993 ; Berger et al 1994 , 1996 Based on the conventions established in Brown et al 1993 these models are commonly referred to as the ( IBM ) Models 1-5 ."},{"claim_score":-0.40797466,"evidence_score":-0.31799029,"text":"One of the big challenges in building actual MT systems within this framework is that of decoding : finding the translation candidate that maximizes the translation probabilityfor the given input Knight has shown the problem to be NP-complete ."},{"claim_score":-0.68297584,"evidence_score":-0.5253694,"text":"Due to the complexity of the task , practical MT systems usually do not employ optimal decoders ( that is , decoders that are guaranteed to find an optimal solution within the constraints of the framework but rely on approximative algorithms instead ."},{"claim_score":0.81781931,"evidence_score":-0.55596426,"claim":"such algorithms can perform resonably well","text":"Empirical evidence suggests that such algorithms can perform resonably well ."},{"claim_score":-0.48110461,"evidence_score":-0.83476584,"text":"For example , Berger et al 1994 attribute only 5percent of the translation errors of their Candide system , which uses 1 Technically , the complexity is still ."},{"claim_score":0.26198559,"evidence_score":-0.49825098,"claim":"it does not have any noticable effect on the translation speed for all reasonable inputs","text":"However , the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs ."},{"claim_score":-1.3147273,"evidence_score":-0.3812575,"text":"a restricted stack search , to search errors ."},{"claim_score":-1.724494,"evidence":"Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rates of 7.9 percent and 9.3 respectively , for their decoders .","evidence_score":0.28861677,"text":"Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rates of 7.9 percent and 9.3 respectively , for their decoders ."},{"claim_score":-1.4983945,"evidence_score":-0.37889233,"text":"Och et al 2001 ) and Germann et al 2001 ) both implemented optimal decoders and benchmarked approximative algorithms against them ."},{"claim_score":-0.13681385,"evidence":"Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with IBM Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper .","evidence_score":0.00179677,"text":"Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with IBM Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper ."},{"claim_score":0.55157748,"evidence_score":-0.34424761,"claim":"approximative algorithms are a feasible choice for practical applications","text":"All these numbers suggest that approximative algorithms are a feasible choice for practical applications ."},{"claim_score":-1.2990004,"evidence_score":-0.81529911,"text":"The purpose of this paper is to describe speed improvements to the greedy decoder mentioned above ."},{"claim_score":-1.0793298,"evidence_score":-0.61375096,"text":"While acceptably fast for the kind of evaluation used in Germann et al 2001 namely sentences of up to 20 words , its speed becomes an issue for more realistic applications ."},{"claim_score":-0.21901931,"evidence":"Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .","evidence_score":0.32595825,"text":"Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality ."},{"claim_score":-0.78933597,"evidence_score":-0.12511479,"text":"In the following , we first describe the underlying IBMinitial string : I do not understandthelogicofthesepeople pick fertilities : I not not understandthelogicofthesepeople replace words : Jene pas comprends la logiquede cesgens ."},{"claim_score":-0.62558867,"evidence_score":-0.0013368473,"text":"reorder : Je ne comprends pas la logique de ces gens insert spurious words : Je ne comprends pas la logique de ces gens - la Figure 1 : How the IBM models model the translation process ."},{"claim_score":-1.0914758,"evidence_score":-0.33548393,"text":"This is a hypothetical example and not taken from any actual training or decoding logs.model ( s ) of machine translation ( Section ) and our hillclimbing algorithm ( Section 3 In Section 4 , we discuss improvements to the algorithm and its implementation , and the effect of restrictions on word reordering ."},{"claim_score":-0.63627755,"evidence":"In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time .","evidence_score":0.3384229,"text":"In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.Experimental data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time ."},{"claim_score":-1.00688,"evidence_score":-0.44508261,"text":"To put the times for decoding in perspective , the dashed line in the lower plot reflects the total decoding time in decoding ."},{"claim_score":-0.56878603,"evidence":"Operations not included in the figures consume so little time that their plots can not be discerned in the graphs .","evidence_score":0.1326544,"text":"Operations not included in the figures consume so little time that their plots can not be discerned in the graphs ."},{"claim_score":-2.1868477,"evidence":"The times shown are averages of 100 sentences each for length10 , 20 80 .","evidence_score":0.1711314,"text":"The times shown are averages of 100 sentences each for length10 , 20 80 ."},{"claim_score":-2.4967784,"evidence":"IBM Model 4 scores and the BLEU metric .","evidence_score":0.21729648,"text":"IBM Model 4 scores and the BLEU metric ."},{"claim_score":-0.83057472,"evidence":"The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6 .","evidence_score":0.030558678,"text":"The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for machine translation with IBM Model 4.6 ."}]}