[
    {
        "claim_score": -0.654889, 
        "evidence": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported .", 
        "evidence_score": 0.68134977, 
        "text": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore and a number of heuristic changes to the estimation procedure , such as smoothing the parameter estimates , were shown to reduce the alignment error rate , but the effects on translation performance was not reported ."
    }, 
    {
        "claim_score": -0.7686924, 
        "evidence": "We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains .", 
        "evidence_score": 0.6315724, 
        "text": "We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs , data sizes and domains ."
    }, 
    {
        "claim_score": -1.5089481, 
        "evidence": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus .", 
        "evidence_score": 0.54509008, 
        "text": "As a result of this increase , Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM9 Each target word was aligned to the source candidate that co-occured the most number of times with that target word in the entire parallel corpus ."
    }, 
    {
        "claim_score": -1.1853427, 
        "evidence": "10 The GIZA implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 .", 
        "evidence_score": 0.46842484, 
        "text": "10 The GIZA implementation of Model 4 artificially limits fertility parameter values to at most nine.Model 4 ."
    }, 
    {
        "claim_score": -0.81366039, 
        "evidence": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points .", 
        "evidence_score": 0.40844196, 
        "text": "We show that Bayesian inference outperforms EM in all of the tested language pairs , domains and data set sizes , by up to 2.99 BLEU points ."
    }, 
    {
        "claim_score": -0.13144222, 
        "evidence": "Recently , Zhao and Gildea proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence .", 
        "evidence_score": 0.1845619, 
        "text": "Recently , Zhao and Gildea proposed fertility extensions to IBM Model 1 and HMM , but they do not place any prior on the parameters and their inference method is actually stochastic EM ( also known as Monte Carlo EM a ML technique in which sampling is used to fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence ."
    }, 
    {
        "claim_score": -1.2354252, 
        "evidence": "We evale = 1 f = 1 ( I s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique .", 
        "evidence_score": 0.090446258, 
        "text": "We evale = 1 f = 1 ( I s uate the inferred alignments in terms of the end-toend translation performance , where we show the results with a variety of input data to illustrate the general applicability of the proposed technique ."
    }, 
    {
        "claim_score": -0.93610418, 
        "evidence": "Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting .", 
        "evidence_score": 0.060755699, 
        "text": "Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting ."
    }, 
    {
        "claim_score": -1.4220576, 
        "evidence": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learning phrase alignments directly ( DeNero et al 2008 Word alignment learning problem was addressed jointly with segmentation learning in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling .", 
        "evidence_score": 0.055574715, 
        "text": "Bayesian inference , the approach in this paper , have recently been applied to several unsupervised learning problems in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other tasks in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learning phrase alignments directly ( DeNero et al 2008 Word alignment learning problem was addressed jointly with segmentation learning in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two works place nonparametric priors ( also known as cache models ) on the parameters and utilize Gibbs sampling ."
    }, 
    {
        "claim_score": -0.93366416, 
        "evidence": "We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which is relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment models , and many state-of-the n n ( te , f ) Ne , f n J art SMT systems use Model 1 translation probabil ities as features in their log-linear model .", 
        "evidence_score": 0.0075819532, 
        "text": "We develop a Gibbs sampler for alignments under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which is relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 is used in bootstrapping the parameter settings for EM training of higher P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment models , and many state-of-the n n ( te , f ) Ne , f n J art SMT systems use Model 1 translation probabil ities as features in their log-linear model ."
    }
]