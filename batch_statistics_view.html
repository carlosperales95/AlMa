<html>
    <head>
      <style type="text/css">

        p {
          font-size: 14px;
        }
        .column {
            float: left;
            width: 50%;
            padding:0%;
            margin:0%;

        }

        .row {
          width:100%;
          padding:0%;
          margin:0%;

        }

        img {

          max-width: 100%;
        }

        .row:after {
            content: "";
            display: table;
            clear: both;
        }
        .collapsible {
          background-color: #66ccff;
          color: #444;
          cursor: pointer;
          padding: 18px;
          width: 100%;
          border: none;
          text-align: left;
          outline: none;
          font-size: 15px;
        }

        .active, .collapsible:hover {
          background-color: #ccc;
        }

        .content {
          padding: 0 18px;
          display: none;
          overflow: hidden;
          background-color: #f1f1f1;
          width: 100%;
          padding:0%;
          margin:0%;

        }

        #subcollapsible {
          background-color: #4bab81 !important;

        }

        #subsubcollapsible {
          background-color: #81a118 !important;

        }
      </style>
    </head>
    <body>

      <button type="button" class="collapsible">Results for the query (QUERY)</button>
      <div class="content">
        <div class="row">
          <div class="column" name="paper_titles">
							<p> (1) - ﻿Semantic Role Features for Machine Translation
							</p>
							<p> (2) - ﻿Computational Complexity of Statistical Machine  Translation
							</p>
							<p> (3) - ﻿An Algorithmic Framework for the Decoding Problem in
 Statistical Machine Translation
							</p>
							<p> (4) - ﻿Greedy Decoding for Statistical Machine Translation in Almost Linear Time
							</p>
							<p> (5) - ﻿Refined Lexicon Models for Statistical Machine Translation using a
 Maximum Entropy Approach
							</p>
							<p> (6) - ﻿Toward hierarchical models for statistical machine translation of
 inflected languages
 							</p>
							<p> (7) - ﻿The RWTH System for Statistical Translation of Spoken
 Dialogues
							</p>
							<p> (8) - ﻿Enriching Entity Translation  Discovery using Selective Temporality
							</p>
							<p> (9) - ﻿Fusion of Multiple Features and Ranking SVM for
 Web-based English-Chinese OOV Term Translation
							</p>
							<p> (10) - ﻿Mining Name Translations from Entity Graph Mapping∗
							</p>
							<p> (11) - ﻿Cross-lingual WSD for Translation Extraction
 from Comparable Corpora
 							</p>
							<p> (12) - ﻿Name-aware Machine Translation

 Haibo Li†	Jing Zheng‡	Heng Ji†	Qi Li†	Wen Wang‡
							</p>
							<p> (13) - ﻿Revisiting Context-based Projection Methods for
 Term-Translation Spotting in Comparable Corpora
							</p>
							<p> (14) - ﻿
 NICT-ATR Speech-to-Speech Translation System
							</p>
							<p> (15) - ﻿Improved Statistical Machine Translation by Multiple Chinese Word
 Segmentation
							</p>
							<p> (16) - ﻿Bayesian Word Alignment for Statistical Machine Translation
							</p>
							<p> (17) - ﻿Topic Models for Dynamic Translation Model Adaptation
							</p>
							<p> (18) - ﻿Domain Adaptation in Statistical Machine Translation with Mixture
 Modelling ∗
							</p>
							<p> (19) - ﻿Cache-based Document-level Statistical Machine Translation
							</p>
							<p> (20) - ﻿Bilingual-LSA Based LM Adaptation for Spoken Language Translation
							</p>
							<p> (21) - ﻿Bilingual Word Embeddings for Phrase-Based Machine Translation
							</p>
							<p> (22) - ﻿Japanese Named Entity Recognition
 Using Structural Natural Language Processing
							</p>
							<p> (23) - ﻿The Karlsruhe Institute of Technology Translation Systems
 for the WMT 2011
							</p>
							<p> (24) - ﻿Non-parametric Bayesian Segmentation of Japanese Noun Phrases
							</p>
							<p> (25) - ﻿A Class-Based Agreement Model for
 Generating Accurately Inflected Translations
							</p>
</div>
          <div class="column">
            <button type="button" class="collapsible">Technology Methods</button>
            <div class="content" name="mentions">
							<p> 
							</p>
							<p>  							</p>
							<p> ['BLEU', 4.600000000000001]
							</p>
							<p>  							</p>
							<p> ['SMT', 4.600000000000001]
							</p>
							<p>  							</p>
							<p> [['suggestive', 'evidence'], 4.4]
							</p>
							<p>  							</p>
							<p> [['human', 'mind'], 4.4]
							</p>
							<p>  							</p>
							<p> [['frequent', 'phenomenon'], 4.4]
							</p>
							<p>  							</p>
							<p> [['additional', 'difficulty'], 4.4]
							</p>
							<p>  							</p>
							<p> [['following', 'contribution'], 4.4]
							</p>
							<p>  							</p>
							<p> [['significant', 'improvement'], 4.4]
							</p>
							<p>  							</p>
							<p> ['IBM', 4.000000000000001]
							</p>
							<p>  							</p>
							<p> ['English', 3.400000000000001]
							</p>
							<p>  							</p>
							<p> ['CWS', 3.400000000000001]
							</p>
							<p>  							</p>
							<p> [['significant', 'gain'], 3.0]
							</p>
							<p>  							</p>
							<p> [['good', 'association', 'measure'], 3.0]
							</p>
							<p>  							</p>
							<p> [['algorithmic', 'handle', 'provide'], 3.0]
							</p>
							<p>  							</p>
							<p> [['model', 'search'], 3.0]
							</p>
							<p>  							</p>
							<p> [['dynamic', 'cache'], 3.0]
							</p>
							<p>  							</p>
							<p> [['cache-based', 'approach'], 3.0]
							</p>
							<p>  							</p>
							<p> [['propose', 'method'], 3.0]
							</p>
							<p>  							</p>
							<p> [['statistical', 'translation'], 3.0]
							</p>
							<p>  							</p>
							<p> [['low', 'frequency'], 3.0]
							</p>
							<p>  							</p>
							<p> [['underlying', 'latent', 'topic'], 3.0]
							</p>
							<p>  							</p>
							<p> [['unsupervised', 'domain'], 3.0]
							</p>
							<p>  							</p>
							<p> [['word', 'pair'], 3.0]
							</p>
							<p>  							</p>
							<p> [['adapted'], 3.0]
							</p>
							<p>  							</p>
							<p> [['low', 'F-score'], 3.0]
							</p>
							<p>  							</p>
							<p> [['selective', 'use'], 3.0]
							</p>
							<p>  							</p>
							<p> [['nonparametric', 'Bayesian', 'language', 'model'], 3.0]
							</p>
							<p>  							</p>
							<p> [['non-parametric', 'Bayesian', 'language', 'model'], 3.0]
							</p>
							<p>  							</p>
							<p> [['Viterbi', 'alignment'], 3.0]
							</p>
							<p>  							</p>
							<p> [['Gibbs', 'sampler'], 3.0]
							</p>
							<p>  							</p>
							<p> ['language pair', 2.1999999999999997]
							</p>
							<p>  							</p>
							<p> ['Morin', 2.1999999999999997]
							</p>
							<p>  							</p>
							<p> ['NER', 2.1999999999999997]
							</p>
							<p>  							</p>
							<p> ['Hillary Clinton', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['LSA', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['word alignment', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['HMM', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['USA', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['Annual Meeting', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['CRF', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['OOV', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['machine translation', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['Ney', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['TSP', 1.9999999999999998]
							</p>
							<p>  							</p>
							<p> ['Models 1-2', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Tsujii', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['syntactic information', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['generative model', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['suggestive evidence', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['word embeddings', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['correct sense', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['cross-lingual Word Sense Disambiguation', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['WSD', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Word Sense Induction', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['WSI', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['SgVerb', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['good selection', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Comparative', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['system show', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['TREC', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Edinburgh', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['translation model', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Empirical', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Language Processing', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['word perplexity', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Republic', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Machine Translation', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['word segmentation', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Experimental', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['cross-lingual', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['speech recognition', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['language model', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Bayesian', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['Waibel', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> ['suboptimal solution', 1.7999999999999998]
							</p>
							<p>  							</p>
							<p> [['log-linear', 'combination'], 1.6]
							</p>
							<p>  							</p>
							<p> [['sense', 'cluster'], 1.6]
							</p>
							<p>  							</p>
							<p> [['related', 'work'], 1.6]
							</p>
							<p>  							</p>
							<p> [['famous', 'female', 'leader'], 1.6]
							</p>
							<p>  							</p>
							<p> [['such', 'algorithm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['approximative', 'algorithm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['narrow', 'verbal', 'paradigm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['initial', 'sentence'], 1.6]
							</p>
							<p>  							</p>
							<p> [['receive', 'increase', 'attention'], 1.6]
							</p>
							<p>  							</p>
							<p> [['standard', 'technique'], 1.6]
							</p>
							<p>  							</p>
							<p> [['first', 'step'], 1.6]
							</p>
							<p>  							</p>
							<p> [['incorporate', 'lexical', 'weighting', 'feature', 'condition'], 1.6]
							</p>
							<p>  							</p>
							<p> [['unsupervised', 'way', 'use', 'topic', 'model'], 1.6]
							</p>
							<p>  							</p>
							<p> [['acquirement', 'ability'], 1.6]
							</p>
							<p>  							</p>
							<p> [['arbitrary', 'objective', 'function'], 1.6]
							</p>
							<p>  							</p>
							<p> [['topic', 'model'], 1.6]
							</p>
							<p>  							</p>
							<p> [['T-component', 'mixture'], 1.6]
							</p>
							<p>  							</p>
							<p> [['graph', 'alignment', 'algorithm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['bLSA', 'framework'], 1.6]
							</p>
							<p>  							</p>
							<p> [['multiple', 'translation', 'model', 'generate'], 1.6]
							</p>
							<p>  							</p>
							<p> [['new', 'approach'], 1.6]
							</p>
							<p>  							</p>
							<p> [['equivalence', 'class'], 1.6]
							</p>
							<p>  							</p>
							<p> [['high', 'F-measure'], 1.6]
							</p>
							<p>  							</p>
							<p> [['name-aware', 'framework'], 1.6]
							</p>
							<p>  							</p>
							<p> [['critical', 'information'], 1.6]
							</p>
							<p>  							</p>
							<p> [['novel', 'Name-aware'], 1.6]
							</p>
							<p>  							</p>
							<p> [['best', 'result'], 1.6]
							</p>
							<p>  							</p>
							<p> [['+1.04', 'BLEU', 'average', 'improvement'], 1.6]
							</p>
							<p>  							</p>
							<p> [['Verbmobil', 'project'], 1.6]
							</p>
							<p>  							</p>
							<p> [['good', 'correlation', 'betweenG1', 'decode'], 1.6]
							</p>
							<p>  							</p>
							<p> [['multi-stack', 'decoder'], 1.6]
							</p>
							<p>  							</p>
							<p> [['new', 'notion'], 1.6]
							</p>
							<p>  							</p>
							<p> [['new', 'task'], 1.6]
							</p>
							<p>  							</p>
							<p> [['good', 'performance'], 1.6]
							</p>
							<p>  							</p>
							<p> [['bilingual', 'word', 'embeddings'], 1.6]
							</p>
							<p>  							</p>
							<p> [['bigram', 'model'], 1.6]
							</p>
							<p>  							</p>
							<p> [['cosine', 'similarity', 'measure'], 1.6]
							</p>
							<p>  							</p>
							<p> [['pattern', 'match', 'approach'], 1.6]
							</p>
							<p>  							</p>
							<p> [['projection-based', 'approach'], 1.6]
							</p>
							<p>  							</p>
							<p> [['Shared', 'Translation', 'Task'], 1.6]
							</p>
							<p>  							</p>
							<p> [['POS-based', 'reordering'], 1.6]
							</p>
							<p>  							</p>
							<p> [['Gibbs', 'sampling-based', 'Bayesian', 'inference', 'method'], 1.6]
							</p>
							<p>  							</p>
							<p> [['Bayesian', 'inference', 'outperforms'], 1.6]
							</p>
							<p>  							</p>
							<p> [['sparse', 'Dirichlet'], 1.6]
							</p>
							<p>  							</p>
							<p> [['algorithmic', 'framework'], 1.6]
							</p>
							<p>  							</p>
							<p> [['mathematical', 'formulation'], 1.6]
							</p>
							<p>  							</p>
							<p> [['restrict', 'stack-based', 'search'], 1.6]
							</p>
							<p>  							</p>
							<p> [['Held-Karp', 'dynamic', 'programming', 'algorithm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['exponential', 'time', 'algorithm'], 1.6]
							</p>
							<p>  							</p>
							<p> [['few', 'illustration'], 1.6]
							</p>
							<p>  							</p>
							<p> ['Since', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['good association measure', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Word segmentation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['language processing', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Bilingual Evaluation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word equally', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['3percent', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Retrieval Evaluation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['supervise learning', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Ranking SVM', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['good translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Topic', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Bilingual', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['BiTAM', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['topic model', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Simple', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word surround', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word pair', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Syntactic', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['syntactic constituent', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['sentence Semantic information', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['WordNet', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['language modeling', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Rosenfeld', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Klakow', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['level translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['BLUE', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['European language', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['E-step', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['dynamic-programming', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['mixture extension', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['specific dynamicprogramming decode algorithm', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['baseline model', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['component mixture', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Renlifang GcAbstracting translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['graph mapping Figure', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Jli', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Engkoo3', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Microsoft Research Asia', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['MIT', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['9-11 October', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Microsoft Research', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Melinda', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Latent Dirichlet-Tree', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['LDTA', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['bLSA', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['text translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['multiple translation model', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['significant difference between', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['ACL', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['represent', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['CRL', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['IREX', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['SVM', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['coreference relation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['syntactic feature', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Japanese NER', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Nakano', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Hirai', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['grammar extraction', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['name-aware', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['training', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['McNamee', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['McKeown', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word include', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['English-to-Arabic translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['simple English sentence', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Koehn', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Verbmobil', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['62percent', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Bayes', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['good correlation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['decode anddecoding', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word consider', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['INSERT', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['TIDES', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['CPU', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['fast', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['sacrifice translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Waibel report', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['sentence feasible', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['restricted version', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['multi-stack', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['SER', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['6-word sentence', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['8-word sentence', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['greedy decoding', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Jamaican', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Hillary', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Early', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['phonetic feature', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['phonetic similarity', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Khudanpur', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Langlais', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['phrase segmentation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['English-Chinese CrossLanguage Information Retrieval', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['CLIR', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Retrieval', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['show good', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['NIST08 Chinese-English', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Stanford Phrasal', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['semantic similarity', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['bilingual embeddings', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['NIST08', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Exact', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['probable translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['NP-Hard', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['domain making', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['context give moreLO', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Zweigenbaum', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word term', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['International Conference', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['multi-word', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['sentence context', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word Dejean', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Shared Translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['submit translation', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['EnglishFrench', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['POS', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['short-range reordering', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['WMT', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['English-German', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Gibbs sampling-based Bayesian', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['GIZA', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word position', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['sparse Dirichlet', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['NLP', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Griffiths', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Gibbs', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Bayesian setting', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['random variable', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['suitable prior', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['NPhard', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Kevin Knight show', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['HeldKarp', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Held-Karp', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Since optimal', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['fast suboptimal search', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['word long', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['simple computing', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> ['Scores7 Logscoresmada', 1.5999999999999999]
							</p>
							<p>  							</p>
							<p> 							</p>
							<p>  							</p>
</div>
            <button type="button" class="collapsible">Stats2</button>
            <div class="content" name="stats2">

            </div>
          </div>
        </div>
      </div>

      <button type="button"  class="collapsible">Claims</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_claims">
							<p> 
							</p>
							<p>  							</p>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> 	We show that it be possible to significantly decrease training and test corpus perplexity of the translation model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that by perform a rescoring on translation word graph we will obtain a more significant improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Out-of-vocabulary recognition may have two-sided effect on SMT performance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We also show that the propose method effectively address the well-known rare word problem in EM-estimated model ; and at the same time induce a much small dictionary of bilingual word-pairs .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We prove that while IBM Models 1-2 be conceptually and computationally simple , computation involve the high ( and more useful ) model be hard.Since it be unlikely that there exist a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The model be independent of the language pair and therefore , can be use to build a translation system for any language pair as long as a parallel corpus of text be available for training .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We also believe that our result may help in the design of effective heuristicsfor some of these task .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	One latent outcome of this work be that Wikipedia be surprisingly suitable for mine medical term .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	One interesting outcome of this study be that significant gain can be obtain by use an association measure that be rarely use in practice .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We plan to check its adequacy for other domain and verify that LO remain a good association measure for different corpus and domain
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Context-based projection method for identify the translation of term in comparable corpus have attract a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those work have systematically investigate the impact of the many parameter control their approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Still , it be already strike that a direct comparison of them be difficult , if not impossible .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Further , our result offer suggestive evidence that bilingual word embeddings act as high-quality semantic feature and embody bilingual translation equivalence across language .6 We report case-insensitive BLEU7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that the algorithmic handle provide by our framework can be employ to develop a very fast decoding algorithm which find good quality translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that decode algorithms derive from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that both of these problem be easy to solve and provide efficient solution for them .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have also show that alternate maximization can be employ tocome up with O ( m2 ) decode algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect the disambiguation to have a beneficial impact on the result give that polysemy be a frequent phenomenon in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , word in a general language corpus like Wikipedia can be polysemous and it be important to identify translation correspond to their different sens .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	An additional advantage be that the sense cluster often contain more than one translation and , therefore , provide supplementary material for the comparison of the vector in the target language .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect that a method capable of identify the correct sense of the feature and translate them accordingly could contribute to produce cleaner vector and to extract high quality lexicons.In this paper , we show how source vector can be translate into the target language by a cross-lingual Word Sense Disambiguation ( WSD ) method which exploit the output of data-driven Word Sense Induction ( WSI Apidianaki , 2009 and demonstrate how feature disambiguation enhance the quality of the translation extract from the comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Moreover , it be clear that disambiguate the vector improve the quality of the extracted lexicon and manages to beat the simpler , but yet powerful , most frequent translation heuristic .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The beauty of the bLSA framework be that the model search for a common latent topic space in an unsupervised fashion , rather than to require manual interaction .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	That be , Hillary Clinton be atemporal , as Figure 1 show , such that use such dissimilarity against decide this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This paper study name entity translation and propose selective temporality as a new feature , as use temporal feature may be harmful for translate atemporal entity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This paper validate that consider temporality selectively be helpful for improve the translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Empirical evidence suggest that such algorithm can perform resonably well .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	All these number suggest that approximative algorithm be a feasible choice for practical application .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , the quadratic component have such a small coefficient that it do not have any noticable effect on the translation speed for all reasonable input .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We assume here that the MT system be capable of provide word alignment ( or equivalent ) information during decoding , which be generally true for current statistical MT system .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Considering our semantic feature be the most basic one , use more sophisticated feature ( e.g the head word and their translation of the sourceside semantic role ) provide a possible direction for further experimentation
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , our analysis have show that for Arabic , these genre typically contain more Latin script and transliterated word , and thus there be less morphology to score .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , LM statistic be sparse , and they be make sparser by morphological variation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2012 Association for Computational LinguisticsIt have also be suggest that this setting require morphological generation because the bitext may not Pron + Fem + SgVerb + Masc +3 + PlPrtConj contain all inflected variant ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , use lexical coverage experiment , we show thatit there be ample room for translation quality improvement through good selection of form that already exist in the translation model.they writewilland .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our motivation to employ similar bilingual document pair in the training parallel corpus be simple : a human translator often collect similar bilingual document pair to help translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	During last decade , tremendous work have be do to improve the quality of statistical machineCorresponding author .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , there have not be until very recently that the application of mixture modelling in SMT have receive increase attention .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Among them , encyclopedias be especially important in that they contain a lot of term that a morphological dictionary fails to cover .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Word segmentation be the first step of natural language processing for Japanese , Chinese and Thai because they do not delimit word by white-space .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Most improvement come from correction of over-segmentation because the initial segmentation by the analyzer show a tendency of oversegmentation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We assume that the meaning of constituent in a noun phrase rarely depend on out context .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Although external lexical resource for human reader be potentially good knowledge source , they have not be utilize due to difference in segmentation criterion .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We can now segment them into word in a more sophisticated way .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that incorporate lexical weighting feature condition on soft domain membership directly into our model be an effective strategy for dynamically bias SMT towards relevant translation , as evidence by significant performance gain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We propose an approach that bias machine translation system toward relevant translation base on topic-specific context , where topic be induce in an unsupervised way use topic model ; this can be think of as induce subcorpora for adaptation without any human annotation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Even without recognition error , speech translation have to cope with a lack of conventional syntactic structure because the structure of spontaneous speech differ from that of write language .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	It be also important to acknowledge that there be many fundamental difference between the translation of name and other token , depend on whether a name be render phonetically , semantically , or a mixture of both ( Ji et al 2009 ) .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Names have be largely neglect in the prior MT research due to the following reason : The current dominant automatic MT score metric ( such as Bilingual Evaluation Understudy ( BLEU Papineni et al 2002 treat all word equally , but name have relative low frequency in text ( about 6percent in newswire and only 3percent in web document ) and thus be vastly outnumber by function word and common noun , etc.Name translation pose a great complexity because the set of name be open and highly dynamic .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While high quality entity translation be essential in cross-lingual information access and trans lation , it be non-trivial to achieve , due to the challenge that entity translation , though typically bear pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing effort to address these challenge can be categorize into transliterationand corpusbased approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Corpus-based similarity can support arbitrary translation , but require highly scarce resource of bilingual co-occurrence , obtain from parallel or comparable bilingual corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
					</div>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> 	There be also significant disagreement on the specification , although much of their content be the same .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In future work , other parameter which influence the performance will be study , among which the use of a terminological extractor to treat complex term ( Daille and Morin , 2005 more contextual window configuration , and the use of syntactic information in combination with lexical information ( Yu and Tsujii , 2009 It would also be interest to compare the projection-based approach to ( Haghighi et al 2008 ) s generative model for bilingual lexicon acquisition from monolingual corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	A close look at the translation candidate obtain when use LL , the most popular association measure in projection-based approach , show that they be often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Nevertheless , they find that human mind be very well capable of derive dependency such as morphology , cognate , proper name , spell variation etc and that this capability be finally at the basis of the good result produce by human compare to corpus base machine translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	As there be a large overlap between the modeled event in the combined probabilistic model , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation do .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The result show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally consider as a prerequisite for translation extraction from parallel corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	An avenue that we intend to explore in future work be to extract translation correspond to different sens of the headword .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our experiment be carry out on the English-Slovene language pair but as the method be totally data-driven , the approach can be easily apply to other languages.The paper be organize as follow : In the next section , we present some related work on bilingual lexicon extraction from comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In such case , as Hillary Clinton be a famous female leader , she may be associate with other Chinese female leader in Chinese corpus , while such association be rarely observe in English corpus , which cause asymmetry .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In such case partial SRSs must be record in such a way that they can be combine later with other partial SRSs .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Finally , we conclude this paper in Section 6 We have show that our cache-based approach significantly improve the performance with the help of various cache , such as the dynamic , static and topic cache , although the cache-based approach may introduce some negative impact onBLEU score for certain documents.In the future , we will far explore how to reflect document divergence during training and dynamically adjust cache weight accord to different documents.There be many useful component in trainingdocuments , such as name entity , event and coreference .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Tiedemann show that the repetition and consistency be very important when model natural language and translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Thirdly , reference translation of a test document write by human translator tend to have flexible expression in order to avoid produce monotonous text .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Mixture modelling be a standard technique for density estimation , but its use in statistical machine translation ( SMT ) have just start to be explore .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Although supervise segmentation be very competitive , we show that it can be supplement + very important to identify hiragana word correctly .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	As hiragana be mainly use to write function word and other basic word , segmentation error concern hiragana often bring disastrous effect on application of morphological analysis .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Experiments show that the propose method efficiently correct the initial segmentation give by a morphological analyzer .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Comparative evaluation with other translation approach of the Verbmobil prototype system show that the statistical translation be superior , especially in the presence of speech input and ungrammatical input
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In comparison with write language , speech and especially spontaneous speech pose additional difficulty for the task of automatic translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	1 Some of the resource and open source program develop in this work be make freely available for research purpose at A typical statistical MT system can only translate 60percent person name correctly ( Ji et al 2009 Incorrect segmentation and translation of name which often carry central meaning of a sentence can also yield incorrect translation of long context .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	It can be observe from the experimental result on the data set of Text Retrieval Evaluation Conference ( TREC ) that the obvious performance improvement for query translation can be obtain , which be very beneficial to CLIR and can improve the whole retrieval performance In this paper , the propose model improve the acquirement ability for OOV term translation through Web mining , and solve the translation pair selection and evaluation in a novel way by fuse multiple feature and introduce the supervise learning base on Ranking SVM .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To summarize , we believe that this paper have the following contribution : We abstract entity translation problem as a graph mapping between entity-relationship graph in two languages.We develop an effective matching algorithm leverage both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_claims">
							<p> 
							</p>
							<p>  							</p>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> 	Context-based projection method for identify the translation of term in comparable corpus have attract a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those work have systematically investigate the impact of the many parameter control their approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that the algorithmic handle provide by our framework can be employ to develop a very fast decoding algorithm which find good quality translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that both of these problem be easy to solve and provide efficient solution for them .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have also show that alternate maximization can be employ tocome up with O ( m2 ) decode algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	An additional advantage be that the sense cluster often contain more than one translation and , therefore , provide supplementary material for the comparison of the vector in the target language .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our experiment be carry out on the English-Slovene language pair but as the method be totally data-driven , the approach can be easily apply to other languages.The paper be organize as follow : In the next section , we present some related work on bilingual lexicon extraction from comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The beauty of the bLSA framework be that the model search for a common latent topic space in an unsupervised fashion , rather than to require manual interaction .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In such case , as Hillary Clinton be a famous female leader , she may be associate with other Chinese female leader in Chinese corpus , while such association be rarely observe in English corpus , which cause asymmetry .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	That be , Hillary Clinton be atemporal , as Figure 1 show , such that use such dissimilarity against decide this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This paper validate that consider temporality selectively be helpful for improve the translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In such case partial SRSs must be record in such a way that they can be combine later with other partial SRSs .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	One problem with the dynamic cache be that those initial sentence in a test document may not benefit from the dynamic cache .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Another problem be that the dynamic cache may be prone to noise and cause error propagation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Thirdly , reference translation of a test document write by human translator tend to have flexible expression in order to avoid produce monotonous text .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	During last decade , tremendous work have be do to improve the quality of statistical machineCorresponding author .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	As the translation process continue , the dynamic cache grows and contribute more and more to the translation of subsequent sentence .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Among them , encyclopedias be especially important in that they contain a lot of term that a morphological dictionary fails to cover .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	As hiragana be mainly use to write function word and other basic word , segmentation error concern hiragana often bring disastrous effect on application of morphological analysis .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that incorporate lexical weighting feature condition on soft domain membership directly into our model be an effective strategy for dynamically bias SMT towards relevant translation , as evidence by significant performance gain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We propose an approach that bias machine translation system toward relevant translation base on topic-specific context , where topic be induce in an unsupervised way use topic model ; this can be think of as induce subcorpora for adaptation without any human annotation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Names have be largely neglect in the prior MT research due to the following reason : The current dominant automatic MT score metric ( such as Bilingual Evaluation Understudy ( BLEU Papineni et al 2002 treat all word equally , but name have relative low frequency in text ( about 6percent in newswire and only 3percent in web document ) and thus be vastly outnumber by function word and common noun , etc.Name translation pose a great complexity because the set of name be open and highly dynamic .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
					</div>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> 	We believe that by perform a rescoring on translation word graph we will obtain a more significant improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that decode algorithms derive from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We assume here that the MT system be capable of provide word alignment ( or equivalent ) information during decoding , which be generally true for current statistical MT system .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	However , our analysis have show that for Arabic , these genre typically contain more Latin script and transliterated word , and thus there be less morphology to score .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Finally , we conclude this paper in Section 6 We have show that our cache-based approach significantly improve the performance with the help of various cache , such as the dynamic , static and topic cache , although the cache-based approach may introduce some negative impact onBLEU score for certain documents.In the future , we will far explore how to reflect document divergence during training and dynamically adjust cache weight accord to different documents.There be many useful component in trainingdocuments , such as name entity , event and coreference .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	1 Some of the resource and open source program develop in this work be make freely available for research purpose at A typical statistical MT system can only translate 60percent person name correctly ( Ji et al 2009 Incorrect segmentation and translation of name which often carry central meaning of a sentence can also yield incorrect translation of long context .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
      </div>


      <button type="button" class="collapsible">Evidences</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_evidences">
							<p> 
							</p>
							<p>  							</p>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> 	Depending on the model use to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have be able to obtain a significant good test corpus perplexity and also a slight improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Other author have apply this approach to language modeling ( Rosenfeld , 1996 ; Martin et al 1999 ; Peters and Klakow , 1999 A short review of the maximum entropy approach be outline in Section 3 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Finally , we conclude this paper in Section 6 We have show that our cache-based approach significantly improve the performance with the help of various cache , such as the dynamic , static and topic cache , although the cache-based approach may introduce some negative impact onBLEU score for certain documents.In the future , we will far explore how to reflect document divergence during training and dynamically adjust cache weight accord to different documents.There be many useful component in trainingdocuments , such as name entity , event and coreference .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 3 present our cache-based approach to documentlevel SMT .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Similarly , we can mine Chinese news article to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 3 then develop our framework .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To summarize , we believe that this paper have the following contribution : We abstract entity translation problem as a graph mapping between entity-relationship graph in two languages.We develop an effective matching algorithm leverage both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We evaluate the adapted LM on SMT and find that the evaluation metric be crucial to reflect the actual improvement in performance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	On Chinese to English speech and text translation the propose bLSA framework successfully reduce word perplexity of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have publish a much more detailed paper ( Zhang et al 2008 ) to describe the relation between CWS and SMT
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In our approach we introduce equivalence class in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We evaluate our approach on CRL NE data and obtain a high F-measure than exist approach that do not use structural information .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We introduce four type of structural information to an SVM-based NER system : cache feature , coreference relation , syntactic feature and caseframe feature , and conduct NER experiment on three data .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Compared to previous method , the novel contribution of our approach be : 1 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	For English-to-Arabic translation , we achieve a +1.04 BLEU average improvement by tile our model on top of a large LM .146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , page 146155 , Jeju , Republic of Korea , 8-14 July 2012 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To overcome such problem , we propose a new notion of selective temporality ( call this fea 2.3 Step 3 : Reinforcement We reinforce R0 by leverage R and obtain a converged matrix R use the following model : ture ST to distinguish from T ) to automatically distinguish temporal and atemporal entity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have show how cross-lingual WSD can be apply to bilingual lexicon extraction from comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 3 present the data use in our experiment and Section 4 provide detail on the approach and the experimental setup .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Segmentation for Japanese be a successful field of research , achieve the F-score of nearly 99 Kudo et al 2004 This success rest on a high-coverage dictionary .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We introduce bilingual word embeddings : semantic embeddings associate across two language in the context of neural language model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In practice , we be never sure that we have find the Viterbi alignment .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Instead , we investigate the impact of some major factor influence projection-based approach on a task of translate 5,000 term of the medical domain ( the most studied domain making use of French and English Wikipedia page extract monolingually thanks to an information retrieval engine .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have present the system for our participation in the WMT 2011 Evaluation for EnglishGerman and EnglishFrench .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have also show that alternate maximization can be employ tocome up with O ( m2 ) decode algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
					</div>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> 	In our case , by build a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derive subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for good translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that incorporate lexical weighting feature condition on soft domain membership directly into our model be an effective strategy for dynamically bias SMT towards relevant translation , as evidence by significant performance gain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Conditioning lexical probability on the topic bias translation toward topicrelevant output , result in significant improvement of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To obtain the lexical probability condition on topic distribution , we first compute the expected count ezn ( e , f of a word pair under topic zn : feature in the translation model , and interpolate them log-linearly with our other feature , thus allowezn ( e , f =p ( zn | di ) cj ( e , f 1 ) ing us to discriminatively optimize their weight on di T xj di an arbitrary objective function .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We can construct a topic model once on the training data , and use it infer topic on any test set to adapt the translation model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this work , we consider the underlying latent topic of the document ( Blei et al 2003 Topic modeling have receive some use in SMT , for instance Bilingual LSA adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which use a bilingual topic model for learn alignment .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	They then learn a mapping from these feature to sentence weight , use the sentence weight to bias the model probability estimate and subsequently learn the model weight .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We induce unsupervised domain from large corpus , and we incorporate soft , probabilistic domain membership into a translation model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that it be possible to significantly decrease training and test corpus perplexity of the translation model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In addition , we perform a rescoring of-Best list use our maximum entropy model and thereby yield an improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that by perform a rescoring on translation word graph we will obtain a more significant improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This additional information could be : Simple context information : information of the word surround the word pair ; Syntactic information : part-of-speech information , syntactic constituent , sentence Semantic information : disambiguation information ( e.g.from WordNet currentspeech or dialog act .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Tiedemann show that the repetition and consistency be very important when model natural language and translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Statistical machine translation system be usually train on a large amount of bilingual sentence pair and translate one sentence at a time , ignore document-level information .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Evaluation show the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Specifically , two type of semantic role feature be propose in this paper : a semantic role reorder feature design to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 page 716724 , Beijing , August 2010 ture design to penalize miss semantic role in the target sentence .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2007 Association for Computational Linguistics take advantage of the localization phenomenon of word alignment in European language , and the efficient and exact computation of the E-step and Viterbi alignment by use a dynamic-programming approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In ( Civera and Juan , 2006 a mixture extension of IBM model 2 along with a specific dynamicprogramming decode algorithm be propose .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical translation model , take IBM Model 1 as a baseline model , be present under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	3 Mixture of HMM alignment model Let us suppose that p ( x have be generate use a T-component mixture of HMM alignment model : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 4 report experimental result and Section 5 conclude our work.English PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graph .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our evaluation result empirically validate the accuracy of our algorithm over real-life datasets , and show the effectiveness on our propose perspective
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	1com430 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , page 430439 , MIT , Massachusetts , USA , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Such engine This work be do when the first two author visit Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To illustrate , Figure 1 demonstrate the query result for Bill Gates , retrieve and visualize the entity-relationship graph of related people name that frequently co-occur with Bill in English corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While high quality entity translation be essential in cross-lingual information access and trans lation , it be non-trivial to achieve , due to the challenge that entity translation , though typically bear pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing effort to address these challenge can be categorize into transliterationand corpusbased approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To illustrate this , an English news article mention Bill Gates and Melinda Gates evidence a relationship between the two entity , which can be quantify from their co-occurrence in the entire English Web corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In Section 3 , we present the effect of LM adaptation on word perplexity , follow by SMT experiment report in BLEU on both speech and text input in Section 3.3 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We organize the paper as follow : In Section 2 , we introduce the bLSA framework include Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The key property of the bLSA model be that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , page 520527 , Prague , Czech Republic , June 2007 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Results show that our approach significantly reduce the word perplexity on the target language in both case use ASR hypothesis and manual transcript .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We will investigate the incorporation of monolingual document for potentially good bilingual LSA modeling
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We also find the correlation between the CWS F-score and SMT BLEU score be very weak .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We test dictionarybased and CRF-based approach and find there be no significant difference between the two in the qualty of the resulting translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This approach produce a significant improvement in translation andachieved the best BLEU score of all the CWSschemes .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We propose a new approach to linear interpolation of translation feature .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Which approach pro 1 A CWS competition organize by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical Machine Translation , page 216223 , Columbus , Ohio , USA , June 2008 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this work , we also propose approach to make use of all the Sighan training data regardless of the specification .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We group all of the CWS method into two class : the class without out-of-vocabulary ( OOV ) recognition and the class with OOV recognition , represent by the dictionary-based CWS and the CRF-based CWS , respectively .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Chinese word segmentation ( CWS ) be a necessary step in Chinese-English statistical machine translation ( SMT ) and its performance have an impact on the result of SMT .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Experimental result on the German-English Verbmobil Source Language Textmorpho-syntactic AnalysisTransformation f J 1 Global Search : maximize Pr ( e I J e I task be report .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We furthermore suggest the use of hierarchical lexicon model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Nevertheless , they find that human mind be very well capable of derive dependency such as morphology , cognate , proper name , spell variation etc and that this capability be finally at the basis of the good result produce by human compare to corpus base machine translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	It have be successfully apply to realistic task in various national and international research program .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We also conduct experiment on IREX NE data and an NE-annotated web corpus and conrmed that structural information improve the performance of NER .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The Japanese NER system propose in ( Nakano and Hirai , 2004 which achieve the high F-measure among conventional system , introduce the bunsetsu1 feature in order to consider wide context , but consider only adjacent bunsetsus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2013 Association for Computational Linguistics name in parallel corpus , update word segmentation , word alignment and grammar extraction ( Section 3.1 We develop a name-aware MT framework which tightly integrate name tag and name translation into training and decoding of MT . Experiments on Chinese-English translation demonstrate the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genre .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Experiments on Chinese-English translation demonstrate the effectiveness of our approach on enhance the quality of overall translation , name translation and word alignment over a high-quality MT baseline1 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Tightly integrate joint bilingual name tag into MT training by coordinate tagged604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , page 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This need can be address in part by cross-lingual information access task such as entity link ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lie in the performance of Machine Translation ( MT Traditional MT approach focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content word include critical information , especially name .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name process into the training and decode process of an end-to-end MT pipeline , and a new name-aware metric to evaluate MT which can assign different weight to different token accord to their importance value in a document .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Speech recognition , speech synthesis , and machine translation research start about half a century ago .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The feasibility of speech-to-speech translation be the focus of research at the beginning because each component be difficult to build and their integration seem more difficult .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	After groundbreaking work for two decade , corpus-based speech and language processing technology have recently enable the achievement of speech-to-speech translation that be usable in the real world .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We achieve best result when the model training data , MT tune set , and MT evaluation set conThe bottom category include all lexical item that the decoder could produce in a translation of the source .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2012 Association for Computational LinguisticsIt have also be suggest that this setting require morphological generation because the bitext may not Pron + Fem + SgVerb + Masc +3 + PlPrtConj contain all inflected variant ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , use lexical coverage experiment , we show thatit there be ample room for translation quality improvement through good selection of form that already exist in the translation model.they writewilland .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	fem The car go quickly Figure 1 : Ungrammatical Arabic output of Google Translate for the English input The car go quickly .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Consider the output of Google Translate for the simple English sentence in Fig . 1 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Agreement relation that cross statistical phrase boundary be not explicitly model in most phrasebased MT system ( Avramidis and Koehn , 2008 We address this shortcoming with an agreement model that score sequence of fine-grained morphosyntactic class .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The experience obtain in the Verbmobil project , in particular a large-scale end-to-end evaluation , show that the statistical approach result in significantly low error rate than three compete translation approach : the sentence error rate be 29percent in comparison with 52percent to 62percent for the other translation approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Even without recognition error , speech translation have to cope with a lack of conventional syntactic structure because the structure of spontaneous speech differ from that of write language .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Starting with the Bayes decision rule as in speech recognition , we show how the required probability distribution can be structure into three part : the language model , the alignment model and the lexicon model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We describe the component of the system and report result on the Verbmobil task .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this paper , we have analyze the complexity of the greedy decode algorithm originally present in Germann et al 2001 ) and presented improvement that drastically reduce the decoder complexity and speed to practically linear time.Experimental data suggest a good correlation betweenG1 decode anddecoding ( with 10 translation per input word consider , a list of 498 candidate for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profile show be cumulative , so that the top curve reflect the total decoding time .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Brute force translation of the 100 short news article in Chinese from the TIDES MT evaluation in June 2002 ( 878 segment ; ca . 25k token ) require , without any of the improvement describe in this paper , over 440 CPU hour , use the simpler , fast algorithm ( de scribe below We will show that this time can be reduce to ca . 40 minute without sacrifice translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rate of 7.9 percent and 9.3 respectively , for their decoder .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	IBM Model 4 score and the BLEU metric .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Operations not include in the figure consume so little time that their plot can not be discern in the graph .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The speed improvement discuss in this paper make multiple randomized search per sentence feasible , lead to a faster and good decoder for machine translation with IBM Model 4.6 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We achieve this by integrate hypothesis evaluation into hypothesis creation , tile improvement over the translation hypothesis at the end of each search iteration , and by impose restriction on the amount of word reorder during decode .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a classifier to distinguish temporalentities and our propose method outperform the state-of-the-art approach by 6.1
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Early effort of name entity translation have focus on use phonetic feature ( call PH ) to estimate a phonetic similarity between two name ( Knight and Graehl , 1998 ; Li et al 2004 ; Virga and Khudanpur , 2003 In contrast , some approach have focus on use context feature ( call CX ) which compare surround word of entity ( Fung and Yee , 1998 ; Diab and Finch , 2000 ; Laroche and Langlais , 2010 Recently , holistic approach combine such similarity have be study ( Shao and Ng , 2004 ; You et al 2010 ; Kim et al 2011 Shao and Ng , 2004 ) rank translation candidate use PH and CX independently and return result with the high average rank You et al 2010 ) compute initial translation score use PH and iteratively update the score use relationship feature ( call R Kim et al 2011 ) boost Yous approach by additionally leverage CX .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This paper study name entity translation and propose selective temporality as a new feature , as use temporal feature may be harmful for translate atemporal entity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	That be , Hillary Clinton be atemporal , as Figure 1 show , such that use such dissimilarity against decide this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect the disambiguation to have a beneficial impact on the result give that polysemy be a frequent phenomenon in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect that a method capable of identify the correct sense of the feature and translate them accordingly could contribute to produce cleaner vector and to extract high quality lexicons.In this paper , we show how source vector can be translate into the target language by a cross-lingual Word Sense Disambiguation ( WSD ) method which exploit the output of data-driven Word Sense Induction ( WSI Apidianaki , 2009 and demonstrate how feature disambiguation enhance the quality of the translation extract from the comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In Section 5 , we report and discuss the obtained result before conclude and present some direction for future work .1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora , page 110 , Sofia , Bulgaria , August 8 , 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The result show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally consider as a prerequisite for translation extraction from parallel corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To segment each noun phrase , we use nonparametric Bayesian language model ( Goldwater et al 2009 ; Mochihashi et al 2009 Our approach605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , page 605615 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We apply non-parametric Bayesian language model to segment each noun phrase in these resource accord to the statistical behavior of its supposed constituent in text .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The presence of clarinet , alto and contrabass and others in the main text allow the model to identext to segment noun phrase in it .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Although supervise segmentation be very competitive , we show that it can be supplement + very important to identify hiragana word correctly .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this paper , we propose a new task of Japanese noun phrase segmentation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	From the experimental result for combine our OOV term translation model with English-Chinese CrossLanguage Information Retrieval ( CLIR ) on the data set of Text Retrieval Evaluation Conference ( TREC it can be find that the obvious performance improvement for both query translation and retrieval can also be obtain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Further , our result offer suggestive evidence that bilingual word embeddings act as high-quality semantic feature and embody bilingual translation equivalence across language .6 We report case-insensitive BLEU7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show good performance on Chinese semantic similarity with bilingual trained embeddings .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	On NIST08 Chinese-English translation task , we obtain an improvement of 0.48 BLEU from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , page 13931398 , Seattle , Washington , USA , 18-21 October 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	When use to compute semantic similarity of phrase pair , bilingual embeddings improve NIST08 end-to-end machine translation result by just below half a BLEU point .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In their seminal paper on SMT , Brown and his colleague highlight the problem we face as we go from IBM Models 1-2 to 3-5 ( Brown et al 1993 ) 3 : As we progress from Model 1 to Model 5 , evaluate the expectation that give us count becomes increasingly difficult .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In ( Knight , 1999 ) it be prove that the Exact Decoding prob Given the model parameter and a sentence f determine the most probable translation of f lem be NP-Hard when the language model be a bigram model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that our result on the computational complexity of the task in SMT will result in a good understanding of these task from a theoretical perspective .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our result showthat there can not exist a closed form expression ( whose representation be polynomial in the size of the input ) for P ( f | e ) and the count in the EMiterations for Models 3-5 unless P NP
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	enomial time solution for any of these hardeproblems ( unless P NP and P # P P our result highlight and justify the need for develop polynomial time approximation for these computation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In future work , other parameter which influence the performance will be study , among which the use of a terminological extractor to treat complex term ( Daille and Morin , 2005 more contextual window configuration , and the use of syntactic information in combination with lexical information ( Yu and Tsujii , 2009 It would also be interest to compare the projection-based approach to ( Haghighi et al 2008 ) s generative model for bilingual lexicon acquisition from monolingual corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	A close look at the translation candidate obtain when use LL , the most popular association measure in projection-based approach , show that they be often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Among the latter , many be translate single-word term ( Chiao and Zweigenbaum , 2002 ; Dejean et al 2005 ; Prochasson et 1 A stoplist be typically use in order to prevent function word from populate the context vector .617 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 page 617625 , Beijing , August 2010 al 2009 while others be tackle the translation of multi-word term ( Daille and Morin , 2005 The type of discourse might as well be of concern in some of the study dedicate to bilingual terminology mining .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our result show that use the log-odds ratio as the association measure allow for significantly good translation spot than the log-likelihood .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The present discussion only focus on a few number of representative study .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The high Top 1 precision , 55.2 be reach with the follow parameter : sentence context , LO , cosine and a 9,000-entry mixed lexicon , with the use of a cognate heuristic .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While the present work do not investigate all the parameter that could potentially impact result , we believe it constitute the most complete and systematic comparison make so far with variant of the context-based projection approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While a few study have investigate pattern match approach to compare source and target context ( Fung , 1995 ; Diab and Finch , 2000 ; Yu and Tsujii , 2009 most variant make use of a bilingual lexicon in order to translate the word of the context of a term ( often call seed word Dejean et al 2005 ) instead use a bilingual thesaurus for translate these.Another distinction between approach lie in the way the context be define .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In the remainder of this paper , we describe the projection-based approach to translation spot in Section 2 and detail the parameter that directly influence its performance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We participate in the Shared Translation Task and submit translation for EnglishGerman and EnglishFrench .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We apply POS-based reordering to improve our translation in all direction , use short-range reordering for EnglishFrench and long-range reordering for EnglishGerman .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In addition , a parallel phrase score technique be implement that could speed up the MT training process tremendously .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	A Discriminative Word Alignment Model lead to an increase in BLEU for English-German .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Problems with the standard EM estimation of IBM Model 1 be point out by Moore and a number of heuristic change to the estimation procedure , such as smooth the parameter estimate , be show to reduce the alignment error rate , but the effect on translation performance be not report .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignment and show that it outperform EM estimation in term of translation BLEU score across several language pair , data size and domain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that Bayesian inference outperforms EM in all of the tested language pair , domain and data set size , by up to 2.99 BLEU point .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We evale = 1 f = 1 ( I s uate the inferred alignment in term of the end-toend translation performance , where we show the result with a variety of input data to illustrate the general applicability of the propose technique .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameter to prevent overfitting .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Bayesian inference , the approach in this paper , have recently be apply to several unsupervised learn problem in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other task in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learn phrase alignment directly ( DeNero et al 2008 Word alignment learn problem be address jointly with segmentation learn in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two work place nonparametric prior ( also know as cache model ) on the parameter and utilize Gibbs sampling .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a Gibbs sampler for alignment under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which be relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 be use in bootstrapping the parameter setting for EM training of high P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment model , and many state-of-the n n ( te , f ) Ne , f n J art SMT system use Model 1 translation probabil ities as feature in their log-linear model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	An O ( m6 ) greedy search algorithm be develop ( Germann et al 2003 ) whose complexity be re duced far to O ( m2 Germann , 2003 In this paper , we propose an algorithmic framework for solve the decoding problem and show that several efficient decode algorithm can be derive from the technique develop in the framework .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that decode algorithms derive from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In an iterative search for a local optimal solution , we alternate between the two algorithm and refine our solution .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We start with a mathematical formulation of the decoding problem ( Section 2 We then develop the alternate search paradigm and use it to develop several decode algorithm ( Section 3 Next , we demonstrate the practical utility of our algorithm with the help of result from our initial experiment ( Section 5 ) .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that the algorithmic handle provide by our framework can be employ to develop a very fast decoding algorithm which find good quality translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that both of these problem be easy to solve and provide efficient solution for them .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Decoding be know to belong to a class of computational problem popularly know as NPhard problem ( Knight , 1999 NP-hard problem be know to be computationally hard and have elude polynomial time algorithm ( Garey and Johnson , 1979 The first algorithm for the decoding problem be base on what be know among the speech recognition community as stack-based search ( Jelinek , 1969 The original IBM solution to the decoding problem employ a restrict stack-based search ( Berger et al 1996 This idea be far explore by Wang and Waibel ( Wang and Waibel , 1997 ) who develop a faster stack-based search algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In perhaps the first work on the computational complexity of Decoding , Kevin Knight show that the problem be closely relate to the more famous Traveling Salesman problem ( TSP Independently , Christoph Tillman adapt the Held-Karp dynamic programming algorithm for TSP ( Held and Karp , 1962 ) to Decoding ( Tillman , 2001 The original HeldKarp algorithm for TSP be an exponential time dynamic programming algorithm and Tillmans adaptation to Decoding have a prohibitive com plexity of O ( l3m2 2m ) O ( m5 2m ( where m and l be the length of the source and tar get sentence respectively Tillman and Ney show how to improve the complexity of the Held-Karp algorithm for restrict word re order and give a O ( l3m4 ) O ( m7 ) algo rithm for French-English translation ( Tillman and Ney , 2000 An optimal decoder base on the well-known A heuristic be implement and benchmarked in ( Och et al 2001 Since optimal solution can not be compute for practical problem instance in a reasonable amount of time , much of recent work have focus on good quality suboptimal solution .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our fast suboptimal search algorithm can translate sentence that be 50 word long in about 5 second on a simple computing facility .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	At one end of the spectrum be a provably linear time algorithm for compute a suboptimal solution and at the other end be an exponential time algorithm for computingNIST Scores7 Logscoresmada .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	A family of provably fast decode algorithm can be derive from the basic technique underlie the framework and we present a few illustration .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_evidences">
							<p> 
							</p>
							<p>  							</p>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> 	He propose to employ cache-based language and translation model in a phrase-based SMT system for domain909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , page 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Specifically , two type of semantic role feature be propose in this paper : a semantic role reorder feature design to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 page 716724 , Beijing , August 2010 ture design to penalize miss semantic role in the target sentence .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	1com430 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , page 430439 , MIT , Massachusetts , USA , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The key property of the bLSA model be that Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , page 520527 , Prague , Czech Republic , June 2007 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Results show that our approach significantly reduce the word perplexity on the target language in both case use ASR hypothesis and manual transcript .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Which approach pro 1 A CWS competition organize by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical Machine Translation , page 216223 , Columbus , Ohio , USA , June 2008 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In our approach we introduce equivalence class in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The Japanese NER system propose in ( Nakano and Hirai , 2004 which achieve the high F-measure among conventional system , introduce the bunsetsu1 feature in order to consider wide context , but consider only adjacent bunsetsus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Tightly integrate joint bilingual name tag into MT training by coordinate tagged604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics , page 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	For English-to-Arabic translation , we achieve a +1.04 BLEU average improvement by tile our model on top of a large LM .146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , page 146155 , Jeju , Republic of Korea , 8-14 July 2012 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The speed improvement discuss in this paper make multiple randomized search per sentence feasible , lead to a faster and good decoder for machine translation with IBM Model 4.6 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect that a method capable of identify the correct sense of the feature and translate them accordingly could contribute to produce cleaner vector and to extract high quality lexicons.In this paper , we show how source vector can be translate into the target language by a cross-lingual Word Sense Disambiguation ( WSD ) method which exploit the output of data-driven Word Sense Induction ( WSI Apidianaki , 2009 and demonstrate how feature disambiguation enhance the quality of the translation extract from the comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In Section 5 , we report and discuss the obtained result before conclude and present some direction for future work .1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora , page 110 , Sofia , Bulgaria , August 8 , 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	On NIST08 Chinese-English translation task , we obtain an improvement of 0.48 BLEU from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , page 13931398 , Seattle , Washington , USA , 18-21 October 2013 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While a few study have investigate pattern match approach to compare source and target context ( Fung , 1995 ; Diab and Finch , 2000 ; Yu and Tsujii , 2009 most variant make use of a bilingual lexicon in order to translate the word of the context of a term ( often call seed word Dejean et al 2005 ) instead use a bilingual thesaurus for translate these.Another distinction between approach lie in the way the context be define .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Bayesian inference , the approach in this paper , have recently be apply to several unsupervised learn problem in NLP ( Goldwater and Griffiths , 2007 ; Johnson et al 2007 ) as well as to other task in SMT such as synchronous grammar induction ( Blunsom et al 2009 ) and learn phrase alignment directly ( DeNero et al 2008 Word alignment learn problem be address jointly with segmentation learn in Xu et al 2008 Nguyen et al 2010 and Chung and Gildea ( 2009 The former two work place nonparametric prior ( also know as cache model ) on the parameter and utilize Gibbs sampling .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	An O ( m6 ) greedy search algorithm be develop ( Germann et al 2003 ) whose complexity be re duced far to O ( m2 Germann , 2003 In this paper , we propose an algorithmic framework for solve the decoding problem and show that several efficient decode algorithm can be derive from the technique develop in the framework .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We start with a mathematical formulation of the decoding problem ( Section 2 We then develop the alternate search paradigm and use it to develop several decode algorithm ( Section 3 Next , we demonstrate the practical utility of our algorithm with the help of result from our initial experiment ( Section 5 ) .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Decoding be know to belong to a class of computational problem popularly know as NPhard problem ( Knight , 1999 NP-hard problem be know to be computationally hard and have elude polynomial time algorithm ( Garey and Johnson , 1979 The first algorithm for the decoding problem be base on what be know among the speech recognition community as stack-based search ( Jelinek , 1969 The original IBM solution to the decoding problem employ a restrict stack-based search ( Berger et al 1996 This idea be far explore by Wang and Waibel ( Wang and Waibel , 1997 ) who develop a faster stack-based search algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
					</div>
						<button type="button" id="subsubcollapsible" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> 	In our case , by build a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derive subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for good translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that incorporate lexical weighting feature condition on soft domain membership directly into our model be an effective strategy for dynamically bias SMT towards relevant translation , as evidence by significant performance gain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Depending on the model use to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this work , we consider the underlying latent topic of the document ( Blei et al 2003 Topic modeling have receive some use in SMT , for instance Bilingual LSA adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which use a bilingual topic model for learn alignment .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that by perform a rescoring on translation word graph we will obtain a more significant improvement in translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 3 present our cache-based approach to documentlevel SMT .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Evaluation show the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical translation model , take IBM Model 1 as a baseline model , be present under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 4 report experimental result and Section 5 conclude our work.English PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graph .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Similarly , we can mine Chinese news article to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To highlight the advantage of our propose approach , we compare our result with commercial machine translator Engkoo3 develop in Microsoft Research Asia and Google Translator4 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our evaluation result empirically validate the accuracy of our algorithm over real-life datasets , and show the effectiveness on our propose perspective
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Such engine This work be do when the first two author visit Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While high quality entity translation be essential in cross-lingual information access and trans lation , it be non-trivial to achieve , due to the challenge that entity translation , though typically bear pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing effort to address these challenge can be categorize into transliterationand corpusbased approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We will investigate the incorporation of monolingual document for potentially good bilingual LSA modeling
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this work , we also propose approach to make use of all the Sighan training data regardless of the specification .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Chinese word segmentation ( CWS ) be a necessary step in Chinese-English statistical machine translation ( SMT ) and its performance have an impact on the result of SMT .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We furthermore suggest the use of hierarchical lexicon model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We also conduct experiment on IREX NE data and an NE-annotated web corpus and conrmed that structural information improve the performance of NER .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	As a consequence , the performance of NER be improve by use structural information and our approach achieve a high F-measure than exist approach
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	This need can be address in part by cross-lingual information access task such as entity link ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lie in the performance of Machine Translation ( MT Traditional MT approach focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content word include critical information , especially name .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The feasibility of speech-to-speech translation be the focus of research at the beginning because each component be difficult to build and their integration seem more difficult .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	After groundbreaking work for two decade , corpus-based speech and language processing technology have recently enable the achievement of speech-to-speech translation that be usable in the real world .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	fem The car go quickly Figure 1 : Ungrammatical Arabic output of Google Translate for the English input The car go quickly .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	The experience obtain in the Verbmobil project , in particular a large-scale end-to-end evaluation , show that the statistical approach result in significantly low error rate than three compete translation approach : the sentence error rate be 29percent in comparison with 52percent to 62percent for the other translation approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Starting with the Bayes decision rule as in speech recognition , we show how the required probability distribution can be structure into three part : the language model , the alignment model and the lexicon model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this paper , we have analyze the complexity of the greedy decode algorithm originally present in Germann et al 2001 ) and presented improvement that drastically reduce the decoder complexity and speed to practically linear time.Experimental data suggest a good correlation betweenG1 decode anddecoding ( with 10 translation per input word consider , a list of 498 candidate for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profile show be cumulative , so that the top curve reflect the total decoding time .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We present improvement to a greedy decode algorithm for statistical machine translation that reduce its time complexity from at least cubic ( when apply navely ) to practically linear time1 without sacrifice translation quality .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Using the same evaluation metric ( but different evaluation data Wang and Waibel report search error rate of 7.9 percent and 9.3 respectively , for their decoder .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Operations not include in the figure consume so little time that their plot can not be discern in the graph .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Och et al . report word error rate of 68.68 percent for optimal search ( base on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combine dynamic program with a beam search ( Tillmann and Ney , 2000 Germann et al 2001 ) compare translation obtain by a multi-stack decoder and a greedy hill-climbing algorithm against those produce by an optimal integer program decoder that treat decode as a variant of the traveling-salesman problem ( cf . Knight , 1999 Their overall performance metric be the sentence error rate ( SER For decode with IBM Model 3 , they report SERs of about 57 6-word sentence ) and 76 8-word sentence ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which be the focus of this paper .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a classifier to distinguish temporalentities and our propose method outperform the state-of-the-art approach by 6.1
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To overcome such problem , we propose a new notion of selective temporality ( call this fea 2.3 Step 3 : Reinforcement We reinforce R0 by leverage R and obtain a converged matrix R use the following model : ture ST to distinguish from T ) to automatically distinguish temporal and atemporal entity .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We expect the disambiguation to have a beneficial impact on the result give that polysemy be a frequent phenomenon in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have show how cross-lingual WSD can be apply to bilingual lexicon extraction from comparable corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Section 3 present the data use in our experiment and Section 4 provide detail on the approach and the experimental setup .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	To segment each noun phrase , we use nonparametric Bayesian language model ( Goldwater et al 2009 ; Mochihashi et al 2009 Our approach605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , page 605615 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We apply non-parametric Bayesian language model to segment each noun phrase in these resource accord to the statistical behavior of its supposed constituent in text .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In this paper , we propose a new task of Japanese noun phrase segmentation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	When use to compute semantic similarity of phrase pair , bilingual embeddings improve NIST08 end-to-end machine translation result by just below half a BLEU point .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that our result on the computational complexity of the task in SMT will result in a good understanding of these task from a theoretical perspective .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In practice , we be never sure that we have find the Viterbi alignment .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Our result showthat there can not exist a closed form expression ( whose representation be polynomial in the size of the input ) for P ( f | e ) and the count in the EMiterations for Models 3-5 unless P NP
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	around the Viterbi alignment , i.e . in determine j = 1 tfj | eajdj : aj I = 0 d ( j | i , m the goodness of the Viterbi alignment in compar ison to the rest of the alignments.Decoding be an integral component of all SMT system ( Wang , Table 1 : IBM Model 3 Here , n ( | e ) be the fertility model , t ( f | e ) be the lexicon model and d ( j | i , m be the distortion model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	enomial time solution for any of these hardeproblems ( unless P NP and P # P P our result highlight and justify the need for develop polynomial time approximation for these computation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In future work , other parameter which influence the performance will be study , among which the use of a terminological extractor to treat complex term ( Daille and Morin , 2005 more contextual window configuration , and the use of syntactic information in combination with lexical information ( Yu and Tsujii , 2009 It would also be interest to compare the projection-based approach to ( Haghighi et al 2008 ) s generative model for bilingual lexicon acquisition from monolingual corpus .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	While the present work do not investigate all the parameter that could potentially impact result , we believe it constitute the most complete and systematic comparison make so far with variant of the context-based projection approach .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In the remainder of this paper , we describe the projection-based approach to translation spot in Section 2 and detail the parameter that directly influence its performance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Problems with the standard EM estimation of IBM Model 1 be point out by Moore and a number of heuristic change to the estimation procedure , such as smooth the parameter estimate , be show to reduce the alignment error rate , but the effect on translation performance be not report .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignment and show that it outperform EM estimation in term of translation BLEU score across several language pair , data size and domain .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that Bayesian inference outperforms EM in all of the tested language pair , domain and data set size , by up to 2.99 BLEU point .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Recently , Zhao and Gildea propose fertility extension to IBM Model 1 and HMM , but they do not place any prior on the parameter and their inference method be actually stochastic EM ( also know as Monte Carlo EM a ML technique in which sampling be use to fj be associate with a hidden alignment variable aj whose value range over the word position in the corresponding source sentence .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	Qc 2011 Association for Computational Linguistics Chung and Gildea apply a sparse Dirichlet prior on the multinomial parameter to prevent overfitting .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We develop a Gibbs sampler for alignment under IBM Model 1 , In the proposed Bayesian setting , we treat T as a random variable with a prior P ( T To find a suitable prior for T , we re-write as : VE VF which be relevant for the state-of-the-art SMT sys tems since 1 ) Model 1 be use in bootstrapping the parameter setting for EM training of high P ( E , F , A | T n s P ( I 1 ) J n n ( t e = 1 f = 1e , f ) ne , f VE VF Porder alignment model , and many state-of-the n n ( te , f ) Ne , f n J art SMT system use Model 1 translation probabil ities as feature in their log-linear model .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We believe that decode algorithms derive from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We have also show that alternate maximization can be employ tocome up with O ( m2 ) decode algorithm .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that the algorithmic handle provide by our framework can be employ to develop a very fast decoding algorithm which find good quality translation .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	We show that both of these problem be easy to solve and provide efficient solution for them .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	In perhaps the first work on the computational complexity of Decoding , Kevin Knight show that the problem be closely relate to the more famous Traveling Salesman problem ( TSP Independently , Christoph Tillman adapt the Held-Karp dynamic programming algorithm for TSP ( Held and Karp , 1962 ) to Decoding ( Tillman , 2001 The original HeldKarp algorithm for TSP be an exponential time dynamic programming algorithm and Tillmans adaptation to Decoding have a prohibitive com plexity of O ( l3m2 2m ) O ( m5 2m ( where m and l be the length of the source and tar get sentence respectively Tillman and Ney show how to improve the complexity of the Held-Karp algorithm for restrict word re order and give a O ( l3m4 ) O ( m7 ) algo rithm for French-English translation ( Tillman and Ney , 2000 An optimal decoder base on the well-known A heuristic be implement and benchmarked in ( Och et al 2001 Since optimal solution can not be compute for practical problem instance in a reasonable amount of time , much of recent work have focus on good quality suboptimal solution .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 	At one end of the spectrum be a provably linear time algorithm for compute a suboptimal solution and at the other end be an exponential time algorithm for computingNIST Scores7 Logscoresmada .
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 
							</p>
							<p>  							</p>
							<p> 							</p>
							<p>  							</p>
					</div>
</div>
            </div>
          </div>
        </div>
      </div>


      <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
              content.style.display = "none";
            } else {
              content.style.display = "block";
            }
          });
        }
      </script>

    </body>
</html>
