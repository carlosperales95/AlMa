PCA on evidences
----------------
----------------

Sentences in cluster n0:
.....................................
	We believe that by performing a rescoring on translation word graphs we will obtain a more significant improvement in translation quality .

	We prove that while IBM Models 1-2 are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The models are independent of the language pair and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training .

	One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms .

	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .

	One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice .

	We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains

	Still , it is already striking that a direct comparison of them is difficult , if not impossible .

	Further , our results offer suggestive evidence that bilingual word embeddings act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive BLEU7 With 4-gram BLEU metric from .

	Nevertheless , they found that human mind is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based machine translation .

	As there is a large overlap between the modeled events in the combined probabilistic models , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation does .

	We believe that decoding algorithms derived from our framework can be of practical significance .

	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .

	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general , mixed-domain corpus .

	However , words in a general language corpus like Wikipedia can be polysemous and it is important to identify translations corresponding to their different senses .

	We expect that a method capable of identifying the correct sense of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons.In this paper , we show how source vectors can be translated into the target language by a cross-lingual Word Sense Disambiguation ( WSD ) method which exploits the output of data-driven Word Sense Induction ( WSI Apidianaki , 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus .

	Our experiments are carried out on the English-Slovene language pair but as the methods are totally data-driven , the approach can be easily applied to other languages.The paper is organized as follows : In the next section , we present some related work on bilingual lexicon extraction from comparable corpora .

	Moreover , it is clear that disambiguating the vectors improves the quality of the extracted lexicons and manages to beat the simpler , but yet powerful , most frequent translation heuristic .

	The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion , rather than to require manual interaction .

	This paper validated that considering temporality selectively is helpful for improving the translation quality .

	Empirical evidence suggests that such algorithms can perform resonably well .

	All these numbers suggest that approximative algorithms are a feasible choice for practical applications .

	However , the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs .

	We assume here that the MT system is capable of providing word alignment ( or equivalent ) information during decoding , which is generally true for current statistical MT systems .

	In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs .

	Considering our semantic features are the most basic ones , using more sophisticated features ( e.g the head words and their translations of the sourceside semantic roles ) provides a possible direction for further experimentation

	However , our analysis has shown that for Arabic , these genres typically contain more Latin script and transliterated words , and thus there is less morphology to score .

	However , LM statistics are sparse , and they are made sparser by morphological variation .

	Qc 2012 Association for Computational LinguisticsIt has also been suggested that this setting requires morphological generation because the bitext may not Pron + Fem + SgVerb + Masc +3 + PlPrtConj contain all inflected variants ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , using lexical coverage experiments , we show thatit there is ample room for translation quality improvements through better selection of forms that already exist in the translation model.they writewilland .

	English is a weakly inflected language : it has a narrow verbal paradigm , restricted nominal inflection ( plurals and only the vestiges of a case system .

	One problem with the dynamic cache is that those initial sentences in a test document may not benefit from the dynamic cache .

	Another problem is that the dynamic cache may be prone to noise and cause error propagation .

	At least , the topic of a document can help choose specific translation candidates , since when taken out of the context from their document , some words , phrases and even sentences may be rather ambiguous and thus difficult to understand .

	Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple : a human translator often collects similar bilingual document pairs to help translation .

	During last decade , tremendous work has been done to improve the quality of statistical machineCorresponding author .

	However , there have not been until very recently that the application of mixture modelling in SMT has received increasing attention .

	Among them , encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover .

	Most improvements come from correction of over-segmentation because the initial segmentation by the analyzer shows a tendency of oversegmentation .

	Historically , researchers have devoted extensive human resources to build and maintain high coverage dictionaries ( Yokoi , 1995 Since the orthography of Japanese does not specify a standard for segmentation , researchers define their own criteria before constructing lexical resources .

	We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation .

	Even without recognition errors , speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech differ from that of written language .




Sentences in cluster n1:
.....................................
	We show that it is possible to significantly decrease training and test corpus perplexity of the translation models .

	Out-of-vocabulary recognition may have two-sided effects on SMT performance .

	There is also significant disagreement on the specifications , although much of their contents is the same .

	We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .

	We also believe that our results may help in the design of effective heuristicsfor some of these tasks .

	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and Morin , 2005 more contextual window configurations , and the use of syntactic information in combination with lexical information ( Yu and Tsujii , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s generative model for bilingual lexicon acquisition from monolingual corpora .

	Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach .

	We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations .

	We show that both of these problems are easy to solve and provide efficient solutions for them .

	We have also shown that alternating maximization can be employed tocome up with O ( m2 ) decoding algorithm .

	An additional advantage is that the sense clusters often contain more than one translation and , therefore , provide supplementary material for the comparison of the vectors in the target language .

	An avenue that we intend to explore in future work is to extract translations corresponding to different senses of the headwords .

	The evaluation has demonstrated that our system is both effective and useful in a real-world environment

	In such case , as Hillary Clinton is a famous female leader , she may be associated with other Chinese female leaders in Chinese corpus , while such association is rarely observed in English corpus , which causes asymmetry .

	That is , Hillary Clinton is atemporal , as Figure 1 shows , such that using such dissimilarity against deciding this pair as a correct translation would be harmful .

	This paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities .

	The subject should agree with the verb in both gender and number , but the verb has masculine inflection .

	Finally , we conclude this paper in Section 6 We have shown that our cache-based approach significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact onBLEU scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .

	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .

	Thirdly , reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts .

	As the translation process continues , the dynamic cache grows and contributes more and more to the translation of subsequent sentences .

	Mixture modelling is a standard technique for density estimation , but its use in statistical machine translation ( SMT ) has just started to be explored .

	Although supervised segmentation is very competitive , we showed that it can be supplemented + very important to identify hiragana words correctly .

	As hiragana is mainly used to write function words and other basic words , segmentation errors concerning hiragana often bring disastrous effects on applications of morphological analysis .

	Word segmentation is the first step of natural language processing for Japanese , Chinese and Thai because they do not delimit words by white-space .

	We assume that the meaning of constituents in a noun phrase rarely depends on outer context .

	Although external lexical resources for human readers are potentially good knowledge sources , they have not been utilized due to differences in segmentation criteria .

	Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer .

	We can now segment them into words in a more sophisticated way .

	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .

	Comparative evaluations with other translation approaches of the Verbmobil prototype system show that the statistical translation is superior , especially in the presence of speech input and ungrammatical input




