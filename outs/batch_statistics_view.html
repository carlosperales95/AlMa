<html>
    <head>
      <style type="text/css">

        p {
          font-size: 14px;
        }
        .column {
            float: left;
            width: 50%;
            padding:0%;
            margin:0%;

        }
        .subcolumn {
            float: left;
            width: 30%;
            padding:0%;
            margin:0%;

        }

        .row {
          width:100%;
          padding:0%;
          margin:0%;

        }

        img {

          max-width: 96%;
        }

        .row:after {
            content: "";
            display: table;
            clear: both;
        }
        .collapsible {
          background-color: #66ccff;
          color: #444;
          cursor: pointer;
          padding: 18px;
          width: 100%;
          border: none;
          text-align: left;
          outline: none;
          font-size: 15px;
        }

        .active, .collapsible:hover {
          background-color: #ccc;
        }

        .content {
          padding:1% !important;
          padding: 0 18px;
          display: none;
          overflow: hidden;
          background-color: #f1f1f1;
          max-width: 100%;
          margin:0%;

        }

        #subcollapsible {
          background-color: #4bab81 !important;

        }

        #subsubcollapsible {
          background-color: #81a118 !important;

        }
      </style>
    </head>
    <body>

      <button type="button" class="collapsible">Results for the query (QUERY)</button>
      <div class="content">
        <div class="row">
          <div class="column" name="paper_titles">
							<p> <strong>(1)</strong> - ﻿Semantic Role Features for Machine Translation
 
  							</p>
							<p> <strong>(2)</strong> - ﻿Computational Complexity of Statistical Machine  Translation
 
  							</p>
							<p> <strong>(3)</strong> - ﻿An Algorithmic Framework for the Decoding Problem in
 Statistical Machine Translation
 
  							</p>
							<p> <strong>(4)</strong> - ﻿Greedy Decoding for Statistical Machine Translation in Almost Linear Time
 
  							</p>
							<p> <strong>(5)</strong> - ﻿Refined Lexicon Models for Statistical Machine Translation using a
 Maximum Entropy Approach
 
  							</p>
							<p> <strong>(6)</strong> - ﻿Toward hierarchical models for statistical machine translation of 
 inflected languages
 
  							</p>
							<p> <strong>(7)</strong> - ﻿The RWTH System for Statistical Translation of Spoken
 Dialogues
 
  							</p>
							<p> <strong>(8)</strong> - ﻿Enriching Entity Translation  Discovery using Selective Temporality
 
  							</p>
							<p> <strong>(9)</strong> - ﻿Fusion of Multiple Features and Ranking SVM for
 Web-based English-Chinese OOV Term Translation
 
  							</p>
							<p> <strong>(10)</strong> - ﻿Mining Name Translations from Entity Graph Mapping∗
 
  							</p>
							<p> <strong>(11)</strong> - ﻿Cross-lingual WSD for Translation Extraction 
 from Comparable Corpora
 
  							</p>
							<p> <strong>(12)</strong> - ﻿Name-aware Machine Translation
 
 Haibo Li†	Jing Zheng‡	Heng Ji†	Qi Li†	Wen Wang‡
 
  							</p>
							<p> <strong>(13)</strong> - ﻿Revisiting Context-based Projection Methods for
 Term-Translation Spotting in Comparable Corpora
 
  							</p>
							<p> <strong>(14)</strong> - ﻿
 NICT-ATR Speech-to-Speech Translation System
 
  							</p>
							<p> <strong>(15)</strong> - ﻿Improved Statistical Machine Translation by Multiple Chinese Word
 Segmentation
 
  							</p>
							<p> <strong>(16)</strong> - ﻿Bayesian Word Alignment for Statistical Machine Translation
 
  							</p>
							<p> <strong>(17)</strong> - ﻿Topic Models for Dynamic Translation Model Adaptation
 
  							</p>
							<p> <strong>(18)</strong> - ﻿Domain Adaptation in Statistical Machine Translation with Mixture
 Modelling ∗
 
  							</p>
							<p> <strong>(19)</strong> - ﻿Cache-based Document-level Statistical Machine Translation
 
  							</p>
							<p> <strong>(20)</strong> - ﻿Bilingual-LSA Based LM Adaptation for Spoken Language Translation
 
  							</p>
							<p> <strong>(21)</strong> - ﻿Bilingual Word Embeddings for Phrase-Based Machine Translation
 
  							</p>
							<p> <strong>(22)</strong> - ﻿The Karlsruhe Institute of Technology Translation Systems 
 for the WMT 2011
 
  							</p>
							<p> <strong>(23)</strong> - ﻿A Class-Based Agreement Model for
 Generating Accurately Inflected Translations
 
  							</p>
							<p> <strong>(24)</strong> - ﻿Discovering Commonsense Entailment  Rules Implicit in Sentences
 
  							</p>
</div>
          <div class="column">
            <button type="button" class="collapsible">Technology Methods</button>
            <div class="content" name="mentions">
              <div class="row">
                <div class="subcolumn" name="left-mentions">							<p> BLEU							</p>
							<p>  							</p>
							<p>  human mind							</p>
							<p>  							</p>
							<p>  following contribution							</p>
							<p>  							</p>
							<p> English							</p>
							<p>  							</p>
							<p>  good association measure							</p>
							<p>  							</p>
							<p>  dynamic cache							</p>
							<p>  							</p>
							<p>  statistical translation							</p>
							<p>  							</p>
							<p>  unsupervised domain							</p>
							<p>  							</p>
							<p>  new approach							</p>
							<p>  							</p>
							<p>  new notion							</p>
							<p>  							</p>
							<p> language pair							</p>
							<p>  							</p>
							<p> LSA							</p>
							<p>  							</p>
							<p> USA							</p>
							<p>  							</p>
							<p> OOV							</p>
							<p>  							</p>
							<p> TSP							</p>
							<p>  							</p>
							<p> syntactic information							</p>
							<p>  							</p>
							<p> word embeddings							</p>
							<p>  							</p>
							<p> WSD							</p>
							<p>  							</p>
							<p> SgVerb							</p>
							<p>  							</p>
							<p> system show							</p>
							<p>  							</p>
							<p> Empirical							</p>
							<p>  							</p>
							<p> Republic							</p>
							<p>  							</p>
							<p> Experimental							</p>
							<p>  							</p>
							<p> language model							</p>
							<p>  							</p>
							<p> suboptimal solution							</p>
							<p>  							</p>
</div>
                <div class="subcolumn" name="center-mentions">							<p> SMT							</p>
							<p>  							</p>
							<p>  frequent phenomenon							</p>
							<p>  							</p>
							<p>  significant improvement							</p>
							<p>  							</p>
							<p> CWS							</p>
							<p>  							</p>
							<p>  algorithmic handle provide							</p>
							<p>  							</p>
							<p>  cache-based approach							</p>
							<p>  							</p>
							<p>  low frequency							</p>
							<p>  							</p>
							<p>  word pair							</p>
							<p>  							</p>
							<p>  low F-score							</p>
							<p>  							</p>
							<p>  Viterbi alignment							</p>
							<p>  							</p>
							<p> Morin							</p>
							<p>  							</p>
							<p> word alignment							</p>
							<p>  							</p>
							<p> Annual Meeting							</p>
							<p>  							</p>
							<p> machine translation							</p>
							<p>  							</p>
							<p> Models 1-2							</p>
							<p>  							</p>
							<p> generative model							</p>
							<p>  							</p>
							<p> correct sense							</p>
							<p>  							</p>
							<p> Word Sense Induction							</p>
							<p>  							</p>
							<p> good selection							</p>
							<p>  							</p>
							<p> TREC							</p>
							<p>  							</p>
							<p> Language Processing							</p>
							<p>  							</p>
							<p> Machine Translation							</p>
							<p>  							</p>
							<p> cross-lingual							</p>
							<p>  							</p>
							<p> Bayesian							</p>
							<p>  							</p>
</div>
                <div class="subcolumn" name="right-mentions">							<p>  suggestive evidence							</p>
							<p>  							</p>
							<p>  additional difficulty							</p>
							<p>  							</p>
							<p> IBM							</p>
							<p>  							</p>
							<p>  significant gain							</p>
							<p>  							</p>
							<p>  model search							</p>
							<p>  							</p>
							<p>  unsupervised way use topic model							</p>
							<p>  							</p>
							<p>  underlying latent topic							</p>
							<p>  							</p>
							<p>  adapted							</p>
							<p>  							</p>
							<p>  selective use							</p>
							<p>  							</p>
							<p>  Gibbs sampler							</p>
							<p>  							</p>
							<p> Hillary Clinton							</p>
							<p>  							</p>
							<p> HMM							</p>
							<p>  							</p>
							<p> CRF							</p>
							<p>  							</p>
							<p> Ney							</p>
							<p>  							</p>
							<p> Tsujii							</p>
							<p>  							</p>
							<p> suggestive evidence							</p>
							<p>  							</p>
							<p> cross-lingual Word Sense Disambiguation							</p>
							<p>  							</p>
							<p> WSI							</p>
							<p>  							</p>
							<p> Comparative							</p>
							<p>  							</p>
							<p> translation model							</p>
							<p>  							</p>
							<p> word perplexity							</p>
							<p>  							</p>
							<p> word segmentation							</p>
							<p>  							</p>
							<p> speech recognition							</p>
							<p>  							</p>
							<p> Waibel							</p>
							<p>  							</p>
</div>
              </div>
            </div>
          </div>
          <div class="row">
                <div class="column">
                </div>
                <div class="column">
                  <!--<div id="button"><a href="./ldavis_prepared_11.html">See the LDA</a></div> -->
                </div>
          </div>
          <div class="row">
              <button type="button" id="subcollapsible" class="collapsible">Mentions per paper</button>
              <div class="content" name="papers-mentions">
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (1) </button>
									<div class="content"> 
										<p> human mind, English,  statistical translation, language pair, machine translation, Experimental, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (2) </button>
									<div class="content"> 
										<p>BLEU, IBM, machine translation, Ney, Empirical, Experimental, Waibel, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (3) </button>
									<div class="content"> 
										<p>BLEU, English,  low frequency, word alignment, Annual Meeting, Machine Translation, word segmentation, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (4) </button>
									<div class="content"> 
										<p> frequent phenomenon, English, language pair, correct sense, cross-lingual Word Sense Disambiguation, WSD, Word Sense Induction, WSI, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (5) </button>
									<div class="content"> 
										<p></p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (6) </button>
									<div class="content"> 
										<p>English, OOV, machine translation, WSD, TREC, translation model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (7) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English, word alignment, machine translation, generative model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (8) </button>
									<div class="content"> 
										<p>machine translation, speech recognition, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (9) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English, CWS,  new approach, USA, CRF, OOV, machine translation, translation model, Machine Translation, word segmentation, speech recognition, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (10) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  adapted, machine translation, Ney, TSP, translation model, Machine Translation, speech recognition, language model, Waibel, suboptimal solution, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (11) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English,  underlying latent topic,  unsupervised domain,  word pair, LSA, Annual Meeting, machine translation, translation model, Republic, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (12) </button>
									<div class="content"> 
										<p>English,  statistical translation, machine translation, Comparative, system show, speech recognition, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (13) </button>
									<div class="content"> 
										<p> following contribution, English,  adapted, language pair, USA, Empirical, Language Processing, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (14) </button>
									<div class="content"> 
										<p> significant improvement, IBM,  word pair, machine translation, translation model, Experimental, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (15) </button>
									<div class="content"> 
										<p>BLEU, English, Annual Meeting, CRF, machine translation, SgVerb, translation model, Republic, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (16) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  significant gain,  Viterbi alignment, word alignment, HMM, machine translation, Ney, translation model, Machine Translation, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (17) </button>
									<div class="content"> 
										<p>BLEU, SMT, English, word alignment, Machine Translation, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (18) </button>
									<div class="content"> 
										<p>English,  significant gain, Morin, Tsujii, syntactic information, generative model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (19) </button>
									<div class="content"> 
										<p>BLEU, SMT,  dynamic cache,  cache-based approach, machine translation, translation model, Empirical, Language Processing, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (20) </button>
									<div class="content"> 
										<p>BLEU,  suggestive evidence, English, word alignment, USA, machine translation, suggestive evidence, word embeddings, Empirical, Language Processing, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (21) </button>
									<div class="content"> 
										<p>BLEU, SMT, IBM,  Gibbs sampler, language pair, word alignment, HMM, Annual Meeting, machine translation, generative model, Bayesian, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (22) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  Viterbi alignment, language pair, machine translation, Models 1-2, translation model, Language Processing, Machine Translation, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (23) </button>
									<div class="content"> 
										<p>English,  selective use,  new notion, Hillary Clinton, Annual Meeting, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (24) </button>
									<div class="content"> 
										<p>BLEU, SMT, English,  model search,  adapted, language pair, LSA, word alignment, Annual Meeting, machine translation, translation model, word perplexity, Republic, speech recognition, language model, Bayesian, </p> 
									</div> 
</div>
          </div>
          <div class="row">
              <button type="button" id="subcollapsible" class="collapsible">ScatterPlot of Methods (TSNE)</button>
              <div class="content" name="stats2">
                <div class="column">
                  <img src="./men_bigrams.png" alt="plot" width="1000" height="1000">
                </div>
                <div class="column">
                  <img src="./men_trigrams.png" alt="plot" width="1000" height="1000">
                </div>
              </div>
          </div>
        </div>
      </div>

      <button type="button"  class="collapsible">Claims</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_claims">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 6) </strong>	As there is a large overlap between the modeled events in the combined probabilistic models , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation does .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	In such case , as <strong>Hillary Clinton</strong> is a famous female leader , she may be associated with other Chinese female leaders in Chinese corpus , while such association is rarely observed in <strong>English</strong> corpus , which causes asymmetry .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	That is , <strong>Hillary Clinton</strong> is atemporal , as Figure 1 shows , such that using such dissimilarity against deciding this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper validated that considering temporality selectively is helpful for improving the translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	An additional advantage is that the sense clusters often contain more than one translation and , therefore , provide supplementary material for the comparison of the vectors in the target language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Thirdly , reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The beauty of the b<strong>LSA</strong> framework is that the<strong> model search</strong>es for a common latent topic space in an unsupervised fashion , rather than to require manual interaction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	However , LM statistics are sparse , and they are made sparser by morphological variation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	<strong>English</strong> is a weakly inflected language : it has a narrow verbal paradigm , restricted nominal inflection ( plurals and only the vestiges of a case system .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	We assume here that the MT system is capable of providing <strong>word alignment</strong> ( or equivalent ) information during decoding , which is generally true for current statistical MT systems .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 1) </strong>	In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 1) </strong>	Considering our semantic features are the most basic ones , using more sophisticated features ( e.g the head words and their translations of the sourceside semantic roles ) provides a possible direction for further experimentation
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We prove that while <strong>IBM</strong> <strong>Models 1-2</strong> are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The models are independent of the <strong>language pair</strong> and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We also believe that our results may help in the design of effective heuristicsfor some of these tasks .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We believe that decoding algorithms derived from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that both of these problems are easy to solve and provide efficient solutions for them .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We have also shown that alternating maximization can be employed tocome up with O ( m2 ) decoding algorithm .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	<strong>Empirical</strong> evidence suggests that such algorithms can perform resonably well .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	All these numbers suggest that approximative algorithms are a feasible choice for practical applications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	However , the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	<strong>Comparative</strong> evaluations with other translation approaches of the Verbmobil prototype <strong>system show</strong> that the<strong> statistical translation</strong> is superior , especially in the presence of speech input and ungrammatical input
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Even without recognition errors , speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech differ from that of written language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	However , words in a general language corpus like Wikipedia can be polysemous and it is important to identify translations corresponding to their different senses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect that a method capable of identifying the <strong>correct sense</strong> of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons.In this paper , we show how source vectors can be translated into the target language by a <strong><strong>cross-lingual</strong> Word Sense Disambiguation</strong> ( <strong>WSD</strong> ) method which exploits the output of data-driven <strong>Word Sense Induction</strong> ( <strong>WSI</strong> Apidianaki , 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	An avenue that we intend to explore in future work is to extract translations corresponding to different senses of the headwords .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Our experiments are carried out on the <strong>English</strong>-Slovene <strong>language pair</strong> but as the methods are totally data-driven , the approach can be easily applied to other languages.The paper is organized as follows : In the next section , we present some related work on bilingual lexicon extraction from comparable corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Moreover , it is clear that disambiguating the vectors improves the quality of the extracted lexicons and manages to beat the simpler , but yet powerful , most frequent translation heuristic .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One interesting outcome of this study is that<strong> significant gain</strong>s can be obtained by using an association measure that is rarely used in practice .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Still , it is already striking that a direct comparison of them is difficult , if not impossible .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The evaluation has demonstrated that our system is both effective and useful in a real-world environment
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Out-of-vocabulary recognition may have two-sided effects on <strong>SMT</strong> performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	There is also significant disagreement on the specifications , although much of their contents is the same .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 16) </strong>	We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We propose an approach that biases <strong>machine translation</strong> systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	However , there have not been until very recently that the application of mixture modelling in <strong>SMT</strong> has received increasing attention .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Mixture modelling is a standard technique for density estimation , but its use in statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) has just started to be explored .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	One problem with the<strong> dynamic cache</strong> is that those initial sentences in a test document may not benefit from the dynamic cache .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Another problem is that the<strong> dynamic cache</strong> may be prone to noise and cause error propagation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	At least , the topic of a document can help choose specific translation candidates , since when taken out of the context from their document , some words , phrases and even sentences may be rather ambiguous and thus difficult to understand .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple : a human translator often collects similar bilingual document pairs to help translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	During last decade , tremendous work has been done to improve the quality of statistical machineCorresponding author .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	As the translation process continues , the<strong> dynamic cache</strong> grows and contributes more and more to the translation of subsequent sentences .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	However , our analysis has shown that for Arabic , these genres typically contain more Latin script and transliterated words , and thus there is less morphology to score .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	The subject should agree with the verb in both gender and number , but the verb has masculine inflection .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Qc 2012 Association for Computational LinguisticsIt has also been suggested that this setting requires morphological generation because the bitext may not Pron + Fem + <strong>SgVerb</strong> + Masc +3 + PlPrtConj contain all inflected variants ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , using lexical coverage experiments , we show thatit there is ample room for translation quality improvements through better selection of forms that already exist in the <strong>translation model</strong>.they writewilland .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_claims">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	We also believe that our results may help in the design of effective heuristicsfor some of these tasks .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We believe that decoding algorithms derived from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We have also shown that alternating maximization can be employed tocome up with O ( m2 ) decoding algorithm .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One interesting outcome of this study is that<strong> significant gain</strong>s can be obtained by using an association measure that is rarely used in practice .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Still , it is already striking that a direct comparison of them is difficult , if not impossible .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Out-of-vocabulary recognition may have two-sided effects on <strong>SMT</strong> performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 16) </strong>	We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	We prove that while <strong>IBM</strong> <strong>Models 1-2</strong> are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The models are independent of the <strong>language pair</strong> and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that both of these problems are easy to solve and provide efficient solutions for them .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	As there is a large overlap between the modeled events in the combined probabilistic models , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation does .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	There is also significant disagreement on the specifications , although much of their contents is the same .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
      </div>


      <button type="button" class="collapsible">Evidences</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_evidences">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	Specifically , two types of semantic role features are proposed in this paper : a semantic role reordering feature designed to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 716724 , Beijing , August 2010 ture designed to penalize missing semantic roles in the target sentence .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and <strong>Ney</strong> , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with <strong>IBM</strong> Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We furthermore suggest the use of hierarchical lexicon models .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Even without recognition errors , speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech differ from that of written language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 9) </strong>	All these aspects will be our research focus in the future
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Similarly , we can mine Chinese news articles to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Our evaluation results empirically validated the accuracy of our algorithm over real-life datasets , and showed the effectiveness on our proposed perspective
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate this , an <strong>English</strong> news article mentioning Bill Gates and Melinda Gates evidences a relationship between the two entities , which can be quantified from their co-occurrences in the entire English Web corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To summarize , we believe that this paper has the<strong> following contribution</strong>s : We abstract entity translation problem as a graph mapping between entity-relationship graphs in two languages.We develop an effective matching algorithm leveraging both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and <strong>word alignment</strong> over a high-quality MT baseline1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Tightly integrate joint bilingual name tagging into MT training by coordinating tagged604 Proceedings of the 51st <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline , and a new name-aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Therefore , LL may fare better in an indirect approach , like the one in ( Daille and <strong>Morin</strong> , 2005 Moreover , we have seen that the cosine similarity measure and sentence contexts give moreLO is used .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	While the present work does not investigate all the parameters that could potentially impact results , we believe it constitutes the most complete and systematic comparison made so far with variants of the context-based projection approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	Speech recognition , speech synthesis , and <strong>machine translation</strong> research started about half a century ago .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We also found the correlation between the <strong>CWS</strong> F-score and <strong>SMT</strong> <strong>BLEU</strong> score was very weak .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	This approach produced a<strong> significant improvement</strong> in translation andachieved the best <strong>BLEU</strong> score of all the <strong>CWS</strong>schemes .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Secondly , we investigated the advantages and disadvantages of various <strong>CWS</strong> approaches , both dictionary-based and <strong>CRF</strong>-based , and built CWSs using these approaches to examine their effect on translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in<strong> significant improvement</strong>s of up to 1 <strong>BLEU</strong> and 3 TER on Chinese to <strong>English</strong> translation over a strong baseline .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a<strong> word pair</strong> under topic zn : features in the <strong>translation model</strong> , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f 1 ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Civera and Juan , 2006 a mixture extension of <strong>IBM</strong> model 2 along with a specific dynamicprogramming decoding algorithm were proposed .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	3 Mixture of <strong>HMM</strong> alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Section 3 presents our<strong> cache-based approach</strong> to documentlevel <strong>SMT</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We evaluated the<strong> adapted</strong> LM on <strong>SMT</strong> and found that the evaluation metrics are crucial to reflect the actual improvement in performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	In Section 3 , we present the effect of LM adaptation on <strong>word perplexity</strong> , followed by <strong>SMT</strong> experiments reported in <strong>BLEU</strong> on both speech and text input in Section 3.3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We will investigate the incorporation of monolingual documents for potentially better bilingual <strong>LSA</strong> modeling
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	We introduce bilingual <strong>word embeddings</strong> : semantic embeddings associated across two languages in the context of neural <strong>language model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	We achieved best results when the model training data , MT tuning set , and MT evaluation set conThe bottom category includes all lexical items that the decoder could produce in a translation of the source .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Qc 2012 Association for Computational LinguisticsIt has also been suggested that this setting requires morphological generation because the bitext may not Pron + Fem + <strong>SgVerb</strong> + Masc +3 + PlPrtConj contain all inflected variants ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , using lexical coverage experiments , we show thatit there is ample room for translation quality improvements through better selection of forms that already exist in the <strong>translation model</strong>.they writewilland .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	In their seminal paper on <strong>SMT</strong> , Brown and his colleagues highlighted the problems we face as we go from <strong>IBM</strong> <strong>Models 1-2</strong> to 3-5 ( Brown et al 1993 ) 3 : As we progress from Model 1 to Model 5 , evaluating the expectations that gives us counts becomes increasingly difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	In ( Knight , 1999 ) it was proved that the Exact Decoding prob Given the model parameters and a sentence f determine the most probable translation of f lem is NP-Hard when the <strong>language model</strong> is a bigram model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We believe that our results on the computational complexity of the tasks in <strong>SMT</strong> will result in a better understanding of these tasks from a theoretical perspective .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	In practice , we are never sure that we have found the<strong> Viterbi alignment</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	Our results showthat there can not exist a closed form expression ( whose representation is polynomial in the size of the input ) for P ( f | e ) and the counts in the EMiterations for Models 3-5 unless P NP
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	around the<strong> Viterbi alignment</strong> , i.e. in determining j = 1 tfj | eajdj : aj I = 0 d ( j | i , m the goodness of the Viterbi alignment in compar ison to the rest of the alignments.Decoding is an integral component of all <strong>SMT</strong> systems ( Wang , Table 1 : <strong>IBM</strong> Model 3 Here , n ( | e ) is the fertility model , t ( f | e ) is the lexicon model and d ( j | i , m is the distortion model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	enomial time solution for any of these hardeproblems ( unless P NP and P #P P our results highlight and justify the need for developing polynomial time approximations for these computations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.<strong>Experimental</strong> data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	We present improvements to a greedy decoding algorithm for statistical <strong>machine translation</strong> that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Using the same evaluation metric ( but different evaluation data Wang and <strong>Waibel</strong> report search error rates of 7.9 percent and 9.3 respectively , for their decoders .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	<strong>IBM</strong> Model 4 scores and the <strong>BLEU</strong> metric .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	The times shown are averages of 100 sentences each for length10 , 20 80 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Operations not included in the figures consume so little time that their plots can not be discerned in the graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for <strong>machine translation</strong> with <strong>IBM</strong> Model 4.6 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	In addition , we perform a rescoring of-Best lists using our maximum entropy model and thereby yield an improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We have been able to obtain a significant better test corpus perplexity and also a slight improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	This additional information could be : Simple context information : information of the words surrounding the<strong> word pair</strong> ; Syntactic information : part-of-speech information , syntactic constituent , sentence Semantic information : disambiguation information ( e.g.from WordNet currentspeech or dialog act .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	Other authors have applied this approach to <strong>language model</strong>ing ( Rosenfeld , 1996 ; Martin et al 1999 ; Peters and Klakow , 1999 A short review of the maximum entropy approach is outlined in Section 3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	<strong>Experimental</strong> results on the German-<strong>English</strong> Verbmobil Source Language Textmorpho-syntactic AnalysisTransformation f J 1 Global Search : maximize Pr ( e I J e I task are reported .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	In our approach we introduce equivalence classes in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	It has been successfully applied to realistic tasks in various national and international research programs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	The experience obtained in the Verbmobil project , in particular a large-scale end-to-end evaluation , showed that the statistical approach resulted in significantly lower error rates than three competing translation approaches : the sentence error rate was 29percent in comparison with 52percent to 62percent for the other translation approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	In comparison with written language , speech and especially spontaneous speech poses additional difficulties for the task of automatic translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Starting with the Bayes decision rule as in <strong>speech recognition</strong> , we show how the required probability distributions can be structured into three parts : the <strong>language model</strong> , the alignment model and the lexicon model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	<strong>Comparative</strong> evaluations with other translation approaches of the Verbmobil prototype <strong>system show</strong> that the<strong> statistical translation</strong> is superior , especially in the presence of speech input and ungrammatical input
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	We describe the components of the system and report results on the Verbmobil task .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	We validate this<strong> selective use</strong> of temporal features boosts the accuracy by 6.1 percent .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	We developed a classifier to distinguish temporalentities and our proposed method outperforms the state-of-the-art approach by 6.1
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	In contrast , Figure 1 illustrates asymmetry , by showing the frequencies of Usain Bolt , a Jamaican sprinter , and <strong>Hillary Clinton</strong> , an American politician , in comparable news articles during the year 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	To overcome such problems , we propose a<strong> new notion</strong> of selective temporality ( called this fea 2.3 Step 3 : Reinforcement We reinforce R0 by leveraging R and obtain a converged matrix R using the following model : ture ST to distinguish from T ) to automatically distinguish temporal and atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	Early efforts of named entity translation have focused on using phonetic feature ( called PH ) to estimate a phonetic similarity between two names ( Knight and Graehl , 1998 ; Li et al 2004 ; Virga and Khudanpur , 2003 In contrast , some approaches have focused on using context feature ( called CX ) which compares surrounding words of entities ( Fung and Yee , 1998 ; Diab and Finch , 2000 ; Laroche and Langlais , 2010 Recently , holistic approaches combining such similarities have been studied ( Shao and Ng , 2004 ; You et al 2010 ; Kim et al 2011 Shao and Ng , 2004 ) rank translation candidates using PH and CX independently and return results with the highest average rank You et al 2010 ) compute initial translation scores using PH and iteratively update the scores using relationship feature ( called R Kim et al 2011 ) boost Yous approach by additionally leveraging CX .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	That is , <strong>Hillary Clinton</strong> is atemporal , as Figure 1 shows , such that using such dissimilarity against deciding this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 9) </strong>	From the experimental results for combining our <strong>OOV</strong> term <strong>translation model</strong> with <strong>English</strong>-Chinese CrossLanguage Information Retrieval ( CLIR ) on the data sets of Text Retrieval Evaluation Conference ( <strong>TREC</strong> it can be found that the obvious performance improvement for both query translation and retrieval can also be obtained .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 4 reports experimental results and Section 5 concludes our work.<strong>English</strong> PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To highlight the advantage of our proposed approach , we compare our results with commercial machine translators Engkoo3 developed in Microsoft Research Asia and Google Translator4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To achieve this goal , we developed a graph alignment algorithm that iteratively reinforces the matching similarity exploiting relational similarity and then extracts correct matches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	1com430 Proceedings of the 2010 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 430439 , MIT , Massachusetts , <strong>USA</strong> , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Such engine This work was done when the first two authors visited Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 3 then develops our framework .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate , Figure 1 demonstrates the query result for Bill Gates , retrieving and visualizing the entity-relationship graph of related people names that frequently co-occur with Bill in <strong>English</strong> corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	While high quality entity translation is essential in <strong>cross-lingual</strong> information access and trans lation , it is non-trivial to achieve , due to the challenge that entity translation , though typically bearing pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing efforts to address these challenges can be categorized into transliterationand corpusbased approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We have shown how <strong>cross-lingual</strong> <strong>WSD</strong> can be applied to bilingual lexicon extraction from comparable corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect that a method capable of identifying the <strong>correct sense</strong> of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons.In this paper , we show how source vectors can be translated into the target language by a <strong><strong>cross-lingual</strong> Word Sense Disambiguation</strong> ( <strong>WSD</strong> ) method which exploits the output of data-driven <strong>Word Sense Induction</strong> ( <strong>WSI</strong> Apidianaki , 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	In Section 5 , we report and discuss the obtained results before concluding and presenting some directions for future work .1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora , pages 110 , Sofia , Bulgaria , August 8 , 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Qc 2013 Association for Computational Linguistics names in parallel corpora , updating <strong>word segmentation</strong> , <strong>word alignment</strong> and grammar extraction ( Section 3.1 We developed a name-aware MT framework which tightly integrates name tagging and name translation into training and decoding of MT. Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genres .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Compared to previous methods , the novel contributions of our approach are : 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	This need can be addressed in part by <strong>cross-lingual</strong> information access tasks such as entity linking ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lies in the performance of <strong>Machine Translation</strong> ( MT Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information , especially names .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The experimental pro tocol we followed is described in Section 3 and we analyze our results in Section 4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Instead , we investigate the impact of some major factors influencing projection-based approaches on a task of translating 5,000 terms of the medical domain ( the most studied domain making use of French and <strong>English</strong> Wikipedia pages extracted monolingually thanks to an information retrieval engine .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Among the latter , many are translating single-word terms ( Chiao and Zweigenbaum , 2002 ; Dejean et al 2005 ; Prochasson et 1 A stoplist is typically used in order to prevent function words from populating the context vectors .617 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 617625 , Beijing , August 2010 al 2009 while others are tackling the translation of multi-word terms ( Daille and <strong>Morin</strong> , 2005 The type of discourse might as well be of concern in some of the studies dedicated to bilingual terminology mining .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Our results show that using the log-odds ratio as the association measure allows for significantly better translation spotting than the log-likelihood .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The present discussion only focuses on a few number of representative studies .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The highest Top 1 precision , 55.2 was reached with the following parameters : sentence contexts , LO , cosine and a 9,000-entry mixed lexicon , with the use of a cognate heuristic .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Naturally , studies differ in the way each cooccurrence ( either window or syntax-based ) is weighted , and a plethora of association scores have been investigated and compared , the likelihood score ( Dunning , 1993 ) being among the most popular .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	While a few studies have investigated pattern matching approaches to compare source and target contexts ( Fung , 1995 ; Diab and Finch , 2000 ; Yu and <strong>Tsujii</strong> , 2009 most variants make use of a bilingual lexicon in order to translate the words of the context of a term ( often called seed words Dejean et al 2005 ) instead use a bilingual thesaurus for translating these.Another distinction between approaches lies in the way the context is defined .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The feasibility of speech-to-speech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The evaluation has demonstrated that our system is both effective and useful in a real-world environment
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	After groundbreaking work for two decades , corpus-based speech and language processing technology have recently enabled the achievement of speech-to-speech translation that is usable in the real world .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We interpolated multiple <strong>translation model</strong>s generated by the <strong>CWS</strong> schemes and found our approaches were very effective in improving the translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We found these approaches were very effective in improving quality of translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We tested dictionarybased and <strong>CRF</strong>-based approaches and found there was no significant difference between the two in the qualty of the resulting translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We have published a much more detailed paper ( Zhang et al 2008 ) to describe the relations between <strong>CWS</strong> and <strong>SMT</strong>
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We proposed a<strong> new approach</strong> to linear interpolation of translation features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We investigated the effect of <strong>CWS</strong> on <strong>SMT</strong> from two points of view .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Which approach pro 1 A <strong>CWS</strong> competition organized by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical <strong>Machine Translation</strong> , pages 216223 , Columbus , Ohio , <strong>USA</strong> , June 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	In this work , we also propose approaches to make use of all the Sighan training data regardless of the specifications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We grouped all of the <strong>CWS</strong> methods into two classes : the class without out-of-vocabulary ( <strong>OOV</strong> ) recognition and the class with OOV recognition , represented by the dictionary-based CWS and the <strong>CRF</strong>-based CWS , respectively .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Chinese <strong>word segmentation</strong> ( <strong>CWS</strong> ) is a necessary step in Chinese-<strong>English</strong> statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) and its performance has an impact on the results of SMT .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	On the other hand , the dictionarybased approach that does not support <strong>OOV</strong> recognition produced a lower F-score , but with a relatively weak data spareness problem .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In this work , we consider the<strong> underlying latent topic</strong>s of the documents ( Blei et al 2003 Topic modeling has received some use in <strong>SMT</strong> , for instance Bilingual <strong>LSA</strong> adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which uses a bilingual topic model for learning alignment .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We induce<strong> unsupervised domain</strong>s from large corpora , and we incorporate soft , probabilistic domain membership into a <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of <strong>word alignment</strong> in European languages , and the efficient and exact computation of the E-step and<strong> Viterbi alignment</strong> by using a dynamic-programming approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical <strong>translation model</strong>s , taking <strong>IBM</strong> Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Statistical <strong>machine translation</strong> systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time , ignoring document-level information .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	He proposed to employ cache-based language and <strong>translation model</strong>s in a phrase-based <strong>SMT</strong> system for domain909 Proceedings of the 2011 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	First of all , most of parallel corpora lack the annotation of document boundaries ( Tam , 2007 Secondly , although it is easy to incorporate a new feature into the classical log-linear model ( Och , 2003 it is difficult to capture document-level information and model it via some simple features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In order to resolve this problem , this paper employs a topic model to weaken those noisybilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In particular , three new features are designed to explore various kinds of document-level information in above three kinds of caches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Evaluation shows the effectiveness of our<strong> cache-based approach</strong> to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We organize the paper as follows : In Section 2 , we introduce the b<strong>LSA</strong> framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The key property of the b<strong>LSA</strong> model is that Proceedings of the 45th <strong>Annual Meeting</strong> of the Association of Computational Linguistics , pages 520527 , Prague , Czech <strong>Republic</strong> , June 2007 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	Results showed that our approach significantly reduces the <strong>word perplexity</strong> on the target language in both cases using ASR hypotheses and manual transcripts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	On Chinese to <strong>English</strong> speech and text translation the proposed b<strong>LSA</strong> framework successfully reduced <strong>word perplexity</strong> of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	We show good performance on Chinese semantic similarity with bilingual trained embeddings .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	On NIST08 Chinese-<strong>English</strong> translation task , we obtain an improvement of 0.48 <strong>BLEU</strong> from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .1393 Proceedings of the 2013 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 13931398 , Seattle , Washington , <strong>USA</strong> , 18-21 October 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	When used to compute semantic similarity of phrase pairs , bilingual embeddings improve NIST08 end-to-end <strong>machine translation</strong> results by just below half a <strong>BLEU</strong> point .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	fem The car goes quickly Figure 1 : Ungrammatical Arabic output of Google Translate for the <strong>English</strong> input The car goes quickly .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	( 1 ll .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	For <strong>English</strong>-to-Arabic translation , we achieve a +1.04 <strong>BLEU</strong> average improvement by tiling our model on top of a large LM .146 Proceedings of the 50th <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 146155 , Jeju , <strong>Republic</strong> of Korea , 8-14 July 2012 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Consider the output of Google Translate for the simple <strong>English</strong> sentence in Fig. 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Agreement relations that cross statistical phrase boundaries are not explicitly modeled in most phrasebased MT systems ( Avramidis and Koehn , 2008 We address this shortcoming with an agreement model that scores sequences of fine-grained morphosyntactic classes .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_evidences">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 5) </strong>	This additional information could be : Simple context information : information of the words surrounding the<strong> word pair</strong> ; Syntactic information : part-of-speech information , syntactic constituent , sentence Semantic information : disambiguation information ( e.g.from WordNet currentspeech or dialog act .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	Other authors have applied this approach to <strong>language model</strong>ing ( Rosenfeld , 1996 ; Martin et al 1999 ; Peters and Klakow , 1999 A short review of the maximum entropy approach is outlined in Section 3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	<strong>Experimental</strong> results on the German-<strong>English</strong> Verbmobil Source Language Textmorpho-syntactic AnalysisTransformation f J 1 Global Search : maximize Pr ( e I J e I task are reported .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We furthermore suggest the use of hierarchical lexicon models .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Our evaluation results empirically validated the accuracy of our algorithm over real-life datasets , and showed the effectiveness on our proposed perspective
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To achieve this goal , we developed a graph alignment algorithm that iteratively reinforces the matching similarity exploiting relational similarity and then extracts correct matches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Such engine This work was done when the first two authors visited Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Tightly integrate joint bilingual name tagging into MT training by coordinating tagged604 Proceedings of the 51st <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	This need can be addressed in part by <strong>cross-lingual</strong> information access tasks such as entity linking ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lies in the performance of <strong>Machine Translation</strong> ( MT Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information , especially names .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We interpolated multiple <strong>translation model</strong>s generated by the <strong>CWS</strong> schemes and found our approaches were very effective in improving the translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We investigated the effect of <strong>CWS</strong> on <strong>SMT</strong> from two points of view .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a<strong> word pair</strong> under topic zn : features in the <strong>translation model</strong> , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f 1 ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In this work , we consider the<strong> underlying latent topic</strong>s of the documents ( Blei et al 2003 Topic modeling has received some use in <strong>SMT</strong> , for instance Bilingual <strong>LSA</strong> adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which uses a bilingual topic model for learning alignment .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We induce<strong> unsupervised domain</strong>s from large corpora , and we incorporate soft , probabilistic domain membership into a <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Civera and Juan , 2006 a mixture extension of <strong>IBM</strong> model 2 along with a specific dynamicprogramming decoding algorithm were proposed .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical <strong>translation model</strong>s , taking <strong>IBM</strong> Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	He proposed to employ cache-based language and <strong>translation model</strong>s in a phrase-based <strong>SMT</strong> system for domain909 Proceedings of the 2011 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In order to resolve this problem , this paper employs a topic model to weaken those noisybilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In particular , three new features are designed to explore various kinds of document-level information in above three kinds of caches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Section 3 presents our<strong> cache-based approach</strong> to documentlevel <strong>SMT</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Evaluation shows the effectiveness of our<strong> cache-based approach</strong> to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	Results showed that our approach significantly reduces the <strong>word perplexity</strong> on the target language in both cases using ASR hypotheses and manual transcripts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We will investigate the incorporation of monolingual documents for potentially better bilingual <strong>LSA</strong> modeling
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	Specifically , two types of semantic role features are proposed in this paper : a semantic role reordering feature designed to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 716724 , Beijing , August 2010 ture designed to penalize missing semantic roles in the target sentence .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	In addition , we perform a rescoring of-Best lists using our maximum entropy model and thereby yield an improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We have been able to obtain a significant better test corpus perplexity and also a slight improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	In our approach we introduce equivalence classes in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	It has been successfully applied to realistic tasks in various national and international research programs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 4 reports experimental results and Section 5 concludes our work.<strong>English</strong> PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Similarly , we can mine Chinese news articles to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To highlight the advantage of our proposed approach , we compare our results with commercial machine translators Engkoo3 developed in Microsoft Research Asia and Google Translator4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	1com430 Proceedings of the 2010 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 430439 , MIT , Massachusetts , <strong>USA</strong> , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 3 then develops our framework .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate , Figure 1 demonstrates the query result for Bill Gates , retrieving and visualizing the entity-relationship graph of related people names that frequently co-occur with Bill in <strong>English</strong> corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	While high quality entity translation is essential in <strong>cross-lingual</strong> information access and trans lation , it is non-trivial to achieve , due to the challenge that entity translation , though typically bearing pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing efforts to address these challenges can be categorized into transliterationand corpusbased approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate this , an <strong>English</strong> news article mentioning Bill Gates and Melinda Gates evidences a relationship between the two entities , which can be quantified from their co-occurrences in the entire English Web corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To summarize , we believe that this paper has the<strong> following contribution</strong>s : We abstract entity translation problem as a graph mapping between entity-relationship graphs in two languages.We develop an effective matching algorithm leveraging both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Qc 2013 Association for Computational Linguistics names in parallel corpora , updating <strong>word segmentation</strong> , <strong>word alignment</strong> and grammar extraction ( Section 3.1 We developed a name-aware MT framework which tightly integrates name tagging and name translation into training and decoding of MT. Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genres .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Compared to previous methods , the novel contributions of our approach are : 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and <strong>word alignment</strong> over a high-quality MT baseline1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline , and a new name-aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	Speech recognition , speech synthesis , and <strong>machine translation</strong> research started about half a century ago .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The feasibility of speech-to-speech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We also found the correlation between the <strong>CWS</strong> F-score and <strong>SMT</strong> <strong>BLEU</strong> score was very weak .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We found these approaches were very effective in improving quality of translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We tested dictionarybased and <strong>CRF</strong>-based approaches and found there was no significant difference between the two in the qualty of the resulting translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	This approach produced a<strong> significant improvement</strong> in translation andachieved the best <strong>BLEU</strong> score of all the <strong>CWS</strong>schemes .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We have published a much more detailed paper ( Zhang et al 2008 ) to describe the relations between <strong>CWS</strong> and <strong>SMT</strong>
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Secondly , we investigated the advantages and disadvantages of various <strong>CWS</strong> approaches , both dictionary-based and <strong>CRF</strong>-based , and built CWSs using these approaches to examine their effect on translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We proposed a<strong> new approach</strong> to linear interpolation of translation features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Which approach pro 1 A <strong>CWS</strong> competition organized by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical <strong>Machine Translation</strong> , pages 216223 , Columbus , Ohio , <strong>USA</strong> , June 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	In this work , we also propose approaches to make use of all the Sighan training data regardless of the specifications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We grouped all of the <strong>CWS</strong> methods into two classes : the class without out-of-vocabulary ( <strong>OOV</strong> ) recognition and the class with OOV recognition , represented by the dictionary-based CWS and the <strong>CRF</strong>-based CWS , respectively .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Chinese <strong>word segmentation</strong> ( <strong>CWS</strong> ) is a necessary step in Chinese-<strong>English</strong> statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) and its performance has an impact on the results of SMT .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	On the other hand , the dictionarybased approach that does not support <strong>OOV</strong> recognition produced a lower F-score , but with a relatively weak data spareness problem .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in<strong> significant improvement</strong>s of up to 1 <strong>BLEU</strong> and 3 TER on Chinese to <strong>English</strong> translation over a strong baseline .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of <strong>word alignment</strong> in European languages , and the efficient and exact computation of the E-step and<strong> Viterbi alignment</strong> by using a dynamic-programming approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	3 Mixture of <strong>HMM</strong> alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Statistical <strong>machine translation</strong> systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time , ignoring document-level information .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	First of all , most of parallel corpora lack the annotation of document boundaries ( Tam , 2007 Secondly , although it is easy to incorporate a new feature into the classical log-linear model ( Och , 2003 it is difficult to capture document-level information and model it via some simple features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We evaluated the<strong> adapted</strong> LM on <strong>SMT</strong> and found that the evaluation metrics are crucial to reflect the actual improvement in performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	In Section 3 , we present the effect of LM adaptation on <strong>word perplexity</strong> , followed by <strong>SMT</strong> experiments reported in <strong>BLEU</strong> on both speech and text input in Section 3.3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We organize the paper as follows : In Section 2 , we introduce the b<strong>LSA</strong> framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The key property of the b<strong>LSA</strong> model is that Proceedings of the 45th <strong>Annual Meeting</strong> of the Association of Computational Linguistics , pages 520527 , Prague , Czech <strong>Republic</strong> , June 2007 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	On Chinese to <strong>English</strong> speech and text translation the proposed b<strong>LSA</strong> framework successfully reduced <strong>word perplexity</strong> of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .
							</p>
							<p>  							</p>
					</div>
</div>
            </div>
          </div>
        </div>
      </div>


      <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
              content.style.display = "none";
            } else {
              content.style.display = "block";
            }
          });
        }
      </script>

    </body>
</html>
