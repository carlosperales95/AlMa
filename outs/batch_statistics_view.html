<html>
    <head>
      <style type="text/css">

        p {
          font-size: 14px;
        }
        .column {
            float: left;
            width: 50%;
            padding:0%;
            margin:0%;

        }
        .subcolumn {
            float: left;
            width: 30%;
            padding:0%;
            margin:0%;

        }
        .tricolumn {
            float: left;
            width: 33%;
            padding:0%;
            margin:0%;

        }
        .greatcolumn {
            float: left;
            width: 70%;
            padding:0%;
            margin:0%;

        }

        .row {
          width:100%;
          padding:0%;
          margin:0%;

        }

        img {

          max-width: 96%;
        }

        .row:after {
            content: "";
            display: table;
            clear: both;
        }
        .collapsible {
          background-color: #66ccff;
          color: #444;
          cursor: pointer;
          padding: 18px;
          width: 100%;
          border: none;
          text-align: left;
          outline: none;
          font-size: 15px;
        }

        .active, .collapsible:hover {
          background-color: #ccc;
        }

        .content {
          padding:1% !important;
          padding: 0 18px;
          display: none;
          overflow: hidden;
          background-color: #f1f1f1;
          max-width: 100%;
          margin:0%;

        }

        #subcollapsible {
          background-color: #4bab81 !important;

        }

        #subsubcollapsible {
          background-color: #81a118 !important;

        }
      </style>
    </head>
    <body>

      <button type="button" class="collapsible" name="topicsquery">Paper results for query: <strong> (translation sentences ) </strong> </button>
      <div class="content">
        <div class="row">
          <div class="column" name="paper_titles">
							<p> <strong>(1)</strong> - ﻿Semantic Role Features for Machine Translation
 
  							</p>
							<p> <strong>(2)</strong> - ﻿Computational Complexity of Statistical Machine  Translation
 
  							</p>
							<p> <strong>(3)</strong> - ﻿An Algorithmic Framework for the Decoding Problem in
 Statistical Machine Translation
 
  							</p>
							<p> <strong>(4)</strong> - ﻿Greedy Decoding for Statistical Machine Translation in Almost Linear Time
 
  							</p>
							<p> <strong>(5)</strong> - ﻿Refined Lexicon Models for Statistical Machine Translation using a
 Maximum Entropy Approach
 
  							</p>
							<p> <strong>(6)</strong> - ﻿Toward hierarchical models for statistical machine translation of 
 inflected languages
 
  							</p>
							<p> <strong>(7)</strong> - ﻿The RWTH System for Statistical Translation of Spoken
 Dialogues
 
  							</p>
							<p> <strong>(8)</strong> - ﻿Enriching Entity Translation  Discovery using Selective Temporality
 
  							</p>
							<p> <strong>(9)</strong> - ﻿Fusion of Multiple Features and Ranking SVM for
 Web-based English-Chinese OOV Term Translation
 
  							</p>
							<p> <strong>(10)</strong> - ﻿Mining Name Translations from Entity Graph Mapping∗
 
  							</p>
							<p> <strong>(11)</strong> - ﻿Cross-lingual WSD for Translation Extraction 
 from Comparable Corpora
 
  							</p>
							<p> <strong>(12)</strong> - ﻿Name-aware Machine Translation
 
 Haibo Li†	Jing Zheng‡	Heng Ji†	Qi Li†	Wen Wang‡
 
  							</p>
							<p> <strong>(13)</strong> - ﻿Revisiting Context-based Projection Methods for
 Term-Translation Spotting in Comparable Corpora
 
  							</p>
							<p> <strong>(14)</strong> - ﻿
 NICT-ATR Speech-to-Speech Translation System
 
  							</p>
							<p> <strong>(15)</strong> - ﻿Improved Statistical Machine Translation by Multiple Chinese Word
 Segmentation
 
  							</p>
							<p> <strong>(16)</strong> - ﻿Bayesian Word Alignment for Statistical Machine Translation
 
  							</p>
							<p> <strong>(17)</strong> - ﻿Topic Models for Dynamic Translation Model Adaptation
 
  							</p>
							<p> <strong>(18)</strong> - ﻿Domain Adaptation in Statistical Machine Translation with Mixture
 Modelling ∗
 
  							</p>
							<p> <strong>(19)</strong> - ﻿Cache-based Document-level Statistical Machine Translation
 
  							</p>
							<p> <strong>(20)</strong> - ﻿Bilingual-LSA Based LM Adaptation for Spoken Language Translation
 
  							</p>
							<p> <strong>(21)</strong> - ﻿Bilingual Word Embeddings for Phrase-Based Machine Translation
 
  							</p>
							<p> <strong>(22)</strong> - ﻿The Karlsruhe Institute of Technology Translation Systems 
 for the WMT 2011
 
  							</p>
							<p> <strong>(23)</strong> - ﻿A Class-Based Agreement Model for
 Generating Accurately Inflected Translations
 
  							</p>
							<p> <strong>(24)</strong> - ﻿Discovering Commonsense Entailment  Rules Implicit in Sentences
 
  							</p>
</div>
          <div class="column">
            <button type="button" class="collapsible">Found Technologies/Methods</button>
            <div class="content" name="mentions">
              <div class="row">
                <div class="subcolumn" name="left-mentions">							<p> BLEU							</p>
							<p>  							</p>
							<p>  human mind							</p>
							<p>  							</p>
							<p>  following contribution							</p>
							<p>  							</p>
							<p> English							</p>
							<p>  							</p>
							<p>  good association measure							</p>
							<p>  							</p>
							<p>  dynamic cache							</p>
							<p>  							</p>
							<p>  statistical translation							</p>
							<p>  							</p>
							<p>  unsupervised domain							</p>
							<p>  							</p>
							<p>  new approach							</p>
							<p>  							</p>
							<p>  new notion							</p>
							<p>  							</p>
							<p> language pair							</p>
							<p>  							</p>
							<p> LSA							</p>
							<p>  							</p>
							<p> USA							</p>
							<p>  							</p>
							<p> OOV							</p>
							<p>  							</p>
							<p> TSP							</p>
							<p>  							</p>
							<p> syntactic information							</p>
							<p>  							</p>
							<p> word embeddings							</p>
							<p>  							</p>
							<p> WSD							</p>
							<p>  							</p>
							<p> SgVerb							</p>
							<p>  							</p>
							<p> system show							</p>
							<p>  							</p>
							<p> Empirical							</p>
							<p>  							</p>
							<p> Republic							</p>
							<p>  							</p>
							<p> Experimental							</p>
							<p>  							</p>
							<p> language model							</p>
							<p>  							</p>
							<p> suboptimal solution							</p>
							<p>  							</p>
</div>
                <div class="subcolumn" name="center-mentions">							<p> SMT							</p>
							<p>  							</p>
							<p>  frequent phenomenon							</p>
							<p>  							</p>
							<p>  significant improvement							</p>
							<p>  							</p>
							<p> CWS							</p>
							<p>  							</p>
							<p>  algorithmic handle provide							</p>
							<p>  							</p>
							<p>  cache-based approach							</p>
							<p>  							</p>
							<p>  low frequency							</p>
							<p>  							</p>
							<p>  word pair							</p>
							<p>  							</p>
							<p>  low F-score							</p>
							<p>  							</p>
							<p>  Viterbi alignment							</p>
							<p>  							</p>
							<p> Morin							</p>
							<p>  							</p>
							<p> word alignment							</p>
							<p>  							</p>
							<p> Annual Meeting							</p>
							<p>  							</p>
							<p> machine translation							</p>
							<p>  							</p>
							<p> Models 1-2							</p>
							<p>  							</p>
							<p> generative model							</p>
							<p>  							</p>
							<p> correct sense							</p>
							<p>  							</p>
							<p> Word Sense Induction							</p>
							<p>  							</p>
							<p> good selection							</p>
							<p>  							</p>
							<p> TREC							</p>
							<p>  							</p>
							<p> Language Processing							</p>
							<p>  							</p>
							<p> Machine Translation							</p>
							<p>  							</p>
							<p> cross-lingual							</p>
							<p>  							</p>
							<p> Bayesian							</p>
							<p>  							</p>
</div>
                <div class="subcolumn" name="right-mentions">							<p>  suggestive evidence							</p>
							<p>  							</p>
							<p>  additional difficulty							</p>
							<p>  							</p>
							<p> IBM							</p>
							<p>  							</p>
							<p>  significant gain							</p>
							<p>  							</p>
							<p>  model search							</p>
							<p>  							</p>
							<p>  unsupervised way use topic model							</p>
							<p>  							</p>
							<p>  underlying latent topic							</p>
							<p>  							</p>
							<p>  adapted							</p>
							<p>  							</p>
							<p>  selective use							</p>
							<p>  							</p>
							<p>  Gibbs sampler							</p>
							<p>  							</p>
							<p> Hillary Clinton							</p>
							<p>  							</p>
							<p> HMM							</p>
							<p>  							</p>
							<p> CRF							</p>
							<p>  							</p>
							<p> Ney							</p>
							<p>  							</p>
							<p> Tsujii							</p>
							<p>  							</p>
							<p> suggestive evidence							</p>
							<p>  							</p>
							<p> cross-lingual Word Sense Disambiguation							</p>
							<p>  							</p>
							<p> WSI							</p>
							<p>  							</p>
							<p> Comparative							</p>
							<p>  							</p>
							<p> translation model							</p>
							<p>  							</p>
							<p> word perplexity							</p>
							<p>  							</p>
							<p> word segmentation							</p>
							<p>  							</p>
							<p> speech recognition							</p>
							<p>  							</p>
							<p> Waibel							</p>
							<p>  							</p>
</div>
              </div>
            </div>
          </div>
          <div class="row">
                <div class="column">
                </div>
                <div class="column">
                  <!--<div id="button"><a href="./ldavis_prepared_11.html">See the LDA</a></div> -->
                </div>
          </div>
          <div class="row">
              <button type="button" id="subcollapsible" class="collapsible"></button>
              <div class="content" name="">
              </div>
          </div>
          <div class="row">
            <div class="subcolumn">
              <button type="button" id="subcollapsible" class="collapsible">Mentions per paper</button>
              <div class="content" name="papers-mentions">
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (1) </button>
									<div class="content"> 
										<p> human mind, English,  statistical translation, language pair, machine translation, Experimental, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (2) </button>
									<div class="content"> 
										<p>BLEU, IBM, machine translation, Ney, Empirical, Experimental, Waibel, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (3) </button>
									<div class="content"> 
										<p>BLEU, English,  low frequency, word alignment, Annual Meeting, Machine Translation, word segmentation, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (4) </button>
									<div class="content"> 
										<p> frequent phenomenon, English, language pair, correct sense, cross-lingual Word Sense Disambiguation, WSD, Word Sense Induction, WSI, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (5) </button>
									<div class="content"> 
										<p></p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (6) </button>
									<div class="content"> 
										<p>English, OOV, machine translation, WSD, TREC, translation model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (7) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English, word alignment, machine translation, generative model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (8) </button>
									<div class="content"> 
										<p>machine translation, speech recognition, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (9) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English, CWS,  new approach, USA, CRF, OOV, machine translation, translation model, Machine Translation, word segmentation, speech recognition, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (10) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  adapted, machine translation, Ney, TSP, translation model, Machine Translation, speech recognition, language model, Waibel, suboptimal solution, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (11) </button>
									<div class="content"> 
										<p>BLEU, SMT,  significant improvement, English,  underlying latent topic,  unsupervised domain,  word pair, LSA, Annual Meeting, machine translation, translation model, Republic, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (12) </button>
									<div class="content"> 
										<p>English,  statistical translation, machine translation, Comparative, system show, speech recognition, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (13) </button>
									<div class="content"> 
										<p> following contribution, English,  adapted, language pair, USA, Empirical, Language Processing, cross-lingual, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (14) </button>
									<div class="content"> 
										<p> significant improvement, IBM,  word pair, machine translation, translation model, Experimental, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (15) </button>
									<div class="content"> 
										<p>BLEU, English, Annual Meeting, CRF, machine translation, SgVerb, translation model, Republic, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (16) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  significant gain,  Viterbi alignment, word alignment, HMM, machine translation, Ney, translation model, Machine Translation, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (17) </button>
									<div class="content"> 
										<p>BLEU, SMT, English, word alignment, Machine Translation, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (18) </button>
									<div class="content"> 
										<p>English,  significant gain, Morin, Tsujii, syntactic information, generative model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (19) </button>
									<div class="content"> 
										<p>BLEU, SMT,  dynamic cache,  cache-based approach, machine translation, translation model, Empirical, Language Processing, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (20) </button>
									<div class="content"> 
										<p>BLEU,  suggestive evidence, English, word alignment, USA, machine translation, suggestive evidence, word embeddings, Empirical, Language Processing, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (21) </button>
									<div class="content"> 
										<p>BLEU, SMT, IBM,  Gibbs sampler, language pair, word alignment, HMM, Annual Meeting, machine translation, generative model, Bayesian, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (22) </button>
									<div class="content"> 
										<p>SMT, IBM, English,  Viterbi alignment, language pair, machine translation, Models 1-2, translation model, Language Processing, Machine Translation, language model, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (23) </button>
									<div class="content"> 
										<p>English,  selective use,  new notion, Hillary Clinton, Annual Meeting, </p> 
									</div> 
								<button type="button" id="subsubcollapsible" class="collapsible"> Paper (24) </button>
									<div class="content"> 
										<p>BLEU, SMT, English,  model search,  adapted, language pair, LSA, word alignment, Annual Meeting, machine translation, translation model, word perplexity, Republic, speech recognition, language model, Bayesian, </p> 
									</div> 
</div>
            </div>
            <div class="greatcolumn">
              <button type="button" class="collapsible">LDA</button>
              <div class="content" name="lda">
 							
<link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css">


<div id="ldavis_el128671402369675744083111398467"></div>
<script type="text/javascript">

var ldavis_el128671402369675744083111398467_data = {"mdsDat": {"x": [0.12561199951391483, 0.1586360488140142, -0.02016573201341453, 0.11777608203747279, -0.10700092779206682, -0.008054145934520661, -0.06448115128812439, -0.04129798762416257, -0.05021536092793936, -0.072629948704323, -0.038178876080850126], "y": [-0.020084318621066823, -0.13572283131283477, -0.056073690909590156, 0.15837488095910388, -0.09747598612196015, 0.06137140884410894, 0.05800783230905795, 0.015435853353560071, -0.017236999013238087, 0.025492986151764912, 0.007910864361094444], "topics": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "cluster": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "Freq": [34.08569702618946, 18.407324639020565, 10.752031236505545, 9.889505764280623, 6.987132527357126, 6.421984286333945, 5.42820280475887, 2.9194195558467055, 2.69804692156542, 2.39113089449335, 0.019524343648393807]}, "tinfo": {"Term": ["translation", "semantic", "mt", "decoding", "bilingual", "information", "based", "topic", "features", "cws", "approach", "speech", "data", "document", "corpus", "embeddings", "oov", "entity", "role", "word", "similarity", "model", "cache", "ner", "term", "context", "approaches", "roles", "terms", "words", "topic", "document", "lsa", "distribution", "blsa", "test", "adaptation", "cache", "documents", "verbmobil", "dirichlet", "caches", "static", "collection", "relevant", "di", "zn", "crosslingual", "perplexity", "weighting", "bitam", "transfer", "spoken", "directions", "level", "lm", "similar", "store", "kinds", "incorporate", "german", "applied", "process", "data", "inference", "domain", "bayesian", "pairs", "training", "model", "language", "bilingual", "translation", "statistical", "approach", "weights", "sentence", "parallel", "phrase", "word", "target", "alignment", "based", "models", "source", "machine", "smt", "corpus", "information", "paper", "english", "agreement", "germann", "2001", "arabic", "greedy", "decoders", "algorithms", "argmax", "np", "wang", "tillman", "algorithmic", "practical", "stack", "iterations", "probable", "solutions", "fast", "gender", "complexity", "decoding", "polynomial", "original", "forms", "suboptimal", "provably", "alternating", "subproblems", "yields", "bitext", "grammatical", "search", "optimal", "algorithm", "solution", "brown", "decoder", "1993", "time", "ibm", "problem", "known", "models", "problems", "model", "translation", "framework", "computational", "smt", "english", "alignment", "improvements", "language", "sentence", "word", "paper", "based", "machine", "evaluation", "cws", "temporal", "specifications", "entropy", "atemporal", "ph", "clinton", "temporality", "cx", "hillary", "disagreement", "sighan", "guidelines", "georgebush", "dengxiaoping", "usain", "selective", "classifier", "bolt", "necessary", "distinguish", "dictionarybased", "7month", "company", "measured", "fujitsu", "composite", "interpolated", "weak", "fujitsucompany", "cityu", "pku", "msr", "1day", "1997year", "schemes", "eurozone", "entities", "modelling", "mixture", "interpolation", "hmm", "called", "score", "contrast", "refined", "2006", "feature", "smt", "using", "methods", "approaches", "maximum", "models", "translation", "significant", "entity", "kim", "approach", "quality", "chinese", "context", "recognition", "word", "statistical", "paper", "based", "machine", "semantic", "role", "roles", "noun", "tts", "tsuneyama", "transducer", "analyzer", "templates", "constituents", "liu", "srl", "partial", "criteria", "tree", "outputs", "srfs", "encyclopedias", "hybrid", "unknown", "hiragana", "optimum", "jou", "string", "motivates", "skeleton", "did", "deletion", "transducers", "srss", "ssmt", "segment", "semantic", "sampling", "type", "segmentation", "sequence", "phrases", "morphological", "example", "japanese", "words", "mt", "features", "based", "proposed", "source", "phrase", "word", "log", "model", "using", "use", "sentence", "improve", "text", "linear", "target", "resources", "2006", "graph", "tails", "transliteration", "real", "gc", "heads", "ge", "asia", "pronunciation", "life", "gates", "complements", "microsoft", "atr", "nict", "relationship", "people", "occurrences", "extracts", "vi", "similarly", "remaining", "demonstrates", "engines", "reports", "commercial", "requiring", "abstracted", "relational", "categorized", "transliterationand", "holistic", "graphs", "entity", "similarity", "speech", "monolingual", "leveraging", "datasets", "matching", "cooccurrences", "existing", "specifically", "figure", "corpus", "translation", "based", "research", "resources", "bilingual", "work", "approach", "developed", "paper", "scarce", "achieve", "english", "web", "approaches", "chinese", "corpora", "pairs", "language", "problem", "section", "embeddings", "senses", "clusters", "neural", "nist08", "wsi", "disambiguation", "embedding", "bengio", "30", "gigaword", "curriculum", "unlabeled", "permits", "polysemy", "apidianaki", "noisy", "headwords", "frequent", "allow", "sense", "vectors", "vector", "driven", "extraction", "21", "lie", "mandarin", "stanford", "majeure", "lexicons", "wsd", "jin", "stubborn", "initialization", "insensitive", "implies", "washington", "offer", "comparable", "erty", "prop", "2013", "necessity", "quantify", "pruned", "half", "external", "semantics", "languages", "bilingual", "semantic", "word", "translations", "corresponding", "produce", "similarity", "corpus", "equivalence", "bleu", "corpora", "method", "source", "methods", "translation", "features", "chinese", "different", "language", "results", "phrase", "mt", "quality", "morin", "influence", "cosine", "tsujii", "yu", "measures", "medical", "otero", "daille", "projection", "studies", "dejean", "spotting", "versus", "regular", "lo", "surprisingly", "outcome", "benchmark", "attracted", "terminological", "tackling", "window", "measure", "rapp", "science", "plethora", "systematically", "parsers", "varies", "populating", "prochasson", "odds", "popular", "morelo", "objectives", "monolingually", "terms", "variants", "investigated", "context", "parameters", "2005", "approaches", "translating", "association", "wikipedia", "based", "impact", "term", "contexts", "domain", "use", "translation", "similarity", "approach", "different", "source", "used", "target", "words", "lexicon", "results", "information", "section", "corpora", "2009", "aware", "parton", "incorrect", "tightly", "access", "tagging", "ji", "meanings", "filling", "pose", "formal", "typical", "dominant", "phonetically", "understudy", "person", "mckeown", "outnumbered", "enhancing", "equally", "neglected", "mcnamee", "discouraged", "namt", "validation", "extending", "fall", "vastly", "604614", "developers", "dyer", "annotating", "central", "government", "cassidy", "slot", "linking", "processes", "answering", "coordinating", "shrinking", "greater", "latticebased", "propagating", "highquality", "semantically", "tur", "acknowledge", "fraction", "traditional", "rendered", "confidence", "clustering", "hakkani", "conducting", "conduct", "baseline1", "interested", "integrates", "bottleneck", "spend", "names", "overall", "importance", "integrate", "mt", "low", "grammar", "metric", "evaluate", "values", "open", "tokens", "demonstrated", "assign", "translation", "translate", "weights", "including", "jointly", "effectiveness", "compared", "assigning", "lingual", "2009", "words", "different", "quality", "pages", "decoding", "segmentation", "information", "approach", "propose", "word", "new", "training", "framework", "english", "cross", "clir", "mechanism", "retrieval", "oov", "particularly", "selected", "conll2003", "fusion", "emphasizes", "filter", "obvious", "trec", "term", "ranking", "pattern", "crosslanguage", "acquirement", "phoneme", "locate", "englishchinese", "users", "solves", "fields", "fusing", "chien", "hypertext", "mitigate", "querying", "attempts", "influences", "queries", "abundant", "terminologies", "aspects", "svm", "utilizing", "selection", "web", "query", "ability", "strategy", "focuses", "multilingual", "translation", "candidates", "support", "named", "multiple", "sets", "based", "information", "terms", "english", "machine", "paper", "recognition", "evaluation", "model", "new", "performance", "chinese", "results", "data", "features", "structural", "analyses", "ner", "caseframe", "adjacent", "coreference", "uses", "nes", "unit", "chieu", "hwa", "matsumoto", "crl", "specic", "wider", "irex", "classifying", "functional", "fellow", "persons", "recognizes", "token", "relation", "assigned", "consisting", "considers", "conrmed", "immediately", "blank", "mohit", "spaces", "nakano", "grishman", "tight", "promotion", "hirai", "society", "annotated", "integrally", "bunsetsus", "bunsetsu1", "bunsetsu", "malouf", "masayuki", "jsps", "japanese", "svm", "ne", "types", "information", "conducted", "zero", "obtained", "measure", "syntactic", "features", "named", "achieved", "performed", "global", "analysis", "introduced", "relations", "2002", "higher", "focus", "approaches", "context", "approach", "data", "paper", "based", "performance", "entity", "experiments", "words", "recognition", "assigned", "irex", "specic", "annotated", "integrally", "chieu", "fellow", "malouf", "masayuki", "blank", "crl", "consisting", "spaces", "matsumoto", "functional", "immediately", "classifying", "hwa", "mohit", "grishman", "conrmed", "considers", "hirai", "persons", "society", "nakano", "promotion", "wider", "unit", "recognizes", "token", "types", "driven", "bulgaria", "demonstrate", "processing", "sets", "build", "short", "performance", "variety", "lower", "improves", "recognition", "research", "annotations", "goal", "travel", "potential", "presented", "2007", "time", "information", "future", "machine", "intend", "translating", "low", "systems", "previous", "new"], "Freq": [252.0, 59.0, 40.0, 59.0, 64.0, 43.0, 93.0, 59.0, 51.0, 22.0, 70.0, 29.0, 44.0, 44.0, 46.0, 14.0, 12.0, 26.0, 17.0, 85.0, 24.0, 138.0, 29.0, 10.0, 12.0, 25.0, 26.0, 15.0, 18.0, 45.0, 58.44424284026523, 44.325203536698226, 19.85222112823203, 15.145871487402532, 12.322068286484493, 11.380794255495463, 27.38236388276454, 27.382373944748117, 9.498229453984525, 8.556988544283728, 7.615722760391748, 6.6744536551743865, 6.6744536551743865, 6.674453655172288, 11.380799168504598, 5.733184549955234, 5.733184549955234, 5.733184549955091, 5.728130208380479, 4.791915444736646, 4.79191544473653, 4.791915444736499, 4.791915444722395, 4.791895324131696, 17.028410198256232, 23.617285507983993, 11.90457918963948, 3.850646339518609, 3.850646339518609, 3.8506463395181303, 8.556991950203141, 12.32204193255495, 11.380788236021198, 32.57314735859568, 10.439530069988669, 17.0284049600927, 12.322063104574507, 26.347466023907483, 30.711581931567977, 80.01992566716532, 47.983581549401045, 39.02040875824872, 116.05246700644979, 28.030288567811457, 38.75810645837976, 11.813233235468065, 27.38235938577178, 18.54898096703726, 26.441087821713673, 39.1690030683418, 21.65548729723874, 24.46450563003426, 34.30461010871276, 31.009522957075983, 25.166282108651263, 22.43689995027345, 24.3418200309605, 21.538217368118637, 19.81240721035515, 20.942170282933837, 18.10967703354882, 9.915484177217989, 8.127446046898655, 8.127446046898573, 6.339407916579271, 6.339407916578796, 6.339407916578737, 15.27959921872021, 5.445388851420486, 5.445388851420056, 5.445388851419902, 5.445388851419869, 5.445388851419562, 5.445388851419473, 5.445388851419196, 4.551369786260714, 4.551369786260526, 4.551369786259777, 4.551369786259627, 4.551369786259566, 14.38557982461669, 51.04036775808385, 3.6573507211006517, 3.657350721100173, 3.6573507211000114, 3.6573507210999336, 3.6573507210999336, 3.6573507210999336, 3.6573507210999336, 3.6573507210998795, 3.6573507210997183, 3.6573507210997183, 18.855676090889727, 9.915484477680392, 21.537733040913146, 10.809504031190587, 13.491561589848729, 8.127446493386573, 10.809504640200965, 14.385581599330425, 17.961656549920058, 27.795867933670454, 9.021465654844441, 29.5839055857758, 9.915484888705926, 37.63007725093389, 47.46428327421361, 12.597542042506534, 11.703523119917026, 16.173617786780888, 14.385580275021063, 13.491561438274621, 7.233427497398731, 15.279598914434562, 11.703522746869693, 13.491559680613907, 11.703522205056544, 11.703520458623853, 9.915484516398635, 8.127446578920525, 22.367299762703393, 9.982721035028717, 7.505805289494261, 5.854524759754122, 5.028889543958857, 4.203250962113832, 3.377612380268763, 3.377612380268763, 3.377612380268763, 3.377612380268763, 2.5519737984240813, 2.5519737984240813, 2.5519737984240813, 2.5519737984240813, 2.5519737984240813, 2.5519737984236004, 2.5519737984236004, 2.5519737984236004, 2.5519737984236004, 2.5519732570800966, 4.2032509929504895, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 1.7263352165787051, 8.331444123244244, 5.587433818463581, 10.422538639508604, 4.1579057744052355, 5.4227753391875035, 5.793245635945236, 5.854529099733027, 2.5519738112970387, 2.5519708005777666, 8.077103670300788, 8.25414786185054, 14.301034323792216, 12.58826746618657, 7.1856598134951435, 8.331444919984943, 5.133774542777143, 13.406258484993645, 22.822687267135866, 4.804648435885527, 6.6801667590078315, 3.377612533131635, 9.544148098894615, 6.484139656341672, 6.558966668040227, 5.5712447173966435, 5.028890442997912, 7.288874359718272, 6.11182747625766, 5.9616744280675675, 6.242217769202335, 5.599653117078116, 5.246004967346724, 17.30311453310812, 14.84189565555347, 9.919457900443605, 7.458239022889478, 5.817426437852444, 4.9970201453347585, 4.99702014533419, 4.176613852816496, 4.176613852816037, 3.3562075602982078, 3.3562075602982078, 3.3562075602982078, 3.356207560297622, 6.637833205473276, 2.535801267779865, 2.535801267779865, 2.5358012677792567, 2.5358012677792567, 2.5358012677792567, 2.5358012677792567, 2.5358012677792567, 2.5358012677792567, 4.997020412340338, 1.7153949752613726, 1.7153949752613726, 1.7153949752613726, 1.7153949752613726, 1.7153949752613726, 1.7153949752613726, 1.7153949752613726, 4.176614070044122, 37.81327597591694, 9.099052314008608, 5.8174274208059975, 13.201084277301558, 3.35620771745073, 4.997020480029675, 6.637833651014097, 4.9970208757108585, 4.997019902197264, 13.201085314798132, 11.560271911238939, 13.201084886822214, 14.841897709619307, 8.278647072656106, 9.919459345787248, 9.09905259233678, 10.739865136864207, 4.997020477233591, 10.739865061834891, 6.63783346025441, 5.81742719101151, 5.817427070293393, 4.997021220388171, 4.997020900539113, 4.997020751643102, 4.9970207448520325, 4.9970207138652185, 4.9970205249099315, 4.6501400259549595, 3.8866842007977813, 3.8866842007977813, 3.886684200794412, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.359772550483353, 2.3597725504744367, 2.3597725504744367, 5.413596151566814, 4.650140279979332, 9.994331765653905, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 1.596316725326026, 3.1232285367944153, 6.940508487242099, 14.575069086512745, 13.048155102423896, 13.048157081802461, 6.940508382093787, 3.1232284941689534, 3.1232285331962824, 3.1232286817956796, 2.3597726230374514, 5.413597327631031, 2.3597726141973774, 4.650140858586543, 10.757789273183729, 19.155800498104867, 10.757788239593795, 4.650141245555732, 4.650141016041652, 7.703964068015492, 6.177052729775094, 7.703964464506842, 4.650140862172601, 6.940508477807485, 3.12322891668922, 3.1232290168790287, 6.177053344180211, 3.8866850997391795, 4.650140919713323, 4.650140960862814, 4.650140839451636, 4.650140518959034, 5.41359650960771, 4.650140929044732, 3.8866848991558953, 14.29380663893286, 5.30912723690751, 3.811680804067334, 2.314235360585931, 2.314235360585931, 2.314234337432675, 6.057851324522082, 1.5655121556889913, 1.5655121556889913, 1.5655121556889913, 1.5655121556889913, 1.5655121556889913, 1.5655121556889913, 1.565511055262388, 1.565511055262388, 1.565511055262388, 1.565511055262388, 1.565511055262388, 1.565511055262388, 1.565511055262388, 6.648777563516782, 7.555297382886208, 4.560404244336813, 3.7462824323150072, 6.640586024316773, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 3.646406767780045, 1.5655110607311111, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 7.492977426074928, 0.8167889507913276, 0.8167889507913276, 3.811681554517961, 0.8167889507913276, 0.8167889507913276, 0.8167889507913276, 1.5655123025783715, 2.3142343778566286, 2.3142352275486573, 7.184119521592306, 14.31366288675449, 12.030565935140977, 10.769436791588143, 6.478802483835279, 3.543654406746342, 2.314235270884503, 5.309128111063882, 7.190535928208892, 2.3142356537916617, 4.560406092507191, 5.627656978629559, 4.5228682379905445, 7.018235313656531, 4.361831922624532, 13.166162352994812, 6.601837371274971, 5.3091291796790046, 4.536913094370552, 6.425935022202272, 4.88715880726653, 5.3091288719718355, 4.560405119873912, 3.998322337249713, 3.64354706086263, 3.64354706086263, 2.927850316764212, 2.927850316764212, 2.927850316764212, 2.927850316764212, 2.212153572665765, 2.212153572665765, 2.212153572665765, 5.074941123226876, 5.79063784189139, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 1.4964568285672386, 2.9278505007440736, 4.359243856326997, 2.212153661136607, 1.496456854342423, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 4.35924460727425, 0.7807600844683344, 0.7807600844683344, 0.7807600844683344, 7.937729861474499, 2.2121538219863246, 2.2121536599323752, 7.937729631594292, 5.79063859796475, 3.6435479793688472, 5.790639313147144, 2.9278514773486064, 4.359245092492315, 2.2121536775935784, 7.937729780161852, 2.212153961546946, 2.9278503250408896, 2.2121538424911416, 3.643547878663065, 4.3592450507951925, 7.937728316766545, 3.6435471570332845, 5.074941384221006, 3.6435481635635325, 4.359244848239261, 3.643548004472654, 3.643547966386928, 3.6435478956285046, 2.9278512576345235, 2.9278510335811028, 2.9278509537748687, 2.927850943021506, 2.92785086257286, 2.9278507667593194, 3.5019447127950376, 1.2021597700180653, 1.2021597700180653, 2.3520521530041387, 1.7771059624480585, 1.7771059934880211, 1.2006341352663772, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 0.6272135852146631, 4.636845823324776, 2.337577833785468, 1.2021598695239908, 1.739358935889487, 10.88152169125702, 1.1859170414693159, 1.185913107107729, 1.7771061505173198, 1.2021598837001595, 1.7160556457293978, 1.2021599953780069, 1.2021598621956406, 1.1859134199184571, 1.1859131429621843, 9.108718087250454, 1.7160769670960083, 2.0879129895235837, 1.1612273601892338, 1.1612273508360387, 1.1843773814054808, 1.1820258085972384, 1.135721798633206, 1.2021598008034204, 1.7117167363042416, 2.7151799939693793, 2.123055936880148, 2.1826727042265706, 1.6813921581655709, 2.332583526169054, 1.7393587321492454, 1.8931980621749562, 2.0255363251464624, 1.636524323108234, 2.0345990105678347, 1.5969350976927308, 1.6181399453222496, 1.5236089514128874, 1.3332109305509732, 1.2021599720858505, 2.8295372339505422, 1.7179333206120666, 3.3853393113670824, 8.387557386829846, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 1.1621313639427433, 7.831756138494902, 4.4969435004669975, 1.7179333716454799, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 0.6063294072730729, 1.717933320347138, 1.162131425191934, 1.7179340175124498, 3.9411432515930223, 1.1621313769217185, 1.717933668532083, 1.162131558205672, 1.1621315483806811, 1.16213169020207, 13.38978026329501, 1.717933942396002, 1.1621315211374301, 1.7179338903097792, 1.7179338787973943, 1.1621321236135762, 5.0527472584302275, 3.385340970843255, 2.273736073389827, 2.8295386032687624, 2.273736434422246, 2.2737360978128116, 1.7179343445459292, 1.7179341397395267, 2.2737356104313804, 1.7179341692604209, 1.7179343333730208, 1.7179338146190497, 1.717933894411742, 1.7179337468797329, 1.717933507585733, 5.305960891635853, 3.72851305898685, 8.98667286853268, 1.0994333379049543, 1.0994333379049543, 2.6768816260046715, 3.2026980273270755, 1.0994333710564836, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 0.5736173936881521, 3.7285132148189284, 1.6252492823725302, 2.6768819176161465, 1.6244382717850256, 6.380496147266718, 1.0994334820682208, 0.5736173936881521, 2.676882452225043, 1.6252492443836326, 2.6768827749280546, 4.276694134366449, 1.6252498581220944, 1.0994338217873203, 1.0994335391782077, 1.131176882002017, 1.6252498052598578, 1.0994337352197856, 1.0994334779979236, 1.103817967519259, 1.0994336849041435, 1.1038961128746796, 1.648670653816755, 1.62524968409663, 2.1581958598308364, 1.625249713817503, 1.6252497301866, 1.6289622398189954, 1.1050998439054285, 1.1038957837882748, 1.1030854009572408, 1.1025167579812205, 1.0994338874424734, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926087127066, 0.0008189926096164239, 0.0008189926090333476, 0.0008189926089270985, 0.0008189926088982568, 0.0008189926088972047, 0.0008189926088963853, 0.0008189926088939575, 0.0008189926088899026, 0.0008189926088894775, 0.0008189926088891854, 0.0008189926088856687, 0.000818992608885587, 0.0008189926088844446, 0.0008189926088765446, 0.0008189926088739754, 0.000818992608872444, 0.0008189926088722374, 0.0008189926088714995, 0.000818992608871473, 0.0008189926088695265, 0.0008189926088681698, 0.0008189926088676161, 0.0008189926088664603, 0.000818992608865881, 0.0008189926088638383, 0.0008189926088638323, 0.0008189926088631196, 0.0008189926088621713, 0.0008189926088616372, 0.0008189926088615165], "Total": [252.0, 59.0, 40.0, 59.0, 64.0, 43.0, 93.0, 59.0, 51.0, 22.0, 70.0, 29.0, 44.0, 44.0, 46.0, 14.0, 12.0, 26.0, 17.0, 85.0, 24.0, 138.0, 29.0, 10.0, 12.0, 25.0, 26.0, 15.0, 18.0, 45.0, 59.029116376427254, 44.910077438533065, 20.43708591429367, 15.730739882724027, 12.906933072546197, 11.965663362217192, 28.86125373734809, 29.018870633337, 10.083113485771921, 9.141856243654928, 8.20058754645376, 7.2593184412358545, 7.2593184412358545, 7.259318441234819, 12.52146591877027, 6.318049336017064, 6.318049336017064, 6.318049336016997, 6.3174284337012585, 5.376780230798554, 5.376780230798497, 5.376780230798483, 5.376780230791523, 5.37677611492515, 19.25409073984989, 26.88421476595161, 13.57025494751269, 4.43551112558031, 4.43551112558031, 4.435511125580074, 10.035875721067734, 14.62135337541954, 13.623137492630093, 44.50785337941806, 12.665207446364969, 21.95521657613634, 15.36815131304232, 36.370480789623414, 44.3857610873917, 138.27354062123896, 83.11692561600219, 64.69852265132548, 252.50245154975912, 43.78179363368353, 70.46260644802688, 15.254149276304174, 47.319521585295576, 28.7258376824118, 47.54962963559069, 85.158347557731, 36.869996313103925, 44.74641637345686, 93.46604192895089, 79.27897282037924, 54.74677084048388, 49.19942873542392, 58.52662937509333, 46.30099675878382, 43.57831961296702, 56.62780708829778, 50.50329971758129, 10.504644421465336, 8.716606291145986, 8.716606291145945, 6.928568160826612, 6.928568160826388, 6.928568160826359, 16.810027883250957, 6.034549095667351, 6.034549095667147, 6.034549095667076, 6.034549095667058, 6.034549095666914, 6.034549095666874, 6.034549095666742, 5.140530030507624, 5.140530030507537, 5.140530030507183, 5.140530030507112, 5.140530030507083, 16.37009241060158, 59.019003536722074, 4.246510965347761, 4.246510965347534, 4.246510965347458, 4.246510965347422, 4.246510965347422, 4.246510965347422, 4.246510965347422, 4.246510965347395, 4.246510965347321, 4.246510965347321, 22.85428484509263, 11.886084986040382, 26.883711284298546, 13.192654580720996, 16.904526587724824, 10.599144477973837, 14.733393834727053, 20.84077518605618, 30.13071944855547, 51.969459027140715, 13.37569955613965, 79.27897282037924, 16.936714515599945, 138.27354062123896, 252.50245154975912, 26.190469304967714, 28.182807469447326, 58.52662937509333, 50.50329971758129, 44.74641637345686, 11.46680081544277, 83.11692561600219, 47.319521585295576, 85.158347557731, 56.62780708829778, 93.46604192895089, 49.19942873542392, 21.024979636759422, 22.96267641452379, 10.57809768684942, 8.101181941314747, 6.44990524903253, 5.624266195779665, 4.798627613934683, 3.972989032089683, 3.972989032089683, 3.972989032089683, 3.972989032089683, 3.1473504502448457, 3.1473504502448457, 3.1473504502448457, 3.1473504502448457, 3.1473504502448457, 3.1473504502446388, 3.1473504502446388, 3.1473504502446388, 3.1473504502446388, 3.1473505260594923, 5.514324362138185, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 2.3217118683997024, 12.298806006805057, 8.369849494023272, 19.929179291395865, 6.687516409112913, 9.33417898736178, 10.619188007637014, 10.762564964439228, 3.9108062763699163, 3.910806690422493, 19.33994732571565, 19.99316970164544, 58.52662937509333, 47.696919381477485, 19.179516257871295, 26.439661765097895, 11.115441584713325, 79.27897282037924, 252.50245154975912, 11.270926874192517, 26.222293819967703, 5.855527221110981, 70.46260644802688, 28.440250445550948, 31.546836853533573, 25.70494371960513, 17.92402726487842, 85.158347557731, 43.78179363368353, 56.62780708829778, 93.46604192895089, 49.19942873542392, 59.081585867415825, 17.898966847593595, 15.437747970038952, 10.51531021492942, 8.054091337375, 6.413278752338309, 5.592872459820322, 5.592872459820077, 4.772466167302085, 4.7724661673018876, 3.9520598747838362, 3.9520598747838362, 3.9520598747838362, 3.9520598747835844, 8.1749540800739, 3.1316535822655625, 3.1316535822655625, 3.1316535822653, 3.1316535822653, 3.1316535822653, 3.1316535822653, 3.1316535822653, 3.1316535822653, 6.486891501016335, 2.311247289747217, 2.311247289747217, 2.311247289747217, 2.311247289747217, 2.311247289747217, 2.311247289747217, 2.311247289747217, 5.666485212964238, 59.081585867415825, 13.282166984153948, 8.703338319911543, 21.556555198743595, 4.846078925836864, 8.001226566178522, 12.89716203604279, 9.147818200990224, 9.273583982028825, 45.6168670148282, 40.38530511565824, 51.87324401522803, 93.46604192895089, 33.74174335194831, 54.74677084048388, 47.54962963559069, 85.158347557731, 12.4463080982891, 138.27354062123896, 47.696919381477485, 38.81338805225659, 47.319521585295576, 16.262823405287737, 20.405893335835955, 18.09277303530717, 36.869996313103925, 16.732626771664957, 19.33994732571565, 5.251169655654589, 4.4877138304974284, 4.4877138304974284, 4.487713830496111, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801830735, 2.9608021801795883, 2.9608021801795883, 6.840264038181113, 6.076808216805659, 13.193595209555495, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 2.197346355025846, 4.549896574055547, 10.912471978023738, 26.222293819967703, 24.12000696173497, 29.424046215808566, 14.090821044643974, 5.37553515937306, 5.375535400855571, 5.381223782432791, 3.7095253864777553, 11.60448217156624, 3.781208467946151, 11.283031023595736, 46.30099675878382, 252.50245154975912, 93.46604192895089, 16.144127006170805, 16.732626771664957, 64.69852265132548, 38.152955138286714, 70.46260644802688, 18.211398965877816, 56.62780708829778, 6.432434689005571, 6.4535650948889565, 50.50329971758129, 12.788168351423677, 26.439661765097895, 31.546836853533573, 31.98574996020117, 36.370480789623414, 83.11692561600219, 51.969459027140715, 29.778245262215563, 14.896175597747806, 5.911497381563145, 4.414050977696633, 2.916604319401121, 2.916604319401121, 2.916604582520811, 8.311498208515397, 2.167881114504348, 2.167881114504348, 2.167881114504348, 2.167881114504348, 2.167881114504348, 2.167881114504348, 2.167881397496075, 2.167881397496075, 2.167881397496075, 2.167881397496075, 2.167881397496075, 2.167881397496075, 2.167881397496075, 9.332059813124912, 11.130395741488565, 7.51589081782593, 6.265987369981943, 11.229527542319062, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 6.339087687378343, 2.7236833555738515, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 14.42581788365435, 1.4191579096072775, 1.4191579096072775, 7.215220341146807, 1.4191579096072775, 1.4191579096072775, 1.4191579096072775, 2.9313369367650717, 4.557417163684877, 4.631029728673737, 23.23305698154986, 64.69852265132548, 59.081585867415825, 85.158347557731, 33.50956190906127, 10.479742022640439, 4.636262013844111, 24.12000696173497, 46.30099675878382, 4.799142454423777, 19.422175294982097, 31.98574996020117, 20.402339174075955, 54.74677084048388, 19.179516257871295, 252.50245154975912, 51.87324401522803, 31.546836853533573, 25.40269983868208, 83.11692561600219, 33.56196398154232, 47.54962963559069, 40.38530511565824, 28.440250445550948, 4.248918425203833, 4.248918425203833, 3.533221681105439, 3.533221681105439, 3.533221681105439, 3.533221681105439, 2.8175249370070325, 2.8175249370070325, 2.8175249370070325, 6.62158083764498, 7.985102991905067, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 2.1018281929085942, 4.358860234684514, 6.54206301557877, 3.5662484917081367, 2.62764414396293, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 8.211184599989924, 1.3861314488100032, 1.3861314488100032, 1.3861314488100032, 18.472394952912857, 4.460267511257136, 4.468802087288645, 25.70494371960513, 19.337367414121616, 10.624470047369135, 26.439661765097895, 7.711394104312684, 21.92513670003689, 5.07842744881633, 93.46604192895089, 5.363056326986152, 12.140087332527424, 5.878143678632143, 21.95521657613634, 38.81338805225659, 252.50245154975912, 24.12000696173497, 70.46260644802688, 25.40269983868208, 54.74677084048388, 31.173968111055675, 36.869996313103925, 45.6168670148282, 14.69132105817574, 33.56196398154232, 43.57831961296702, 29.778245262215563, 31.98574996020117, 13.701137210361665, 4.945750217516518, 1.8203270948164756, 1.8203270948164756, 3.790625543067672, 3.158729002508358, 3.215679450768571, 2.346012670881135, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 1.2453810223748116, 12.276441765870386, 5.640368251830987, 2.7143461047496653, 4.30186162979244, 40.38530511565824, 2.771916558760376, 2.7719476503496314, 5.82605348722104, 3.389456550929753, 5.973675018869231, 3.5399849373569277, 3.6083651739765124, 3.5354033215994933, 3.5923539283617427, 252.50245154975912, 9.549727747221256, 15.254149276304174, 4.549351409194724, 4.549351418721793, 4.8245476983948, 5.010127421942014, 4.686464910210679, 5.593408613288514, 13.701137210361665, 45.6168670148282, 25.40269983868208, 28.440250445550948, 15.715502163412042, 59.019003536722074, 21.556555198743595, 43.57831961296702, 70.46260644802688, 20.965574488701098, 85.158347557731, 22.419591205456506, 44.3857610873917, 26.190469304967714, 50.50329971758129, 6.279773682502085, 3.449444488058315, 2.3378405747199205, 4.72094315408794, 12.310018097539194, 1.782038618050707, 1.782038618050707, 1.782038618050707, 1.782038618050707, 1.782038618050707, 1.782038618050707, 1.782038618050707, 1.782038618050707, 12.140087332527424, 7.368649966151091, 3.053537304135863, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 1.2262366613814295, 3.9152884073543626, 2.5307618016856037, 4.057498548485354, 12.788168351423677, 3.308950263514859, 6.080165884205011, 3.439004327052354, 3.439004334890212, 3.6645648282721965, 252.50245154975912, 7.371429474429637, 3.896948893236843, 8.902602041304505, 9.165316257838247, 4.470129218272593, 93.46604192895089, 43.57831961296702, 18.472394952912857, 50.50329971758129, 49.19942873542392, 56.62780708829778, 17.92402726487842, 21.024979636759422, 138.27354062123896, 22.419591205456506, 27.889465512165742, 31.546836853533573, 33.56196398154232, 44.50785337941806, 51.87324401522803, 5.928594146875728, 4.35114631422675, 10.7209096487771, 1.7220665931450927, 1.7220665931450927, 4.240783171148273, 6.554636910626167, 2.277868547923304, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 9.273583982028825, 3.9152884073543626, 6.970090200973863, 5.608444946739448, 43.57831961296702, 3.3681113857823455, 1.1962506489287072, 14.674508332504802, 6.54206301557877, 17.499726340196904, 51.87324401522803, 8.902602041304505, 4.377760683259902, 4.545873749762896, 5.341155532287345, 12.218363167758131, 5.4392283124771135, 6.055400861631818, 6.1817787479579165, 7.619876934442355, 7.885405032411523, 26.439661765097895, 25.70494371960513, 70.46260644802688, 44.50785337941806, 56.62780708829778, 93.46604192895089, 27.889465512165742, 26.222293819967703, 12.59808819508492, 45.6168670148282, 17.92402726487842, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 1.1962506489287072, 5.608444946739448, 6.265987369981943, 3.3625237538524697, 4.2201184139665555, 10.098895249487079, 4.470129218272593, 6.792520821245149, 4.589496325325209, 27.889465512165742, 4.183523625686559, 4.094817223094224, 5.292053609604151, 17.92402726487842, 16.144127006170805, 4.340936653683364, 5.9514491011107085, 2.3751596051186787, 4.178979210325918, 8.355821945156661, 18.828597481547963, 20.84077518605618, 43.57831961296702, 10.960914797960806, 49.19942873542392, 2.1998915303536744, 7.711394104312684, 2.771916558760376, 29.643234815646252, 8.332170663077964, 22.419591205456506], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic5", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic6", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic7", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic8", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic9", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic10", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11", "Topic11"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.2153, -4.4919, -5.2951, -5.5657, -5.772, -5.8515, -4.9735, -4.9735, -6.0323, -6.1367, -6.2532, -6.3851, -6.3851, -6.3851, -5.8515, -6.5371, -6.5371, -6.5371, -6.538, -6.7165, -6.7165, -6.7165, -6.7165, -6.7165, -5.4485, -5.1214, -5.8065, -6.9352, -6.9352, -6.9352, -6.1367, -5.772, -5.8515, -4.7999, -5.9378, -5.4485, -5.772, -5.012, -4.8588, -3.9011, -4.4126, -4.6193, -3.5294, -4.9501, -4.6261, -5.8142, -4.9735, -5.363, -5.0085, -4.6155, -5.2082, -5.0862, -4.7481, -4.8491, -5.0579, -5.1727, -5.0912, -5.2136, -5.2971, -5.2417, -5.387, -5.3732, -5.572, -5.572, -5.8205, -5.8205, -5.8205, -4.9408, -5.9725, -5.9725, -5.9725, -5.9725, -5.9725, -5.9725, -5.9725, -6.1519, -6.1519, -6.1519, -6.1519, -6.1519, -5.0011, -3.7347, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -6.3705, -4.7305, -5.3732, -4.5975, -5.2869, -5.0652, -5.572, -5.2869, -5.0011, -4.779, -4.3424, -5.4677, -4.2801, -5.3732, -4.0395, -3.8073, -5.1338, -5.2074, -4.8839, -5.0011, -5.0652, -5.6886, -4.9408, -5.2074, -5.0652, -5.2074, -5.2074, -5.3732, -5.572, -4.022, -4.8288, -5.114, -5.3624, -5.5144, -5.6938, -5.9125, -5.9125, -5.9125, -5.9125, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -6.1928, -5.6938, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -6.5836, -5.0096, -5.4091, -4.7857, -5.7046, -5.439, -5.3729, -5.3624, -6.1928, -6.1928, -5.0406, -5.0189, -4.4693, -4.5969, -5.1575, -5.0096, -5.4938, -4.5339, -4.0019, -5.56, -5.2305, -5.9125, -4.8737, -5.2603, -5.2488, -5.412, -5.5144, -5.1433, -5.3194, -5.3443, -5.2983, -5.4069, -5.4722, -4.1951, -4.3486, -4.7515, -5.0367, -5.2852, -5.4372, -5.4372, -5.6165, -5.6165, -5.8352, -5.8352, -5.8352, -5.8352, -5.1532, -6.1155, -6.1155, -6.1155, -6.1155, -6.1155, -6.1155, -6.1155, -6.1155, -5.4372, -6.5064, -6.5064, -6.5064, -6.5064, -6.5064, -6.5064, -6.5064, -5.6165, -3.4134, -4.8378, -5.2852, -4.4657, -5.8352, -5.4372, -5.1532, -5.4372, -5.4372, -4.4657, -4.5984, -4.4657, -4.3486, -4.9323, -4.7515, -4.8378, -4.672, -5.4372, -4.672, -5.1532, -5.2852, -5.2852, -5.4372, -5.4372, -5.4372, -5.4372, -5.4372, -5.4372, -5.1617, -5.3411, -5.3411, -5.3411, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.84, -5.0097, -5.1617, -4.3966, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -6.2309, -5.5597, -4.7612, -4.0193, -4.13, -4.13, -4.7612, -5.5597, -5.5597, -5.5597, -5.84, -5.0097, -5.84, -5.1617, -4.323, -3.746, -4.323, -5.1617, -5.1617, -4.6569, -4.8778, -4.6569, -5.1617, -4.7612, -5.5597, -5.5597, -4.8778, -5.3411, -5.1617, -5.1617, -5.1617, -5.1617, -5.0097, -5.1617, -5.3411, -3.9544, -4.9448, -5.2762, -5.7752, -5.7752, -5.7752, -4.8129, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -6.1661, -4.7198, -4.592, -5.0969, -5.2935, -4.7211, -6.8166, -6.8166, -6.8166, -6.8166, -6.8166, -5.3205, -6.1661, -6.8166, -6.8166, -6.8166, -6.8166, -6.8166, -6.8166, -6.8166, -4.6003, -6.8166, -6.8166, -5.2762, -6.8166, -6.8166, -6.8166, -6.1661, -5.7752, -5.7752, -4.6424, -3.9531, -4.1268, -4.2376, -4.7457, -5.3491, -5.7752, -4.9448, -4.6415, -5.7752, -5.0969, -4.8866, -5.1051, -4.6658, -5.1414, -4.0366, -4.7269, -4.9448, -5.102, -4.7539, -5.0277, -4.9448, -5.0969, -5.2284, -5.1532, -5.1532, -5.3719, -5.3719, -5.3719, -5.3719, -5.6522, -5.6522, -5.6522, -4.8218, -4.6899, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -6.043, -5.3719, -4.9738, -5.6522, -6.043, -6.6936, -6.6936, -6.6936, -6.6936, -6.6936, -6.6936, -6.6936, -4.9738, -6.6936, -6.6936, -6.6936, -4.3745, -5.6522, -5.6522, -4.3745, -4.6899, -5.1532, -4.6899, -5.3719, -4.9738, -5.6522, -4.3745, -5.6522, -5.3719, -5.6522, -5.1532, -4.9738, -4.3745, -5.1532, -4.8218, -5.1532, -4.9738, -5.1532, -5.1532, -5.1532, -5.3719, -5.3719, -5.3719, -5.3719, -5.3719, -5.3719, -4.5726, -5.6418, -5.6418, -4.9706, -5.2509, -5.2509, -5.6431, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -6.2924, -4.2919, -4.9768, -5.6418, -5.2724, -3.4389, -5.6554, -5.6554, -5.2509, -5.6418, -5.2859, -5.6418, -5.6418, -5.6554, -5.6554, -3.6167, -5.2859, -5.0898, -5.6764, -5.6764, -5.6567, -5.6587, -5.6987, -5.6418, -5.2884, -4.8271, -5.0731, -5.0454, -5.3063, -4.9789, -5.2724, -5.1877, -5.1201, -5.3333, -5.1156, -5.3578, -5.3446, -5.4048, -5.5383, -5.6418, -4.707, -5.2059, -4.5276, -3.6203, -5.5968, -5.5968, -5.5968, -5.5968, -5.5968, -5.5968, -5.5968, -5.5968, -3.6889, -4.2437, -5.2059, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -6.2474, -5.2059, -5.5968, -5.2059, -4.3756, -5.5968, -5.2059, -5.5968, -5.5968, -5.5968, -3.1526, -5.2059, -5.5968, -5.2059, -5.2059, -5.5968, -4.1271, -4.5276, -4.9256, -4.707, -4.9256, -4.9256, -5.2059, -5.2059, -4.9256, -5.2059, -5.2059, -5.2059, -5.2059, -5.2059, -5.2059, -3.9575, -4.3103, -3.4306, -5.5315, -5.5315, -4.6417, -4.4623, -5.5315, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -6.1821, -4.3103, -5.1406, -4.6417, -5.1411, -3.7731, -5.5315, -6.1821, -4.6417, -5.1406, -4.6417, -4.1731, -5.1406, -5.5315, -5.5315, -5.503, -5.1406, -5.5315, -5.5315, -5.5275, -5.5315, -5.5275, -5.1263, -5.1406, -4.857, -5.1406, -5.1406, -5.1384, -5.5264, -5.5275, -5.5282, -5.5287, -5.5315, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259, -7.9259], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0663, 1.0632, 1.0473, 1.0384, 1.0299, 1.0262, 1.0237, 1.0182, 1.0165, 1.0102, 1.0023, 0.9923, 0.9923, 0.9923, 0.9808, 0.9792, 0.9792, 0.9792, 0.9784, 0.9611, 0.9611, 0.9611, 0.9611, 0.9611, 0.9535, 0.9467, 0.9453, 0.9349, 0.9349, 0.9349, 0.9169, 0.9052, 0.8964, 0.7641, 0.883, 0.8222, 0.8554, 0.7539, 0.708, 0.5293, 0.5269, 0.5706, 0.2989, 0.6304, 0.4786, 0.8207, 0.5293, 0.6389, 0.4894, 0.2997, 0.5442, 0.4725, 0.074, 0.1376, 0.2991, 0.2911, 0.199, 0.311, 0.288, 0.0816, 0.0507, 1.6347, 1.6224, 1.6224, 1.6036, 1.6036, 1.6036, 1.597, 1.5897, 1.5897, 1.5897, 1.5897, 1.5897, 1.5897, 1.5897, 1.5707, 1.5707, 1.5707, 1.5707, 1.5707, 1.5632, 1.5472, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5431, 1.5001, 1.5112, 1.4707, 1.4932, 1.4669, 1.4269, 1.3827, 1.3217, 1.1751, 1.0667, 1.2986, 0.7067, 1.157, 0.391, 0.021, 0.9605, 0.8136, 0.4063, 0.4366, 0.4935, 1.2317, -0.0013, 0.2954, -0.15, 0.1158, -0.3853, 0.0906, 0.742, 2.2038, 2.1721, 2.1537, 2.1332, 2.1182, 2.0976, 2.0677, 2.0677, 2.0677, 2.0677, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 2.0204, 1.9586, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.9338, 1.8406, 1.826, 1.5819, 1.7548, 1.687, 1.6241, 1.6212, 1.8032, 1.8032, 1.3569, 1.3454, 0.8209, 0.898, 1.2483, 1.0752, 1.4576, 0.4528, -0.1736, 1.3774, 0.8626, 1.6799, 0.2309, 0.7516, 0.6594, 0.701, 0.9591, -0.2281, 0.2611, -0.0211, -0.4762, 0.0569, -0.1914, 2.2798, 2.2743, 2.2554, 2.2368, 2.2162, 2.201, 2.201, 2.1803, 2.1803, 2.1503, 2.1503, 2.1503, 2.1503, 2.1054, 2.1026, 2.1026, 2.1026, 2.1026, 2.1026, 2.1026, 2.1026, 2.1026, 2.0528, 2.0156, 2.0156, 2.0156, 2.0156, 2.0156, 2.0156, 2.0156, 2.0086, 1.8674, 1.9354, 1.9108, 1.8233, 1.9463, 1.8429, 1.6495, 1.709, 1.6954, 1.0737, 1.0628, 0.9452, 0.4736, 0.9086, 0.6055, 0.6601, 0.2431, 1.4011, -0.2416, 0.3416, 0.4158, 0.2176, 1.1337, 0.9067, 1.027, 0.3151, 1.1052, 0.9604, 2.5395, 2.5173, 2.5173, 2.5173, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4342, 2.4272, 2.3935, 2.3834, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.3415, 2.2849, 2.2086, 2.0738, 2.0467, 1.8479, 1.953, 2.1181, 2.1181, 2.1171, 2.2088, 1.8986, 2.1896, 1.7747, 1.2016, 0.0823, 0.4991, 1.4164, 1.3806, 0.5331, 0.8403, 0.4478, 1.2959, 0.562, 1.9386, 1.9353, 0.5599, 1.4701, 0.9231, 0.7465, 0.7327, 0.6042, -0.0702, 0.2473, 0.6249, 2.7042, 2.638, 2.5987, 2.5141, 2.5141, 2.5141, 2.4292, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4199, 2.4064, 2.358, 2.2458, 2.2311, 2.2201, 2.193, 2.193, 2.193, 2.193, 2.193, 2.1925, 2.1917, 2.193, 2.193, 2.193, 2.193, 2.193, 2.193, 2.193, 2.0904, 2.193, 2.193, 2.1073, 2.193, 2.193, 2.193, 2.1182, 2.0678, 2.0517, 1.5717, 1.2369, 1.154, 0.6776, 1.1021, 1.6612, 2.0506, 1.2318, 0.883, 2.0161, 1.2964, 1.0078, 1.2389, 0.6912, 1.2645, -0.2083, 0.684, 0.9634, 1.0228, 0.1855, 0.8187, 0.5531, 0.5644, 0.7835, 2.7599, 2.7599, 2.7256, 2.7256, 2.7256, 2.7256, 2.6717, 2.6717, 2.6717, 2.6475, 2.5922, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5739, 2.5156, 2.5076, 2.436, 2.3506, 2.3396, 2.3396, 2.3396, 2.3396, 2.3396, 2.3396, 2.3396, 2.2804, 2.3396, 2.3396, 2.3396, 2.0689, 2.2123, 2.2104, 1.7385, 1.7078, 1.8434, 1.3949, 1.9451, 1.2982, 2.0825, 0.4476, 2.028, 1.4913, 1.9363, 1.1175, 0.7271, -0.5462, 1.0235, 0.2828, 0.9717, 0.3831, 0.7669, 0.5991, 0.3862, 1.3006, 0.4744, 0.2133, 0.5941, 0.5225, 1.3704, 3.1886, 3.1189, 3.1189, 3.0565, 2.9586, 2.9407, 2.8639, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.8479, 2.5601, 2.653, 2.7194, 2.6283, 2.2224, 2.6848, 2.6847, 2.3464, 2.4972, 2.2865, 2.4538, 2.4347, 2.4415, 2.4255, 0.2116, 1.8173, 1.5451, 2.1683, 2.1683, 2.1293, 2.0896, 2.1164, 1.9963, 1.4538, 0.7124, 1.0518, 0.9665, 1.2988, 0.3029, 1.0166, 0.3975, -0.0155, 0.9835, -0.2004, 0.8919, 0.2221, 0.6895, -0.1007, 1.8806, 3.4145, 3.3045, 3.2801, 3.229, 3.1851, 3.1851, 3.1851, 3.1851, 3.1851, 3.1851, 3.1851, 3.1851, 3.1743, 3.1188, 3.0375, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.9084, 2.7889, 2.8344, 2.7532, 2.4356, 2.5663, 2.3487, 2.5277, 2.5277, 2.4642, 0.6757, 2.1562, 2.4027, 1.9674, 1.9383, 2.2655, 0.695, 1.0575, 1.5178, 0.7307, 0.5382, 0.3976, 1.2676, 1.1081, -0.4952, 1.0438, 0.8255, 0.7023, 0.6404, 0.3581, 0.205, 3.6224, 3.579, 3.557, 3.2847, 3.2847, 3.2733, 3.0172, 3.005, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.9984, 2.8222, 2.8542, 2.7764, 2.4943, 1.8121, 2.6138, 2.9984, 2.0319, 2.3408, 1.8559, 1.2378, 2.0327, 2.3517, 2.314, 2.1812, 1.7161, 2.1346, 2.0272, 2.0106, 1.7974, 1.7672, 0.9585, 0.9724, 0.2476, 0.4234, 0.1826, -0.3163, 0.5051, 0.5656, 1.298, 0.0107, 0.9421, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, 1.2546, -0.2904, -0.4013, 0.2211, -0.006, -0.8786, -0.0636, -0.482, -0.0899, -1.8944, 0.0027, 0.0241, -0.2324, -1.4523, -1.3477, -0.0343, -0.3498, 0.5688, 0.0038, -0.6891, -1.5015, -1.6031, -2.3407, -0.9605, -2.4621, 0.6454, -0.6089, 0.4143, -1.9554, -0.6863, -1.6761]}, "token.table": {"Topic": [1, 2, 5, 7, 3, 3, 2, 2, 3, 7, 8, 9, 10, 1, 3, 4, 7, 10, 1, 3, 4, 6, 1, 2, 4, 5, 7, 10, 1, 4, 5, 6, 7, 8, 3, 6, 8, 6, 6, 8, 3, 1, 4, 8, 9, 5, 9, 5, 8, 1, 2, 5, 1, 2, 4, 10, 8, 9, 1, 2, 10, 2, 2, 3, 4, 5, 2, 1, 2, 1, 2, 3, 4, 5, 9, 6, 2, 10, 1, 2, 4, 10, 4, 10, 8, 1, 2, 8, 6, 1, 2, 4, 1, 3, 4, 5, 7, 8, 10, 1, 3, 5, 7, 10, 2, 2, 5, 9, 1, 4, 8, 10, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 3, 5, 9, 7, 3, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 1, 4, 7, 6, 1, 5, 6, 7, 1, 2, 10, 1, 2, 3, 4, 6, 1, 3, 8, 1, 2, 1, 2, 4, 5, 1, 3, 10, 10, 10, 1, 10, 1, 1, 2, 3, 7, 1, 2, 3, 7, 9, 10, 8, 5, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 3, 10, 3, 9, 8, 6, 1, 5, 3, 1, 3, 5, 6, 7, 1, 6, 7, 8, 5, 2, 4, 8, 3, 1, 2, 3, 4, 5, 6, 7, 8, 8, 3, 4, 10, 8, 8, 9, 10, 10, 10, 4, 1, 3, 4, 6, 7, 10, 1, 6, 7, 8, 3, 5, 5, 6, 8, 1, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 5, 6, 7, 9, 10, 1, 3, 4, 5, 6, 7, 4, 10, 2, 5, 6, 8, 9, 9, 1, 6, 3, 3, 7, 1, 2, 3, 9, 10, 3, 5, 1, 2, 2, 1, 2, 3, 4, 8, 7, 4, 1, 2, 4, 1, 5, 8, 5, 3, 1, 2, 3, 5, 8, 8, 1, 3, 4, 1, 2, 3, 4, 6, 7, 8, 9, 1, 1, 3, 3, 6, 8, 3, 7, 1, 1, 1, 1, 5, 7, 8, 1, 2, 6, 8, 1, 5, 8, 10, 6, 6, 9, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 3, 5, 9, 10, 1, 3, 5, 6, 8, 9, 10, 3, 8, 1, 6, 6, 3, 4, 6, 8, 1, 2, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 1, 2, 4, 5, 10, 1, 2, 4, 8, 9, 10, 8, 4, 6, 1, 6, 7, 8, 5, 8, 2, 1, 2, 3, 4, 6, 9, 10, 1, 2, 3, 4, 6, 9, 10, 10, 9, 1, 2, 3, 5, 8, 9, 1, 2, 5, 8, 9, 10, 1, 7, 9, 8, 2, 8, 1, 2, 5, 8, 6, 3, 3, 10, 9, 9, 1, 2, 4, 7, 9, 5, 5, 5, 2, 3, 1, 2, 2, 6, 1, 4, 10, 1, 3, 5, 7, 9, 8, 1, 8, 2, 5, 2, 3, 5, 8, 2, 10, 3, 8, 5, 6, 5, 6, 1, 2, 3, 6, 10, 8, 3, 4, 10, 1, 3, 3, 5, 10, 4, 9, 1, 2, 3, 10, 1, 3, 6, 7, 6, 2, 8, 1, 2, 4, 6, 9, 1, 2, 4, 1, 2, 5, 9, 10, 1, 4, 8, 1, 8, 1, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 6, 10, 1, 8, 8, 1, 8, 8, 3, 1, 3, 1, 2, 10, 3, 7, 10, 2, 4, 10, 8, 10, 6, 1, 4, 8, 4, 10, 1, 3, 1, 1, 2, 1, 2, 3, 4, 5, 6, 9, 1, 2, 4, 5, 6, 7, 8, 1, 4, 3, 5, 1, 2, 3, 7, 1, 6, 6, 5, 1, 2, 3, 4, 5, 6, 8, 8, 4, 1, 2, 7, 9, 1, 4, 7, 1, 8, 1, 2, 3, 5, 1, 1, 2, 3, 4, 5, 6, 9, 10, 6, 10, 6, 10, 1, 5, 7, 10, 1, 2, 3, 8, 8, 8, 7, 10, 3, 7, 9, 7, 1, 3, 4, 5, 6, 1, 3, 4, 5, 6, 7, 8, 2, 6, 8, 5, 9, 1, 3, 1, 2, 3, 4, 6, 7, 9, 1, 3, 1, 2, 3, 4, 6, 10, 1, 5, 6, 7, 7, 7, 7, 1, 2, 4, 10, 4, 3, 1, 2, 4, 6, 8, 1, 9, 1, 2, 3, 8, 9, 10, 1, 3, 4, 6, 9, 10, 1, 3, 5, 8, 10, 8, 1, 2, 10, 3, 6, 8, 9, 10, 9, 10, 6, 1, 2, 3, 4, 5, 6, 8, 9, 5, 6, 6, 4, 2, 7, 1, 2, 5, 7, 9, 10, 9, 1, 5, 7, 7, 6, 3, 9, 2, 3, 8, 2, 3, 9, 4, 2, 7, 7, 8, 4, 1, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 5, 6, 7, 8, 1, 2, 7, 7, 4, 9, 8, 7, 9, 3, 5, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 10, 6, 1, 8, 10, 3, 9, 8, 1, 2, 4, 6, 1, 4, 10, 3, 7, 2, 6, 2, 3, 5, 7, 7, 8, 1, 2, 3, 4, 2, 1, 2, 3, 5, 10, 1, 2, 3, 7, 10, 2, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 1, 2, 5, 8, 1, 2, 4, 5, 6, 8, 7, 2, 3, 6, 1, 7, 10, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 9, 10, 2, 6, 1, 2, 3, 4, 5, 6, 8, 9, 6, 9, 5, 9, 9, 4, 7, 9, 6, 7, 5, 1, 2, 3, 5, 6, 9, 10, 10, 3, 5, 7, 10, 5, 2, 3, 10, 3, 5, 1, 9, 5, 8, 5, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 7, 9, 4, 4, 1, 4, 5, 1, 3, 5, 3, 7, 10, 1, 2, 3, 4, 7, 1, 2, 5, 1, 2, 4, 5, 6, 7, 2, 4, 1, 3, 4, 8, 9, 2, 3, 9, 3, 1, 3, 4, 6, 8, 2, 4, 6, 1, 6, 6, 1, 2, 4, 7, 9, 2, 4, 1, 2, 3, 9, 1, 2, 8, 9, 8, 3, 1, 3, 4, 7, 9, 1, 6, 3, 5, 6, 7, 5, 4, 8, 1, 2, 3, 4, 10, 1, 2, 3, 2, 9, 1, 2, 3, 4, 5, 6, 7, 10, 10, 4, 5, 3, 1, 2, 3, 5, 8, 1, 7, 4, 4, 4, 4, 2, 6, 1, 1, 2, 3, 4, 6, 9, 1, 1, 7, 9, 2, 4, 10, 6, 3, 5, 7, 2, 2, 3, 5, 9, 10, 7, 9, 10, 1, 2, 3, 4, 7, 10, 7, 1, 2, 4, 5, 6, 10, 7, 4, 8, 5, 1, 2, 3, 4, 5, 7, 4, 3, 3, 3, 7, 9, 7, 9, 1, 4, 7, 9, 1, 1, 2, 4, 5, 6, 9, 10, 4, 8, 2, 1, 2, 3, 4, 5, 8, 10, 2, 8, 1, 8, 1, 2, 3, 6, 8, 4, 4, 1, 1, 2, 7, 8, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 5, 5, 1, 5, 9, 1, 4, 7, 4, 4, 8, 3, 4, 6, 7, 2, 3, 4, 10, 8, 8, 10, 4, 6, 3, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 6, 7, 10, 9, 1, 2, 10, 1, 2, 3, 4, 5, 6, 7, 10, 6, 9, 8, 1, 7, 8, 2, 6, 7, 7, 1, 2, 5, 8, 6, 7, 9, 10, 3, 6, 7, 1, 7, 5, 2, 6, 3, 1, 3, 5, 8, 9, 10, 1, 1, 4, 8, 10, 5, 6, 7, 3, 7, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 6, 9, 6, 2, 7, 10, 1], "Freq": [0.13574604890327033, 0.7466032689679868, 0.06787302445163516, 0.06787302445163516, 0.8614333359886512, 0.8614333359886512, 0.9177883837802963, 0.32353147557419104, 0.16176573778709552, 0.16176573778709552, 0.16176573778709552, 0.16176573778709552, 0.16176573778709552, 0.2823670250492042, 0.18824468336613612, 0.09412234168306806, 0.37648936673227223, 0.09412234168306806, 0.2585322449845391, 0.4136515919752626, 0.2585322449845391, 0.05170644899690782, 0.5842177045199467, 0.10622140082180849, 0.10622140082180849, 0.053110700410904245, 0.10622140082180849, 0.053110700410904245, 0.2189599267520079, 0.2189599267520079, 0.0729866422506693, 0.0729866422506693, 0.2189599267520079, 0.1459732845013386, 0.277191811952636, 0.554383623905272, 0.138595905976318, 0.7046432206242146, 0.9225598150280803, 0.8029671096907389, 0.8614333359886512, 0.16446919690099068, 0.32893839380198137, 0.16446919690099068, 0.32893839380198137, 0.9101887808562957, 0.815503264168794, 0.3165830304549382, 0.6331660609098764, 0.15495311278288215, 0.3099062255657643, 0.46485933834864646, 0.2284272879108936, 0.2284272879108936, 0.2284272879108936, 0.2284272879108936, 0.8029671096907389, 0.815503264168794, 0.9355102950728879, 0.03464852944714399, 0.5806976361893486, 0.9519598759160149, 0.8183393939678678, 0.03719724518035763, 0.07439449036071526, 0.07439449036071526, 0.8285623201889653, 0.059488301087017956, 0.8923245163052693, 0.5363558010924103, 0.29052605892505556, 0.08939263351540172, 0.02234815837885043, 0.02234815837885043, 0.02234815837885043, 0.9225596945986161, 0.9419497636155864, 0.9192979760118333, 0.4910641399031931, 0.24553206995159654, 0.16368804663439768, 0.16368804663439768, 0.893994997368642, 0.8359452100573923, 0.8029671096907389, 0.4607300588694294, 0.4607300588694294, 0.8029671096907389, 0.9225596945986161, 0.820717459723914, 0.0683931216436595, 0.0683931216436595, 0.5534850606011339, 0.14191924630798303, 0.07095962315399151, 0.11353539704638643, 0.07095962315399151, 0.028383849261596608, 0.028383849261596608, 0.22693179864805976, 0.30257573153074635, 0.18910983220671648, 0.22693179864805976, 0.07564393288268659, 0.8659797898681812, 0.8285623201889053, 0.6754926125717509, 0.815503264168794, 0.27836900816062965, 0.27836900816062965, 0.27836900816062965, 0.8359452100573923, 0.6401413554732315, 0.21338045182441048, 0.3648779986847945, 0.09121949967119862, 0.18243899934239724, 0.04560974983559931, 0.04560974983559931, 0.04560974983559931, 0.18243899934239724, 0.04560974983559931, 0.8890048632036475, 0.6754926125725459, 0.815503264168794, 0.47577628056085774, 0.2021937938673629, 0.8087751754694515, 0.36376847995601896, 0.12838887527859494, 0.06419443763929747, 0.16048609409824366, 0.11768980233871203, 0.010699072939882912, 0.08559258351906329, 0.05349536469941456, 0.021398145879765823, 0.8029671096907389, 0.7808356226826119, 0.19520890567065297, 0.47577628056085774, 0.9225598150280803, 0.6027958352338206, 0.1236504277402709, 0.21638824854547406, 0.04636891040260158, 0.929924561796244, 0.9419497636156089, 0.8359452100573923, 0.3604127701292304, 0.15446261576967016, 0.15446261576967016, 0.05148753858989006, 0.2574376929494503, 0.9297328755445942, 0.9531826999966955, 0.8029671096907389, 0.17746725910552513, 0.7690247894572756, 0.44166224571838186, 0.14722074857279396, 0.2944414971455879, 0.14722074857279396, 0.5947913372235911, 0.29739566861179556, 0.8359452100573923, 0.8359452100573923, 0.8359452100573923, 0.930429041886361, 0.03446033488468003, 0.9642778528955527, 0.09416915862877923, 0.18833831725755845, 0.5650149517726754, 0.09416915862877923, 0.2713177962208951, 0.13565889811044754, 0.13565889811044754, 0.13565889811044754, 0.2713177962208951, 0.5806976361893486, 0.8029671096907389, 0.9101887808562957, 0.8029671096907389, 0.815503264168794, 0.8359452100573923, 0.2535911932198653, 0.06339779830496632, 0.22189229406738215, 0.06339779830496632, 0.1584944957624158, 0.1584944957624158, 0.03169889915248316, 0.06339779830496632, 0.03169889915248316, 0.8614333359886512, 0.9531826999966955, 0.8359452100573923, 0.7550989886378021, 0.8697052555522335, 0.8029671096907389, 0.9061970557683284, 0.9642778528956902, 0.9101887808562957, 0.8614333359886512, 0.06932015973479626, 0.06932015973479626, 0.20796047920438876, 0.4852411181435738, 0.06932015973479626, 0.1995957219811352, 0.1995957219811352, 0.1995957219811352, 0.1995957219811352, 0.6754926125717509, 0.8552181410370865, 0.0610870100740776, 0.0610870100740776, 0.8614333359886512, 0.28386100315494517, 0.4257915047324178, 0.10644787618310445, 0.07096525078873629, 0.035482625394368146, 0.035482625394368146, 0.035482625394368146, 0.035482625394368146, 0.8029671096907389, 0.29690229492446546, 0.29690229492446546, 0.29690229492446546, 0.8029671096907389, 0.8029671096907389, 0.5611550669389284, 0.8359452100573923, 0.8359452100573923, 0.8359452100573923, 0.8381410909532752, 0.3112241787908841, 0.23341813409316306, 0.03890302234886051, 0.07780604469772102, 0.3112241787908841, 0.07780604469772102, 0.17012173479786433, 0.17012173479786433, 0.34024346959572865, 0.17012173479786433, 0.7671052432657586, 0.25570174775525284, 0.5391525307497699, 0.26957626537488494, 0.8029671096907389, 0.23580550092808233, 0.707416502784247, 0.4064309893054088, 0.031263922254262215, 0.12505568901704886, 0.15631961127131105, 0.18758353352557328, 0.09379176676278664, 0.031263922254262215, 0.4751517578469054, 0.04319561434971867, 0.04319561434971867, 0.2375758789234527, 0.15118465022401537, 0.021597807174859335, 0.021597807174859335, 0.021597807174859335, 0.28626658876896005, 0.09542219625632001, 0.09542219625632001, 0.19084439251264002, 0.38168878502528003, 0.8490834345444722, 0.7590978110280479, 0.8359452100573923, 0.15924140750269275, 0.15924140750269275, 0.3184828150053855, 0.15924140750269275, 0.15924140750269275, 0.815503264168794, 0.9496602006249131, 0.9225598150280803, 0.9580764716993136, 0.7550989886378021, 0.7098428744075418, 0.7414421836677552, 0.08987177983851577, 0.11233972479814472, 0.044935889919257885, 0.044935889919257885, 0.37205596296169485, 0.5580839444425423, 0.18869447474333567, 0.7547778989733427, 0.8659797898682128, 0.01694369508251342, 0.8641284492081844, 0.01694369508251342, 0.050831085247540264, 0.03388739016502684, 0.47577628056085774, 0.8653336269433729, 0.23696017549898188, 0.47392035099796376, 0.23696017549898188, 0.2828531596071416, 0.2828531596071416, 0.2828531596071416, 0.9101887808562957, 0.9531826999966329, 0.27455331736833394, 0.3294639808420007, 0.054910663473666785, 0.27455331736833394, 0.054910663473666785, 0.8029671096907389, 0.949660200624903, 0.8614333359886512, 0.8653336269433729, 0.39365894426593406, 0.039365894426593405, 0.07873178885318681, 0.039365894426593405, 0.19682947213296703, 0.15746357770637362, 0.07873178885318681, 0.039365894426593405, 0.9299252736450614, 0.9755398567093524, 0.9531826999966329, 0.24063050364986371, 0.7218915109495911, 0.8029671096907389, 0.7253835170568377, 0.18134587926420942, 0.9535470112549158, 0.9797355629195105, 0.8925814444813815, 0.7743034527146371, 0.04554726192439041, 0.18218904769756164, 0.8029671096907389, 0.15959176757850405, 0.15959176757850405, 0.6383670703140162, 0.8029671096907389, 0.2072733160732798, 0.4145466321465596, 0.2072733160732798, 0.2072733160732798, 0.9225598150280803, 0.9398385450099487, 0.5611550669389284, 0.957960362215393, 0.9101887808562957, 0.3564123552452516, 0.2772096096351957, 0.05940205920754194, 0.01980068640251398, 0.11880411841508388, 0.03960137280502796, 0.01980068640251398, 0.01980068640251398, 0.05940205920754194, 0.01980068640251398, 0.815503264168794, 0.8029671096907389, 0.6504696468562491, 0.16261741171406227, 0.08130870585703114, 0.08130870585703114, 0.03813548909434162, 0.26694842366039134, 0.5720323364151243, 0.03813548909434162, 0.03813548909434162, 0.03813548909434162, 0.03813548909434162, 0.930246223523979, 0.8029671096907389, 0.41674111968825395, 0.41674111968825395, 0.7046432206242146, 0.8614333359886512, 0.29503254724586825, 0.29503254724586825, 0.29503254724586825, 0.23781235874579185, 0.38049977399326695, 0.04756247174915837, 0.09512494349831674, 0.09512494349831674, 0.04756247174915837, 0.04756247174915837, 0.09512494349831674, 0.10931568359018688, 0.21863136718037376, 0.10931568359018688, 0.5465784179509344, 0.2585207987436715, 0.08617359958122384, 0.08617359958122384, 0.4308679979061192, 0.08617359958122384, 0.4762627397973662, 0.15875424659912207, 0.15875424659912207, 0.07937712329956104, 0.07937712329956104, 0.07937712329956104, 0.8029671096907389, 0.43884505810368035, 0.43884505810368035, 0.17810188295659773, 0.623356590348092, 0.08905094147829887, 0.08905094147829887, 0.9101887808562957, 0.8029671096907389, 0.9726623461640882, 0.10003416315900124, 0.15005124473850187, 0.40013665263600495, 0.10003416315900124, 0.20006832631800248, 0.05001708157950062, 0.05001708157950062, 0.3084441758703775, 0.09638880495949295, 0.09638880495949295, 0.2506108928946817, 0.13494432694329014, 0.038555521983797185, 0.07711104396759437, 0.8359452100573923, 0.815503264168794, 0.08862866705841199, 0.17725733411682398, 0.265886001175236, 0.44314333529205996, 0.8029671096907389, 0.5611550669389284, 0.1268165675560966, 0.2536331351121932, 0.2536331351121932, 0.1268165675560966, 0.1268165675560966, 0.1268165675560966, 0.2907818375960042, 0.2907818375960042, 0.2907818375960042, 0.8029671096907389, 0.9419497636155785, 0.8029671096907389, 0.4200001104185468, 0.49636376685828254, 0.03818182821986789, 0.07636365643973578, 0.9225596945986161, 0.8614333359886512, 0.8614333359886512, 0.8359452100573923, 0.815503264168794, 0.5611550669389284, 0.6386328266416501, 0.09123326094880715, 0.09123326094880715, 0.09123326094880715, 0.09123326094880715, 0.6754926125717509, 0.6754926125717509, 0.6754926125717509, 0.9726623461640937, 0.9531826999966329, 0.8967827273016963, 0.09964252525574402, 0.917788383780292, 0.9225598150280803, 0.5616762106748187, 0.18722540355827288, 0.18722540355827288, 0.16802630468827698, 0.16802630468827698, 0.33605260937655396, 0.16802630468827698, 0.16802630468827698, 0.8029671096907389, 0.36075717370559574, 0.36075717370559574, 0.9419497636156089, 0.9521688172112048, 0.09163826509830829, 0.27491479529492485, 0.641467855688158, 0.8029671096907389, 0.8659797898682091, 0.8359452100573923, 0.9531826999966329, 0.8029671096907389, 0.34114126815580864, 0.6822825363116173, 0.6754926125717509, 0.9225596945986161, 0.13123571530137618, 0.26247143060275235, 0.26247143060275235, 0.13123571530137618, 0.13123571530137618, 0.8029671096907389, 0.7550989886378021, 0.957960362215393, 0.8359452100573923, 0.321399450777826, 0.5356657512963766, 0.21978521571285978, 0.6593556471385793, 0.8359452100573923, 0.957960362215393, 0.815503264168794, 0.2323210374034263, 0.5973969533230962, 0.16594359814530452, 0.8359452100573923, 0.18646084229399928, 0.18646084229399928, 0.18646084229399928, 0.37292168458799857, 0.7046432206242146, 0.3684128557703685, 0.3684128557703685, 0.5534094403972755, 0.06148993782191949, 0.30744968910959747, 0.06148993782191949, 0.06148993782191949, 0.26162484622212917, 0.6104579745183014, 0.08720828207404305, 0.3779251208586304, 0.1889625604293152, 0.1889625604293152, 0.1889625604293152, 0.1889625604293152, 0.4396231067043506, 0.2198115533521753, 0.2198115533521753, 0.9018126404714816, 0.5493518186086328, 0.7895646433229242, 0.15791292866458484, 0.9414160498522889, 0.815503264168794, 0.4589438091607568, 0.02294719045803784, 0.1147359522901892, 0.04589438091607568, 0.02294719045803784, 0.02294719045803784, 0.06884157137411351, 0.04589438091607568, 0.06884157137411351, 0.13768314274822702, 0.7046432206242146, 0.7046432206242146, 0.8359452100573923, 0.4649150000895072, 0.4649150000895072, 0.8029671096907389, 0.4545678667344253, 0.4545678667344253, 0.8029671096907389, 0.8614333359886512, 0.29906468674598685, 0.5981293734919737, 0.5515488278214509, 0.18384960927381694, 0.18384960927381694, 0.44754723098812804, 0.44754723098812804, 0.8359452100573923, 0.9726623461639914, 0.5391658726215716, 0.43133269809725727, 0.4262551572768836, 0.4262551572768836, 0.7046432206242146, 0.43962310578370956, 0.21981155289185478, 0.21981155289185478, 0.957960362215393, 0.8359452100573923, 0.34155762999263045, 0.5123364449889457, 0.9018126404714337, 0.2990497792815583, 0.6728620033835062, 0.5774997047142795, 0.18046865772321236, 0.03609373154464247, 0.04812497539285662, 0.060156219241070785, 0.07218746308928493, 0.012031243848214156, 0.3873790697086138, 0.12912635656953794, 0.04304211885651264, 0.08608423771302529, 0.3012948319955885, 0.04304211885651264, 0.8029671096907389, 0.8829292553823571, 0.10387403004498319, 0.3720559796753812, 0.5580839695130717, 0.6806739816250211, 0.06806739816250211, 0.06806739816250211, 0.20420219448750634, 0.3155028134383073, 0.6310056268766145, 0.7046432206242146, 0.6754926125717509, 0.27635343627219233, 0.27635343627219233, 0.1658120617633154, 0.27635343627219233, 0.3575637215647913, 0.3575637215647913, 0.17878186078239566, 0.8029671096907389, 0.7590978110279996, 0.8927171654050161, 0.11158964567562701, 0.47577628056085774, 0.815503264168794, 0.40172555271127447, 0.40172555271127447, 0.16069022108450978, 0.3607612201888243, 0.3607612201888243, 0.24421114436076247, 0.24421114436076247, 0.24421114436076247, 0.24421114436076247, 0.9786130999239978, 0.44715966354625275, 0.20325439252102398, 0.12195263551261439, 0.06097631775630719, 0.0406508785042048, 0.06097631775630719, 0.0406508785042048, 0.0203254392521024, 0.7046432206242146, 0.8359452100573923, 0.7046432206242146, 0.8359452100573923, 0.185831335107181, 0.557494005321543, 0.185831335107181, 0.8359452100573923, 0.2698948104883025, 0.2698948104883025, 0.44982468414717086, 0.8029671096907389, 0.8029671096907389, 0.8029671096907389, 0.6114279227324324, 0.3057139613662162, 0.8614333359886512, 0.8490834345444722, 0.8554903279662709, 0.7098428744075418, 0.49013987634841444, 0.09802797526968288, 0.14704196290452431, 0.04901398763484144, 0.24506993817420722, 0.2085558335371673, 0.3649727086900428, 0.05213895838429183, 0.05213895838429183, 0.2085558335371673, 0.05213895838429183, 0.05213895838429183, 0.51492833125893, 0.17164277708630998, 0.34328555417261997, 0.6754926125717509, 0.815503264168794, 0.4515991285143197, 0.5017768094603552, 0.5785633291848457, 0.27481758136280166, 0.03616020807405285, 0.07955245776291626, 0.01446408322962114, 0.00723204161481057, 0.01446408322962114, 0.23895292280084088, 0.7168587684025226, 0.39102423880082393, 0.3784105536782167, 0.1639779065938939, 0.05045474049042889, 0.012613685122607222, 0.8359452100573923, 0.21290455612878018, 0.49677729763382045, 0.21290455612878018, 0.07096818537626007, 0.7214323005646414, 0.7214323005646414, 0.9414160498522889, 0.23260931293381532, 0.1550728752892102, 0.5427550635122358, 0.0775364376446051, 0.8653336269433729, 0.8614333359886512, 0.123807409296046, 0.1980918548736736, 0.2971377823105104, 0.123807409296046, 0.2723763004513012, 0.5457673949632319, 0.27288369748161595, 0.3273209473196713, 0.10910698243989043, 0.3273209473196713, 0.10910698243989043, 0.21821396487978087, 0.8359452100573923, 0.11232671025396855, 0.33698013076190564, 0.11232671025396855, 0.11232671025396855, 0.2246534205079371, 0.2246534205079371, 0.08145682756220862, 0.24437048268662587, 0.24437048268662587, 0.4072841378110431, 0.08145682756220862, 0.8029671096907389, 0.2869403325254757, 0.2869403325254757, 0.4304104987882135, 0.9531826770359842, 0.7046432206242146, 0.8029671096907389, 0.09327566715517156, 0.839481004396544, 0.43900689568398665, 0.43900689568398665, 0.6857289439969932, 0.3568307703154262, 0.13381153886828484, 0.1784153851577131, 0.08920769257885655, 0.044603846289428276, 0.08920769257885655, 0.08920769257885655, 0.08920769257885655, 0.6754926125725459, 0.6857289439969932, 0.9225596945986161, 0.950994292665014, 0.8285623201889332, 0.7214323005646414, 0.4088723018208172, 0.06814538363680286, 0.13629076727360573, 0.06814538363680286, 0.06814538363680286, 0.2044361509104086, 0.5611550669389284, 0.15158870408207575, 0.7579435204103787, 0.07579435204103788, 0.7214323005646414, 0.7046432206242146, 0.24370394716151622, 0.6498771924307098, 0.2824870776841875, 0.2824870776841875, 0.2824870776841875, 0.8413199141470471, 0.08413199141470472, 0.08413199141470472, 0.957960362215393, 0.9419497636155616, 0.7098428744075418, 0.47577628056085774, 0.8029671096907389, 0.9579603622153128, 0.17729338854344806, 0.17729338854344806, 0.17729338854344806, 0.3545867770868961, 0.2545257516054802, 0.06363143790137005, 0.1272628758027401, 0.1272628758027401, 0.06363143790137005, 0.06363143790137005, 0.1272628758027401, 0.1272628758027401, 0.7148654467998636, 0.05498964975383567, 0.13747412438458917, 0.0824844746307535, 0.3708425432624546, 0.2119100247214026, 0.1059550123607013, 0.08829584363391775, 0.12361418108748486, 0.01765916872678355, 0.01765916872678355, 0.0353183374535671, 0.0353183374535671, 0.6614254459717038, 0.10443559673237429, 0.13924746230983237, 0.06962373115491619, 0.03481186557745809, 0.03481186557745809, 0.46542012711758873, 0.2585667372875493, 0.31028008474505914, 0.7214323005646414, 0.7590978110279996, 0.5611550669389284, 0.5493518186086328, 0.327489039890081, 0.654978079780162, 0.16456007238050716, 0.8228003619025358, 0.5019816530328637, 0.07171166471898052, 0.10756749707847078, 0.10756749707847078, 0.03585583235949026, 0.03585583235949026, 0.07171166471898052, 0.07171166471898052, 0.03585583235949026, 0.659939137147501, 0.21997971238250033, 0.9225596945986161, 0.9497535370550635, 0.8029671096907389, 0.8359452100573923, 0.8335716629447225, 0.815503264168794, 0.8029671096907389, 0.5467971086054288, 0.12618394813971434, 0.1892759222095715, 0.10515329011642863, 0.2499616756828351, 0.6249041892070878, 0.12498083784141754, 0.8614333359886512, 0.7214323005646414, 0.9419497636155113, 0.9225596945986161, 0.12178510759595237, 0.12178510759595237, 0.24357021519190475, 0.4871404303838095, 0.7214323005646414, 0.8029671096907389, 0.2392928870115174, 0.2392928870115174, 0.2392928870115174, 0.2392928870115174, 0.8285623201889707, 0.47870814220958186, 0.23935407110479093, 0.11967703555239546, 0.11967703555239546, 0.11967703555239546, 0.6000837239395868, 0.12001674478791734, 0.12001674478791734, 0.12001674478791734, 0.12001674478791734, 0.9726623461640079, 0.21166277667534159, 0.5387779769917785, 0.057726211820547706, 0.057726211820547706, 0.09621035303424617, 0.019242070606849234, 0.019242070606849234, 0.29521664283793864, 0.5904332856758773, 0.059043328567587726, 0.8074498261469379, 0.07340452964972162, 0.07340452964972162, 0.8029671096907389, 0.19804146400088463, 0.09902073200044231, 0.29706219600132694, 0.19804146400088463, 0.09902073200044231, 0.09902073200044231, 0.7214323005646414, 0.21569100215085985, 0.21569100215085985, 0.4313820043017197, 0.1510213383358253, 0.7551066916791265, 0.8359452100573923, 0.6754926125717509, 0.7046432206242146, 0.8029671096907389, 0.286183429089127, 0.09539447636304232, 0.1430917145445635, 0.1430917145445635, 0.09539447636304232, 0.04769723818152116, 0.04769723818152116, 0.09539447636304232, 0.4445532005723668, 0.029636880038157787, 0.14818440019078893, 0.2370950403052623, 0.08891064011447336, 0.029636880038157787, 0.029636880038157787, 0.029636880038157787, 0.9419497636155864, 0.7046432206242146, 0.2812914750985071, 0.17580717193656695, 0.21096860632388031, 0.03516143438731339, 0.03516143438731339, 0.14064573754925355, 0.07032286877462678, 0.03516143438731339, 0.7046432206242146, 0.815503264168794, 0.6044212939832901, 0.30221064699164507, 0.815503264168794, 0.135710069631973, 0.135710069631973, 0.542840278527892, 0.2804067081486593, 0.5608134162973186, 0.8913224307704587, 0.33474619912885406, 0.05579103318814234, 0.2789551659407117, 0.11158206637628468, 0.05579103318814234, 0.11158206637628468, 0.05579103318814234, 0.8359452100573923, 0.7671051620492916, 0.25570172068309716, 0.47577628056085774, 0.8359452100573923, 0.9101887808562957, 0.4954255000702886, 0.33028366671352577, 0.16514183335676288, 0.14619318704923984, 0.7309659352461991, 0.8784913900145254, 0.07986285363768413, 0.9101887808562957, 0.8029671096907389, 0.9101887808562957, 0.9101887808562957, 0.18582609012263737, 0.12388406008175826, 0.12388406008175826, 0.06194203004087913, 0.30971015020439563, 0.06194203004087913, 0.06194203004087913, 0.06194203004087913, 0.06194203004087913, 0.1792904390289875, 0.29881739838164584, 0.29881739838164584, 0.05976347967632917, 0.05976347967632917, 0.05976347967632917, 0.26816070730990665, 0.2085694390188163, 0.08938690243663555, 0.029795634145545185, 0.08938690243663555, 0.14897817072772593, 0.08938690243663555, 0.05959126829109037, 0.029795634145545185, 0.21182208032606453, 0.6354662409781936, 0.9497754895437188, 0.9716443116645952, 0.22586675830676547, 0.6776002749202965, 0.07528891943558849, 0.3109242606720026, 0.1554621303360013, 0.4663863910080039, 0.8614333359886512, 0.38056903644944534, 0.38056903644944534, 0.18582930803281872, 0.09291465401640936, 0.5574879240984562, 0.09291465401640936, 0.09291465401640936, 0.08751094219557033, 0.8313539508579181, 0.08751094219557033, 0.4029787482214852, 0.2014893741107426, 0.1007446870553713, 0.13432624940716173, 0.03358156235179043, 0.1007446870553713, 0.17647623922358785, 0.7059049568943514, 0.09277920250061884, 0.1855584050012377, 0.6030648162540225, 0.09277920250061884, 0.5611550669389284, 0.24645726623199793, 0.24645726623199793, 0.49291453246399586, 0.9531826999966955, 0.0677029897094561, 0.08462873713682012, 0.6431784022398329, 0.20310896912836826, 0.8029671096907389, 0.2159346967281046, 0.2159346967281046, 0.4318693934562092, 0.21431495726025404, 0.7501023504108891, 0.8458093909665028, 0.5705890316606705, 0.25359512518252025, 0.12679756259126013, 0.021132927098543353, 0.021132927098543353, 0.20635239650524492, 0.6190571895157347, 0.2237071796296827, 0.2237071796296827, 0.2237071796296827, 0.2237071796296827, 0.43577766670469686, 0.21788883335234843, 0.21788883335234843, 0.21788883335234843, 0.8029671096907389, 0.9531826999966329, 0.1774476955022642, 0.44361923875566045, 0.26617154325339626, 0.0887238477511321, 0.0887238477511321, 0.8842869972903121, 0.07369058310752602, 0.08291871570239956, 0.5389716520655972, 0.2072967892559989, 0.16583743140479912, 0.9101887808562957, 0.8653336269433729, 0.8029671096907389, 0.41006974528099976, 0.2733798301873332, 0.23920735141391652, 0.05125871816012497, 0.8359452100573923, 0.07579975613561078, 0.8337973174917186, 0.07579975613561078, 0.972662346164075, 0.815503264168794, 0.45664793769924267, 0.10959550504781825, 0.018265917507969708, 0.18265917507969706, 0.018265917507969708, 0.12786142255578795, 0.07306367003187883, 0.8359452100573923, 0.8359452100573923, 0.2644657147251055, 0.528931429450211, 0.9875102247983426, 0.4418155105063535, 0.06797161700097745, 0.033985808500488726, 0.4418155105063535, 0.8029671096907389, 0.9299245617974502, 0.47577628056085774, 0.9579603622153128, 0.7590978110279996, 0.8653336269433729, 0.8653336269433729, 0.8285623201889888, 0.7046432206242146, 0.9642778528955527, 0.6395352423035084, 0.11420272183991223, 0.13704326620789467, 0.06852163310394734, 0.022840544367982447, 0.022840544367982447, 0.9018126404714337, 0.29078183825872705, 0.29078183825872705, 0.29078183825872705, 0.15415704114109582, 0.7707852057054791, 0.8433702621784152, 0.7046432206242146, 0.12523319999926794, 0.12523319999926794, 0.7513991999956076, 0.9419497636155864, 0.9419497636155864, 0.256611012203804, 0.256611012203804, 0.256611012203804, 0.256611012203804, 0.47577628056085774, 0.5108180526990703, 0.5108180526990703, 0.34286250443916877, 0.2857187536993073, 0.057143750739861464, 0.11428750147972293, 0.057143750739861464, 0.17143125221958438, 0.7214323005646414, 0.573486669242558, 0.16867254977722293, 0.13493803982177835, 0.03373450995544459, 0.03373450995544459, 0.03373450995544459, 0.47577628056085774, 0.3109762696530566, 0.6219525393061132, 0.891322430770197, 0.596691136423602, 0.13561162191445503, 0.027122324382891003, 0.13561162191445503, 0.027122324382891003, 0.10848929753156401, 0.8381410909532405, 0.9453495605766522, 0.7550989886378021, 0.08237173033514016, 0.24711519100542048, 0.6589738426811212, 0.47577628056085774, 0.815503264168794, 0.21653932856006047, 0.16240449642004534, 0.43307865712012095, 0.10826966428003024, 0.9192971310503041, 0.5390599577761328, 0.049005450706921166, 0.24502725353460583, 0.049005450706921166, 0.049005450706921166, 0.049005450706921166, 0.8359452100573923, 0.2638087008696516, 0.5276174017393032, 0.8285623201889454, 0.14394858028156232, 0.6717600413139575, 0.04798286009385411, 0.04798286009385411, 0.04798286009385411, 0.04798286009385411, 0.8359452100573923, 0.5542676263544435, 0.27713381317722174, 0.9825659532176527, 0.8029671096907389, 0.6984221795580726, 0.11264873863839882, 0.09011899091071905, 0.045059495455359526, 0.045059495455359526, 0.8939949973686028, 0.8653336269433729, 0.9299245617962465, 0.3141450813477828, 0.41886010846371036, 0.10471502711592759, 0.20943005423185518, 0.1296782380037792, 0.1296782380037792, 0.1296782380037792, 0.1296782380037792, 0.3890347140113376, 0.45940148021549243, 0.18613680663903573, 0.09108822452548557, 0.01188107276419377, 0.07524679417322722, 0.05148464864483967, 0.03168286070451672, 0.03564321829258131, 0.05148464864483967, 0.4177911975690223, 0.08952668519336192, 0.14921114198893654, 0.05968445679557461, 0.05968445679557461, 0.17905337038672384, 0.029842228397787305, 0.029842228397787305, 0.891322430770197, 0.9101887808562957, 0.42102433783603915, 0.42102433783603915, 0.5611550669389284, 0.12232484613429906, 0.8562739229400934, 0.8490834345444722, 0.9355588976718615, 0.8691234934866593, 0.8029671096907389, 0.1148984404883118, 0.6893906429298707, 0.1148984404883118, 0.1148984404883118, 0.17830254366343823, 0.17830254366343823, 0.35660508732687646, 0.35660508732687646, 0.8029671096907389, 0.8029671096907389, 0.8359452100573923, 0.957960362215393, 0.9225598150280803, 0.9531826999966955, 0.43799319907635914, 0.1030572233120845, 0.1030572233120845, 0.15458583496812675, 0.05152861165604225, 0.025764305828021125, 0.1030572233120845, 0.025764305828021125, 0.3207804654311414, 0.224546325801799, 0.1603902327155707, 0.09623413962934242, 0.06415609308622829, 0.12831218617245657, 0.03207804654311414, 0.815503264168794, 0.1525637519873652, 0.3051275039747304, 0.45769125596209553, 0.31448571929836344, 0.10482857309945448, 0.27255429005858167, 0.14676000233923628, 0.04193142923978179, 0.0628971438596727, 0.04193142923978179, 0.020965714619890895, 0.3951379380445659, 0.3951379380445659, 0.8029671096907389, 0.5022034159079307, 0.1674011386359769, 0.3348022772719538, 0.2242017989898879, 0.2242017989898879, 0.4484035979797758, 0.7214323005646414, 0.4780659030392782, 0.2390329515196391, 0.2390329515196391, 0.8029671096907389, 0.6652571360059106, 0.13305142720118213, 0.13305142720118213, 0.13305142720118213, 0.08984406513710008, 0.7187525210968007, 0.17968813027420016, 0.9844827746275943, 0.47577628056085774, 0.9101887808562957, 0.8285623201889429, 0.7046432206242146, 0.8614333359886512, 0.15639456293029994, 0.07819728146514997, 0.31278912586059987, 0.07819728146514997, 0.31278912586059987, 0.07819728146514997, 0.9299245617962342, 0.7866712054955974, 0.06555593379129979, 0.13111186758259957, 0.8359452100573923, 0.19691134905020213, 0.39382269810040427, 0.39382269810040427, 0.22941777119687304, 0.688253313590619, 0.45797037070923563, 0.15265679023641188, 0.08219981012729871, 0.1291711302000408, 0.1291711302000408, 0.011742830018185529, 0.023485660036371057, 0.2192171592308039, 0.15345201146156273, 0.08768686369232156, 0.28498230700004507, 0.08768686369232156, 0.08768686369232156, 0.06576514776924117, 0.02192171592308039, 0.02192171592308039, 0.39315434271426625, 0.1834720265999909, 0.10484115805713766, 0.1572617370857065, 0.07863086854285325, 0.05242057902856883, 0.7342997474016656, 0.3671498737008328, 0.685728882134378, 0.9419497636155925, 0.8490834345444722, 0.8359452100573923, 0.949660200624903], "Term": ["1993", "1993", "1993", "1993", "1997year", "1day", "2001", "2002", "2002", "2002", "2002", "2002", "2002", "2005", "2005", "2005", "2005", "2005", "2006", "2006", "2006", "2006", "2007", "2007", "2007", "2007", "2007", "2007", "2009", "2009", "2009", "2009", "2009", "2009", "2013", "2013", "2013", "21", "30", "604614", "7month", "ability", "ability", "ability", "ability", "abstracted", "abundant", "access", "access", "achieve", "achieve", "achieve", "achieved", "achieved", "achieved", "achieved", "acknowledge", "acquirement", "adaptation", "adaptation", "adjacent", "agreement", "algorithm", "algorithm", "algorithm", "algorithm", "algorithmic", "algorithms", "algorithms", "alignment", "alignment", "alignment", "alignment", "alignment", "alignment", "allow", "alternating", "analyses", "analysis", "analysis", "analysis", "analysis", "analyzer", "annotated", "annotating", "annotations", "annotations", "answering", "apidianaki", "applied", "applied", "applied", "approach", "approach", "approach", "approach", "approach", "approach", "approach", "approaches", "approaches", "approaches", "approaches", "approaches", "arabic", "argmax", "asia", "aspects", "assign", "assign", "assign", "assigned", "assigning", "assigning", "association", "association", "association", "association", "association", "association", "association", "association", "atemporal", "atr", "attempts", "attracted", "aware", "aware", "based", "based", "based", "based", "based", "based", "based", "based", "based", "baseline1", "bayesian", "bayesian", "benchmark", "bengio", "bilingual", "bilingual", "bilingual", "bilingual", "bitam", "bitext", "blank", "bleu", "bleu", "bleu", "bleu", "bleu", "blsa", "bolt", "bottleneck", "brown", "brown", "build", "build", "build", "build", "bulgaria", "bulgaria", "bunsetsu", "bunsetsu1", "bunsetsus", "cache", "cache", "caches", "called", "called", "called", "called", "candidates", "candidates", "candidates", "candidates", "candidates", "caseframe", "cassidy", "categorized", "central", "chien", "chieu", "chinese", "chinese", "chinese", "chinese", "chinese", "chinese", "chinese", "chinese", "chinese", "cityu", "classifier", "classifying", "clinton", "clir", "clustering", "clusters", "collection", "commercial", "company", "comparable", "comparable", "comparable", "comparable", "comparable", "compared", "compared", "compared", "compared", "complements", "complexity", "complexity", "complexity", "composite", "computational", "computational", "computational", "computational", "computational", "computational", "computational", "computational", "conduct", "conducted", "conducted", "conducted", "conducting", "confidence", "conll2003", "conrmed", "considers", "consisting", "constituents", "context", "context", "context", "context", "context", "context", "contexts", "contexts", "contexts", "contexts", "contrast", "contrast", "cooccurrences", "cooccurrences", "coordinating", "coreference", "coreference", "corpora", "corpora", "corpora", "corpora", "corpora", "corpora", "corpora", "corpus", "corpus", "corpus", "corpus", "corpus", "corpus", "corpus", "corpus", "corresponding", "corresponding", "corresponding", "corresponding", "corresponding", "cosine", "criteria", "crl", "cross", "cross", "cross", "cross", "cross", "crosslanguage", "crosslingual", "curriculum", "cws", "cx", "daille", "data", "data", "data", "data", "data", "datasets", "datasets", "decoder", "decoder", "decoders", "decoding", "decoding", "decoding", "decoding", "decoding", "dejean", "deletion", "demonstrate", "demonstrate", "demonstrate", "demonstrated", "demonstrated", "demonstrated", "demonstrates", "dengxiaoping", "developed", "developed", "developed", "developed", "developed", "developers", "di", "dictionarybased", "did", "different", "different", "different", "different", "different", "different", "different", "different", "directions", "dirichlet", "disagreement", "disambiguation", "disambiguation", "discouraged", "distinguish", "distinguish", "distribution", "document", "documents", "domain", "domain", "domain", "dominant", "driven", "driven", "driven", "dyer", "effectiveness", "effectiveness", "effectiveness", "effectiveness", "embedding", "embeddings", "emphasizes", "encyclopedias", "engines", "english", "english", "english", "english", "english", "english", "english", "english", "english", "english", "englishchinese", "enhancing", "entities", "entities", "entities", "entities", "entity", "entity", "entity", "entity", "entity", "entity", "entity", "entropy", "equally", "equivalence", "equivalence", "erty", "eurozone", "evaluate", "evaluate", "evaluate", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "evaluation", "example", "example", "example", "example", "existing", "existing", "existing", "existing", "existing", "experiments", "experiments", "experiments", "experiments", "experiments", "experiments", "extending", "external", "external", "extraction", "extraction", "extraction", "extraction", "extracts", "fall", "fast", "feature", "feature", "feature", "feature", "feature", "feature", "feature", "features", "features", "features", "features", "features", "features", "features", "fellow", "fields", "figure", "figure", "figure", "figure", "filling", "filter", "focus", "focus", "focus", "focus", "focus", "focus", "focuses", "focuses", "focuses", "formal", "forms", "fraction", "framework", "framework", "framework", "framework", "frequent", "fujitsu", "fujitsucompany", "functional", "fusing", "fusion", "future", "future", "future", "future", "future", "gates", "gc", "ge", "gender", "georgebush", "german", "german", "germann", "gigaword", "global", "global", "global", "goal", "goal", "goal", "goal", "goal", "government", "grammar", "grammar", "grammatical", "graph", "graphs", "graphs", "graphs", "greater", "greedy", "grishman", "guidelines", "hakkani", "half", "half", "heads", "headwords", "higher", "higher", "higher", "higher", "higher", "highquality", "hillary", "hiragana", "hirai", "hmm", "hmm", "holistic", "holistic", "hwa", "hybrid", "hypertext", "ibm", "ibm", "ibm", "immediately", "impact", "impact", "impact", "impact", "implies", "importance", "importance", "improve", "improve", "improve", "improve", "improve", "improvements", "improvements", "improvements", "improves", "improves", "improves", "improves", "improves", "including", "including", "including", "incorporate", "incorrect", "inference", "inference", "influence", "influences", "information", "information", "information", "information", "information", "information", "information", "information", "information", "information", "initialization", "insensitive", "integrally", "integrate", "integrate", "integrates", "intend", "intend", "interested", "interpolated", "interpolation", "interpolation", "introduced", "introduced", "introduced", "investigated", "investigated", "irex", "iterations", "japanese", "japanese", "ji", "ji", "jin", "jointly", "jointly", "jointly", "jou", "jsps", "kim", "kim", "kinds", "known", "known", "language", "language", "language", "language", "language", "language", "language", "languages", "languages", "languages", "languages", "languages", "languages", "latticebased", "level", "level", "leveraging", "leveraging", "lexicon", "lexicon", "lexicon", "lexicon", "lexicons", "lexicons", "lie", "life", "linear", "linear", "linear", "linear", "lingual", "lingual", "lingual", "linking", "liu", "lm", "lm", "lo", "locate", "log", "log", "log", "low", "low", "lower", "lower", "lower", "lower", "lsa", "machine", "machine", "machine", "machine", "machine", "machine", "machine", "machine", "majeure", "malouf", "mandarin", "masayuki", "matching", "matching", "matching", "matsumoto", "maximum", "maximum", "maximum", "mckeown", "mcnamee", "meanings", "measure", "measure", "measured", "measures", "mechanism", "medical", "method", "method", "method", "method", "method", "methods", "methods", "methods", "methods", "methods", "methods", "methods", "metric", "metric", "metric", "microsoft", "mitigate", "mixture", "mixture", "model", "model", "model", "model", "model", "model", "model", "modelling", "modelling", "models", "models", "models", "models", "models", "mohit", "monolingual", "monolingual", "monolingual", "monolingual", "monolingually", "morelo", "morin", "morphological", "morphological", "morphological", "morphological", "motivates", "msr", "mt", "mt", "mt", "mt", "mt", "multilingual", "multilingual", "multiple", "multiple", "multiple", "multiple", "multiple", "nakano", "named", "named", "named", "named", "named", "named", "names", "names", "names", "names", "names", "namt", "ne", "ne", "ne", "necessary", "necessity", "neglected", "ner", "ner", "nes", "nes", "neural", "new", "new", "new", "new", "new", "new", "new", "new", "nict", "nist08", "noisy", "noun", "np", "objectives", "obtained", "obtained", "obtained", "obtained", "obtained", "obtained", "obvious", "occurrences", "occurrences", "occurrences", "odds", "offer", "oov", "oov", "open", "open", "open", "optimal", "optimal", "optimal", "optimum", "original", "otero", "outcome", "outnumbered", "outputs", "overall", "overall", "overall", "overall", "pages", "pages", "pages", "pages", "pages", "pages", "pages", "pages", "pairs", "pairs", "pairs", "pairs", "paper", "paper", "paper", "paper", "paper", "paper", "paper", "paper", "paper", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parameters", "parameters", "parameters", "parsers", "partial", "particularly", "parton", "pattern", "pattern", "people", "people", "performance", "performance", "performance", "performance", "performance", "performance", "performance", "performance", "performance", "performed", "performed", "permits", "perplexity", "person", "persons", "ph", "phoneme", "phonetically", "phrase", "phrase", "phrase", "phrase", "phrases", "phrases", "phrases", "pku", "plethora", "polynomial", "polysemy", "popular", "popular", "popular", "popular", "populating", "pose", "potential", "potential", "potential", "potential", "practical", "presented", "presented", "presented", "presented", "presented", "previous", "previous", "previous", "previous", "previous", "probable", "problem", "problem", "problem", "problem", "problem", "problem", "problem", "problems", "problems", "problems", "process", "process", "process", "processes", "processing", "processing", "processing", "processing", "processing", "processing", "prochasson", "produce", "produce", "produce", "projection", "projection", "promotion", "pronunciation", "prop", "propagating", "propose", "propose", "propose", "propose", "propose", "propose", "propose", "propose", "proposed", "proposed", "proposed", "proposed", "proposed", "proposed", "proposed", "proposed", "provably", "pruned", "quality", "quality", "quality", "quality", "quality", "quality", "quality", "quality", "quantify", "queries", "query", "query", "querying", "ranking", "ranking", "ranking", "rapp", "rapp", "real", "recognition", "recognition", "recognition", "recognition", "recognition", "recognition", "recognition", "recognizes", "refined", "refined", "regular", "relation", "relational", "relations", "relations", "relations", "relationship", "relationship", "relevant", "relevant", "remaining", "rendered", "reports", "requiring", "research", "research", "research", "research", "research", "research", "research", "research", "research", "resources", "resources", "resources", "resources", "resources", "resources", "results", "results", "results", "results", "results", "results", "results", "results", "results", "retrieval", "retrieval", "role", "roles", "sampling", "sampling", "sampling", "scarce", "scarce", "scarce", "schemes", "science", "science", "score", "score", "score", "score", "score", "search", "search", "search", "section", "section", "section", "section", "section", "section", "segment", "segment", "segmentation", "segmentation", "segmentation", "segmentation", "selected", "selection", "selection", "selection", "selective", "semantic", "semantic", "semantic", "semantic", "semantically", "semantics", "semantics", "semantics", "sense", "sense", "senses", "sentence", "sentence", "sentence", "sentence", "sentence", "sequence", "sequence", "sets", "sets", "sets", "sets", "short", "short", "short", "short", "shrinking", "sighan", "significant", "significant", "significant", "significant", "significant", "similar", "similar", "similarity", "similarity", "similarity", "similarity", "similarly", "skeleton", "slot", "smt", "smt", "smt", "smt", "society", "solution", "solution", "solution", "solutions", "solves", "source", "source", "source", "source", "source", "source", "source", "spaces", "specic", "specifically", "specifically", "specifications", "speech", "speech", "speech", "speech", "spend", "spoken", "spotting", "srfs", "srl", "srss", "ssmt", "stack", "stanford", "static", "statistical", "statistical", "statistical", "statistical", "statistical", "statistical", "store", "strategy", "strategy", "strategy", "string", "string", "structural", "stubborn", "studies", "studies", "studies", "suboptimal", "subproblems", "support", "support", "support", "support", "surprisingly", "svm", "svm", "syntactic", "syntactic", "syntactic", "syntactic", "syntactic", "syntactic", "systematically", "systems", "systems", "systems", "systems", "systems", "systems", "tackling", "tagging", "tagging", "tails", "target", "target", "target", "target", "target", "target", "templates", "temporal", "temporality", "term", "term", "term", "terminological", "terminologies", "terms", "terms", "terms", "terms", "test", "text", "text", "text", "text", "text", "text", "tight", "tightly", "tightly", "tillman", "time", "time", "time", "time", "time", "time", "token", "tokens", "tokens", "topic", "traditional", "training", "training", "training", "training", "training", "transducer", "transducers", "transfer", "translate", "translate", "translate", "translate", "translating", "translating", "translating", "translating", "translating", "translation", "translation", "translation", "translation", "translation", "translation", "translation", "translation", "translation", "translations", "translations", "translations", "translations", "translations", "translations", "translations", "translations", "transliteration", "transliterationand", "travel", "travel", "trec", "tree", "tree", "tsujii", "tsuneyama", "tts", "tur", "type", "type", "type", "type", "types", "types", "types", "types", "typical", "understudy", "unit", "unknown", "unlabeled", "usain", "use", "use", "use", "use", "use", "use", "use", "use", "used", "used", "used", "used", "used", "used", "used", "users", "uses", "uses", "uses", "using", "using", "using", "using", "using", "using", "using", "using", "utilizing", "utilizing", "validation", "values", "values", "values", "variants", "variants", "variants", "varies", "variety", "variety", "variety", "vastly", "vector", "vector", "vector", "vector", "vectors", "vectors", "vectors", "verbmobil", "versus", "vi", "wang", "washington", "weak", "web", "web", "web", "web", "web", "web", "weighting", "weights", "weights", "weights", "wider", "wikipedia", "wikipedia", "wikipedia", "window", "window", "word", "word", "word", "word", "word", "word", "word", "words", "words", "words", "words", "words", "words", "words", "words", "words", "work", "work", "work", "work", "work", "work", "wsd", "wsd", "wsi", "yields", "yu", "zero", "zn"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [11, 8, 10, 3, 6, 7, 9, 4, 1, 5, 2]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el128671402369675744083111398467", ldavis_el128671402369675744083111398467_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
        new LDAvis("#" + "ldavis_el128671402369675744083111398467", ldavis_el128671402369675744083111398467_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js", function(){
         LDAvis_load_lib("https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el128671402369675744083111398467", ldavis_el128671402369675744083111398467_data);
            })
         });
}
</script>							
</div>
            </div>
          </div>
          <div class="row">
              <button type="button" id="subcollapsible" class="collapsible">Interactive ScatterPlots of Mentions (Word2Vec + TSNE)</button>
              <div class="content">
                <div class="tricolumn">
                  <button type="button" id="subcollapsible" class="collapsible">Unigram Model</button>
                  <div class="content" name="scatter_1">
                    <img src="./men_unigrams.png" alt="plot" width="1000" height="1000">
                  </div>
                </div>
                <div class="tricolumn">
                  <button type="button" id="subcollapsible" class="collapsible">Bigram Model</button>
                  <div class="content" name="scatter_2">
                    <img src="./men_bigrams.png" alt="plot" width="1000" height="1000">
                  </div>
                </div>
                <div class="tricolumn">
                  <button type="button" id="subcollapsible" class="collapsible">Trigram Model</button>
                  <div class="content" name="scatter_3">
                    <img src="./men_trigrams.png" alt="plot" width="1000" height="1000">
                  </div>
                </div>
              </div>
          </div>
        </div>
      </div>

      <button type="button"  class="collapsible">Claims</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_claims">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	We also believe that our results may help in the design of effective heuristicsfor some of these tasks .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We believe that decoding algorithms derived from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	However , the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	As there is a large overlap between the modeled events in the combined probabilistic models , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation does .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Even without recognition errors , speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech differ from that of written language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	In such case , as <strong>Hillary Clinton</strong> is a famous female leader , she may be associated with other Chinese female leaders in Chinese corpus , while such association is rarely observed in <strong>English</strong> corpus , which causes asymmetry .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	That is , <strong>Hillary Clinton</strong> is atemporal , as Figure 1 shows , such that using such dissimilarity against deciding this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Our experiments are carried out on the <strong>English</strong>-Slovene <strong>language pair</strong> but as the methods are totally data-driven , the approach can be easily applied to other languages.The paper is organized as follows : In the next section , we present some related work on bilingual lexicon extraction from comparable corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Out-of-vocabulary recognition may have two-sided effects on <strong>SMT</strong> performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 16) </strong>	We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We propose an approach that biases <strong>machine translation</strong> systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	The subject should agree with the verb in both gender and number , but the verb has masculine inflection .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	However , LM statistics are sparse , and they are made sparser by morphological variation .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	We assume here that the MT system is capable of providing <strong>word alignment</strong> ( or equivalent ) information during decoding , which is generally true for current statistical MT systems .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 1) </strong>	In such cases partial SRSs must be recorded in such a way that they can be combined later with other partial SRSs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 1) </strong>	Considering our semantic features are the most basic ones , using more sophisticated features ( e.g the head words and their translations of the sourceside semantic roles ) provides a possible direction for further experimentation
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We prove that while <strong>IBM</strong> <strong>Models 1-2</strong> are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The models are independent of the <strong>language pair</strong> and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that both of these problems are easy to solve and provide efficient solutions for them .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We have also shown that alternating maximization can be employed tocome up with O ( m2 ) decoding algorithm .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	<strong>Empirical</strong> evidence suggests that such algorithms can perform resonably well .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	All these numbers suggest that approximative algorithms are a feasible choice for practical applications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	<strong>Comparative</strong> evaluations with other translation approaches of the Verbmobil prototype <strong>system show</strong> that the<strong> statistical translation</strong> is superior , especially in the presence of speech input and ungrammatical input
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper validated that considering temporality selectively is helpful for improving the translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	However , words in a general language corpus like Wikipedia can be polysemous and it is important to identify translations corresponding to their different senses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	An additional advantage is that the sense clusters often contain more than one translation and , therefore , provide supplementary material for the comparison of the vectors in the target language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect that a method capable of identifying the <strong>correct sense</strong> of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons.In this paper , we show how source vectors can be translated into the target language by a <strong><strong>cross-lingual</strong> Word Sense Disambiguation</strong> ( <strong>WSD</strong> ) method which exploits the output of data-driven <strong>Word Sense Induction</strong> ( <strong>WSI</strong> Apidianaki , 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	An avenue that we intend to explore in future work is to extract translations corresponding to different senses of the headwords .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Moreover , it is clear that disambiguating the vectors improves the quality of the extracted lexicons and manages to beat the simpler , but yet powerful , most frequent translation heuristic .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One interesting outcome of this study is that<strong> significant gain</strong>s can be obtained by using an association measure that is rarely used in practice .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Still , it is already striking that a direct comparison of them is difficult , if not impossible .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The evaluation has demonstrated that our system is both effective and useful in a real-world environment
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	There is also significant disagreement on the specifications , although much of their contents is the same .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	However , there have not been until very recently that the application of mixture modelling in <strong>SMT</strong> has received increasing attention .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Mixture modelling is a standard technique for density estimation , but its use in statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) has just started to be explored .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	One problem with the<strong> dynamic cache</strong> is that those initial sentences in a test document may not benefit from the dynamic cache .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Another problem is that the<strong> dynamic cache</strong> may be prone to noise and cause error propagation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	At least , the topic of a document can help choose specific translation candidates , since when taken out of the context from their document , some words , phrases and even sentences may be rather ambiguous and thus difficult to understand .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Our motivation to employ similar bilingual document pairs in the training parallel corpus is simple : a human translator often collects similar bilingual document pairs to help translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Thirdly , reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	During last decade , tremendous work has been done to improve the quality of statistical machineCorresponding author .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	As the translation process continues , the<strong> dynamic cache</strong> grows and contributes more and more to the translation of subsequent sentences .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The beauty of the b<strong>LSA</strong> framework is that the<strong> model search</strong>es for a common latent topic space in an unsupervised fashion , rather than to require manual interaction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	However , our analysis has shown that for Arabic , these genres typically contain more Latin script and transliterated words , and thus there is less morphology to score .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Qc 2012 Association for Computational LinguisticsIt has also been suggested that this setting requires morphological generation because the bitext may not Pron + Fem + <strong>SgVerb</strong> + Masc +3 + PlPrtConj contain all inflected variants ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , using lexical coverage experiments , we show thatit there is ample room for translation quality improvements through better selection of forms that already exist in the <strong>translation model</strong>.they writewilland .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	<strong>English</strong> is a weakly inflected language : it has a narrow verbal paradigm , restricted nominal inflection ( plurals and only the vestiges of a case system .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Claims (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./claims_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_claims">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	We prove that while <strong>IBM</strong> <strong>Models 1-2</strong> are conceptually and computationally simple , computations involving the higher ( and more useful ) models are hard.Since it is unlikely that there exists a poly-language 1 ( Tillman , 2001 Wang , 1997 Germann et al 2003 Udupa et al 2004 The models are independent of the <strong>language pair</strong> and therefore , can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We also believe that our results may help in the design of effective heuristicsfor some of these tasks .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We believe that decoding algorithms derived from our framework can be of practical significance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Still , it is already striking that a direct comparison of them is difficult , if not impossible .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	There is also significant disagreement on the specifications , although much of their contents is the same .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 3) </strong>	We show that both of these problems are easy to solve and provide efficient solutions for them .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 3) </strong>	We have also shown that alternating maximization can be employed tocome up with O ( m2 ) decoding algorithm .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	As there is a large overlap between the modeled events in the combined probabilistic models , we assume that log-linear combination would result in more improvement of the translation quality than the combination by linear interpolation does .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	One interesting outcome of this study is that<strong> significant gain</strong>s can be obtained by using an association measure that is rarely used in practice .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community , e.g Fung ,1998 ; Rapp , 1999 Surprisingly , none of those works have systematically investigated the impact of the many parameters controlling their approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Out-of-vocabulary recognition may have two-sided effects on <strong>SMT</strong> performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 16) </strong>	We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
      </div>


      <button type="button" class="collapsible">Evidences</button>
      <div class="content">
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Bigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_bigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="bigram_evidences">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	Specifically , two types of semantic role features are proposed in this paper : a semantic role reordering feature designed to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 716724 , Beijing , August 2010 ture designed to penalize missing semantic roles in the target sentence .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	In ( Knight , 1999 ) it was proved that the Exact Decoding prob Given the model parameters and a sentence f determine the most probable translation of f lem is NP-Hard when the <strong>language model</strong> is a bigram model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	Our results showthat there can not exist a closed form expression ( whose representation is polynomial in the size of the input ) for P ( f | e ) and the counts in the EMiterations for Models 3-5 unless P NP
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	enomial time solution for any of these hardeproblems ( unless P NP and P #P P our results highlight and justify the need for developing polynomial time approximations for these computations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	In this paper , we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al 2001 ) and presented improvements that drastically reduce the decoders complexity and speed to practically linear time.<strong>Experimental</strong> data suggests a good correlation betweenG1 decoding anddecoding ( with 10 translations per input word considered , a list of 498 candidates for INSERT , a maximum swap distance of 2 and a maximum swap segment size of 5 The profiles shown are cumulative , so that the top curve reflects the total decoding time .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 ( 878 segments ; ca. 25k tokens ) requires , without any of the improvements described in this paper , over 440 CPU hours , using the simpler , faster algorithm ( de scribed below We will show that this time can be reduced to ca. 40 minutes without sacrificing translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	The speed improvements discussed in this paper make multiple randomized searches per sentence feasible , leading to a faster and better decoder for <strong>machine translation</strong> with <strong>IBM</strong> Model 4.6 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Och et al. report word error rates of 68.68 percent for optimal search ( based on a variant of the A algorithm and 69.65 percent for the most restricted version of a decoder that combines dynamic programming with a beam search ( Tillmann and <strong>Ney</strong> , 2000 Germann et al 2001 ) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem ( cf. Knight , 1999 Their overall performance metric is the sentence error rate ( SER For decoding with <strong>IBM</strong> Model 3 , they report SERs of about 57 6-word sentences ) and 76 8-word sentences ) for optimal decoding , 58percent and 75percent for stack decoding , and 60percent and 75percent for greedy decoding , which is the focus of this paper .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We have been able to obtain a significant better test corpus perplexity and also a slight improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	This additional information could be : Simple context information : information of the words surrounding the<strong> word pair</strong> ; Syntactic information : part-of-speech information , syntactic constituent , sentence Semantic information : disambiguation information ( e.g.from WordNet currentspeech or dialog act .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	Other authors have applied this approach to <strong>language model</strong>ing ( Rosenfeld , 1996 ; Martin et al 1999 ; Peters and Klakow , 1999 A short review of the maximum entropy approach is outlined in Section 3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	<strong>Experimental</strong> results on the German-<strong>English</strong> Verbmobil Source Language Textmorpho-syntactic AnalysisTransformation f J 1 Global Search : maximize Pr ( e I J e I task are reported .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	The experience obtained in the Verbmobil project , in particular a large-scale end-to-end evaluation , showed that the statistical approach resulted in significantly lower error rates than three competing translation approaches : the sentence error rate was 29percent in comparison with 52percent to 62percent for the other translation approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Even without recognition errors , speech translation has to cope with a lack of conventional syntactic structures because the structures of spontaneous speech differ from that of written language .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	In comparison with written language , speech and especially spontaneous speech poses additional difficulties for the task of automatic translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	Starting with the Bayes decision rule as in <strong>speech recognition</strong> , we show how the required probability distributions can be structured into three parts : the <strong>language model</strong> , the alignment model and the lexicon model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	We describe the components of the system and report results on the Verbmobil task .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	We validate this<strong> selective use</strong> of temporal features boosts the accuracy by 6.1 percent .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	We developed a classifier to distinguish temporalentities and our proposed method outperforms the state-of-the-art approach by 6.1
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	To overcome such problems , we propose a<strong> new notion</strong> of selective temporality ( called this fea 2.3 Step 3 : Reinforcement We reinforce R0 by leveraging R and obtain a converged matrix R using the following model : ture ST to distinguish from T ) to automatically distinguish temporal and atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	This paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 9) </strong>	All these aspects will be our research focus in the future
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Similarly , we can mine Chinese news articles to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To highlight the advantage of our proposed approach , we compare our results with commercial machine translators Engkoo3 developed in Microsoft Research Asia and Google Translator4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Our evaluation results empirically validated the accuracy of our algorithm over real-life datasets , and showed the effectiveness on our proposed perspective
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To achieve this goal , we developed a graph alignment algorithm that iteratively reinforces the matching similarity exploiting relational similarity and then extracts correct matches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Such engine This work was done when the first two authors visited Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 3 then develops our framework .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate , Figure 1 demonstrates the query result for Bill Gates , retrieving and visualizing the entity-relationship graph of related people names that frequently co-occur with Bill in <strong>English</strong> corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	While high quality entity translation is essential in <strong>cross-lingual</strong> information access and trans lation , it is non-trivial to achieve , due to the challenge that entity translation , though typically bearing pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing efforts to address these challenges can be categorized into transliterationand corpusbased approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect the disambiguation to have a beneficial impact on the results given that polysemy is a<strong> frequent phenomenon</strong> in a general , mixed-domain corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We expect that a method capable of identifying the <strong>correct sense</strong> of the features and translating them accordingly could contribute to producing cleaner vectors and to extracting higher quality lexicons.In this paper , we show how source vectors can be translated into the target language by a <strong><strong>cross-lingual</strong> Word Sense Disambiguation</strong> ( <strong>WSD</strong> ) method which exploits the output of data-driven <strong>Word Sense Induction</strong> ( <strong>WSI</strong> Apidianaki , 2009 and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	In Section 5 , we report and discuss the obtained results before concluding and presenting some directions for future work .1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora , pages 110 , Sofia , Bulgaria , August 8 , 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Qc 2013 Association for Computational Linguistics names in parallel corpora , updating <strong>word segmentation</strong> , <strong>word alignment</strong> and grammar extraction ( Section 3.1 We developed a name-aware MT framework which tightly integrates name tagging and name translation into training and decoding of MT. Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genres .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Compared to previous methods , the novel contributions of our approach are : 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The experimental pro tocol we followed is described in Section 3 and we analyze our results in Section 4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	In future works , other parameters which influence the performance will be studied , among which the use of a terminological extractor to treat complex terms ( Daille and <strong>Morin</strong> , 2005 more contextual window configurations , and the use of <strong>syntactic information</strong> in combination with lexical information ( Yu and <strong>Tsujii</strong> , 2009 It would also be interesting to compare the projection-based approaches to ( Haghighi et al 2008 ) s <strong>generative model</strong> for bilingual lexicon acquisition from monolingual corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	A closer look at the translation candidates obtained when using LL , the most popular association measure in projection-based approaches , shows that they are often collocates of the reference translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The present discussion only focuses on a few number of representative studies .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	The highest Top 1 precision , 55.2 was reached with the following parameters : sentence contexts , LO , cosine and a 9,000-entry mixed lexicon , with the use of a cognate heuristic .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Naturally , studies differ in the way each cooccurrence ( either window or syntax-based ) is weighted , and a plethora of association scores have been investigated and compared , the likelihood score ( Dunning , 1993 ) being among the most popular .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	While a few studies have investigated pattern matching approaches to compare source and target contexts ( Fung , 1995 ; Diab and Finch , 2000 ; Yu and <strong>Tsujii</strong> , 2009 most variants make use of a bilingual lexicon in order to translate the words of the context of a term ( often called seed words Dejean et al 2005 ) instead use a bilingual thesaurus for translating these.Another distinction between approaches lies in the way the context is defined .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The evaluation has demonstrated that our system is both effective and useful in a real-world environment
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We interpolated multiple <strong>translation model</strong>s generated by the <strong>CWS</strong> schemes and found our approaches were very effective in improving the translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We proposed a<strong> new approach</strong> to linear interpolation of translation features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	In this work , we also propose approaches to make use of all the Sighan training data regardless of the specifications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in<strong> significant improvement</strong>s of up to 1 <strong>BLEU</strong> and 3 TER on Chinese to <strong>English</strong> translation over a strong baseline .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a<strong> word pair</strong> under topic zn : features in the <strong>translation model</strong> , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f 1 ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We induce<strong> unsupervised domain</strong>s from large corpora , and we incorporate soft , probabilistic domain membership into a <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of <strong>word alignment</strong> in European languages , and the efficient and exact computation of the E-step and<strong> Viterbi alignment</strong> by using a dynamic-programming approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical <strong>translation model</strong>s , taking <strong>IBM</strong> Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	He proposed to employ cache-based language and <strong>translation model</strong>s in a phrase-based <strong>SMT</strong> system for domain909 Proceedings of the 2011 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	First of all , most of parallel corpora lack the annotation of document boundaries ( Tam , 2007 Secondly , although it is easy to incorporate a new feature into the classical log-linear model ( Och , 2003 it is difficult to capture document-level information and model it via some simple features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	In Section 3 , we present the effect of LM adaptation on <strong>word perplexity</strong> , followed by <strong>SMT</strong> experiments reported in <strong>BLEU</strong> on both speech and text input in Section 3.3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We organize the paper as follows : In Section 2 , we introduce the b<strong>LSA</strong> framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The key property of the b<strong>LSA</strong> model is that Proceedings of the 45th <strong>Annual Meeting</strong> of the Association of Computational Linguistics , pages 520527 , Prague , Czech <strong>Republic</strong> , June 2007 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	Results showed that our approach significantly reduces the <strong>word perplexity</strong> on the target language in both cases using ASR hypotheses and manual transcripts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	When used to compute semantic similarity of phrase pairs , bilingual embeddings improve NIST08 end-to-end <strong>machine translation</strong> results by just below half a <strong>BLEU</strong> point .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	We achieved best results when the model training data , MT tuning set , and MT evaluation set conThe bottom category includes all lexical items that the decoder could produce in a translation of the source .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Qc 2012 Association for Computational LinguisticsIt has also been suggested that this setting requires morphological generation because the bitext may not Pron + Fem + <strong>SgVerb</strong> + Masc +3 + PlPrtConj contain all inflected variants ( Minkov et al 2007 ; Toutanova et al 2008 ; Fraser et al 2012 However , using lexical coverage experiments , we show thatit there is ample room for translation quality improvements through better selection of forms that already exist in the <strong>translation model</strong>.they writewilland .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	For <strong>English</strong>-to-Arabic translation , we achieve a +1.04 <strong>BLEU</strong> average improvement by tiling our model on top of a large LM .146 Proceedings of the 50th <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 146155 , Jeju , <strong>Republic</strong> of Korea , 8-14 July 2012 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Consider the output of Google Translate for the simple <strong>English</strong> sentence in Fig. 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	Agreement relations that cross statistical phrase boundaries are not explicitly modeled in most phrasebased MT systems ( Avramidis and Koehn , 2008 We address this shortcoming with an agreement model that scores sequences of fine-grained morphosyntactic classes .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 2) </strong>	In their seminal paper on <strong>SMT</strong> , Brown and his colleagues highlighted the problems we face as we go from <strong>IBM</strong> <strong>Models 1-2</strong> to 3-5 ( Brown et al 1993 ) 3 : As we progress from Model 1 to Model 5 , evaluating the expectations that gives us counts becomes increasingly difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	We believe that our results on the computational complexity of the tasks in <strong>SMT</strong> will result in a better understanding of these tasks from a theoretical perspective .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	In practice , we are never sure that we have found the<strong> Viterbi alignment</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 2) </strong>	around the<strong> Viterbi alignment</strong> , i.e. in determining j = 1 tfj | eajdj : aj I = 0 d ( j | i , m the goodness of the Viterbi alignment in compar ison to the rest of the alignments.Decoding is an integral component of all <strong>SMT</strong> systems ( Wang , Table 1 : <strong>IBM</strong> Model 3 Here , n ( | e ) is the fertility model , t ( f | e ) is the lexicon model and d ( j | i , m is the distortion model .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	We present improvements to a greedy decoding algorithm for statistical <strong>machine translation</strong> that reduce its time complexity from at least cubic ( when applied navely ) to practically linear time1 without sacrificing translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Using the same evaluation metric ( but different evaluation data Wang and <strong>Waibel</strong> report search error rates of 7.9 percent and 9.3 respectively , for their decoders .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	<strong>IBM</strong> Model 4 scores and the <strong>BLEU</strong> metric .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	The times shown are averages of 100 sentences each for length10 , 20 80 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	Operations not included in the figures consume so little time that their plots can not be discerned in the graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 4) </strong>	We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration , and by imposing restrictions on the amount of word reordering during decoding .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	In addition , we perform a rescoring of-Best lists using our maximum entropy model and thereby yield an improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	In our approach we introduce equivalence classes in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We furthermore suggest the use of hierarchical lexicon models .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	It has been successfully applied to realistic tasks in various national and international research programs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 7) </strong>	<strong>Comparative</strong> evaluations with other translation approaches of the Verbmobil prototype <strong>system show</strong> that the<strong> statistical translation</strong> is superior , especially in the presence of speech input and ungrammatical input
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	In contrast , Figure 1 illustrates asymmetry , by showing the frequencies of Usain Bolt , a Jamaican sprinter , and <strong>Hillary Clinton</strong> , an American politician , in comparable news articles during the year 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	Early efforts of named entity translation have focused on using phonetic feature ( called PH ) to estimate a phonetic similarity between two names ( Knight and Graehl , 1998 ; Li et al 2004 ; Virga and Khudanpur , 2003 In contrast , some approaches have focused on using context feature ( called CX ) which compares surrounding words of entities ( Fung and Yee , 1998 ; Diab and Finch , 2000 ; Laroche and Langlais , 2010 Recently , holistic approaches combining such similarities have been studied ( Shao and Ng , 2004 ; You et al 2010 ; Kim et al 2011 Shao and Ng , 2004 ) rank translation candidates using PH and CX independently and return results with the highest average rank You et al 2010 ) compute initial translation scores using PH and iteratively update the scores using relationship feature ( called R Kim et al 2011 ) boost Yous approach by additionally leveraging CX .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 8) </strong>	That is , <strong>Hillary Clinton</strong> is atemporal , as Figure 1 shows , such that using such dissimilarity against deciding this pair as a correct translation would be harmful .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 9) </strong>	From the experimental results for combining our <strong>OOV</strong> term <strong>translation model</strong> with <strong>English</strong>-Chinese CrossLanguage Information Retrieval ( CLIR ) on the data sets of Text Retrieval Evaluation Conference ( <strong>TREC</strong> it can be found that the obvious performance improvement for both query translation and retrieval can also be obtained .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 4 reports experimental results and Section 5 concludes our work.<strong>English</strong> PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	1com430 Proceedings of the 2010 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 430439 , MIT , Massachusetts , <strong>USA</strong> , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate this , an <strong>English</strong> news article mentioning Bill Gates and Melinda Gates evidences a relationship between the two entities , which can be quantified from their co-occurrences in the entire English Web corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To summarize , we believe that this paper has the<strong> following contribution</strong>s : We abstract entity translation problem as a graph mapping between entity-relationship graphs in two languages.We develop an effective matching algorithm leveraging both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	We have shown how <strong>cross-lingual</strong> <strong>WSD</strong> can be applied to bilingual lexicon extraction from comparable corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 11) </strong>	The results show that data-driven semantic analysis can help to circumvent the need for an external seed dictionary , traditionally considered as a prerequisite for translation extraction from parallel corpora .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and <strong>word alignment</strong> over a high-quality MT baseline1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Tightly integrate joint bilingual name tagging into MT training by coordinating tagged604 Proceedings of the 51st <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	This need can be addressed in part by <strong>cross-lingual</strong> information access tasks such as entity linking ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lies in the performance of <strong>Machine Translation</strong> ( MT Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information , especially names .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline , and a new name-aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Instead , we investigate the impact of some major factors influencing projection-based approaches on a task of translating 5,000 terms of the medical domain ( the most studied domain making use of French and <strong>English</strong> Wikipedia pages extracted monolingually thanks to an information retrieval engine .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Therefore , LL may fare better in an indirect approach , like the one in ( Daille and <strong>Morin</strong> , 2005 Moreover , we have seen that the cosine similarity measure and sentence contexts give moreLO is used .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Among the latter , many are translating single-word terms ( Chiao and Zweigenbaum , 2002 ; Dejean et al 2005 ; Prochasson et 1 A stoplist is typically used in order to prevent function words from populating the context vectors .617 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 617625 , Beijing , August 2010 al 2009 while others are tackling the translation of multi-word terms ( Daille and <strong>Morin</strong> , 2005 The type of discourse might as well be of concern in some of the studies dedicated to bilingual terminology mining .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	Our results show that using the log-odds ratio as the association measure allows for significantly better translation spotting than the log-likelihood .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 13) </strong>	While the present work does not investigate all the parameters that could potentially impact results , we believe it constitutes the most complete and systematic comparison made so far with variants of the context-based projection approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	Speech recognition , speech synthesis , and <strong>machine translation</strong> research started about half a century ago .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The feasibility of speech-to-speech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	After groundbreaking work for two decades , corpus-based speech and language processing technology have recently enabled the achievement of speech-to-speech translation that is usable in the real world .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We also found the correlation between the <strong>CWS</strong> F-score and <strong>SMT</strong> <strong>BLEU</strong> score was very weak .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We found these approaches were very effective in improving quality of translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We tested dictionarybased and <strong>CRF</strong>-based approaches and found there was no significant difference between the two in the qualty of the resulting translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	This approach produced a<strong> significant improvement</strong> in translation andachieved the best <strong>BLEU</strong> score of all the <strong>CWS</strong>schemes .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We have published a much more detailed paper ( Zhang et al 2008 ) to describe the relations between <strong>CWS</strong> and <strong>SMT</strong>
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Secondly , we investigated the advantages and disadvantages of various <strong>CWS</strong> approaches , both dictionary-based and <strong>CRF</strong>-based , and built CWSs using these approaches to examine their effect on translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We investigated the effect of <strong>CWS</strong> on <strong>SMT</strong> from two points of view .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Which approach pro 1 A <strong>CWS</strong> competition organized by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical <strong>Machine Translation</strong> , pages 216223 , Columbus , Ohio , <strong>USA</strong> , June 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We grouped all of the <strong>CWS</strong> methods into two classes : the class without out-of-vocabulary ( <strong>OOV</strong> ) recognition and the class with OOV recognition , represented by the dictionary-based CWS and the <strong>CRF</strong>-based CWS , respectively .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Chinese <strong>word segmentation</strong> ( <strong>CWS</strong> ) is a necessary step in Chinese-<strong>English</strong> statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) and its performance has an impact on the results of SMT .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	On the other hand , the dictionarybased approach that does not support <strong>OOV</strong> recognition produced a lower F-score , but with a relatively weak data spareness problem .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In this work , we consider the<strong> underlying latent topic</strong>s of the documents ( Blei et al 2003 Topic modeling has received some use in <strong>SMT</strong> , for instance Bilingual <strong>LSA</strong> adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which uses a bilingual topic model for learning alignment .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Civera and Juan , 2006 a mixture extension of <strong>IBM</strong> model 2 along with a specific dynamicprogramming decoding algorithm were proposed .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	3 Mixture of <strong>HMM</strong> alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Statistical <strong>machine translation</strong> systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time , ignoring document-level information .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In order to resolve this problem , this paper employs a topic model to weaken those noisybilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In particular , three new features are designed to explore various kinds of document-level information in above three kinds of caches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Section 3 presents our<strong> cache-based approach</strong> to documentlevel <strong>SMT</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Evaluation shows the effectiveness of our<strong> cache-based approach</strong> to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We evaluated the<strong> adapted</strong> LM on <strong>SMT</strong> and found that the evaluation metrics are crucial to reflect the actual improvement in performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	On Chinese to <strong>English</strong> speech and text translation the proposed b<strong>LSA</strong> framework successfully reduced <strong>word perplexity</strong> of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We will investigate the incorporation of monolingual documents for potentially better bilingual <strong>LSA</strong> modeling
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	Further , our results offer<strong> <strong>suggestive evidence</strong></strong> that bilingual <strong>word embeddings</strong> act as high-quality semantic features and embody bilingual translation equivalence across languages .6 We report case-insensitive <strong>BLEU</strong>7 With 4-gram BLEU metric from .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	We show good performance on Chinese semantic similarity with bilingual trained embeddings .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	On NIST08 Chinese-<strong>English</strong> translation task , we obtain an improvement of 0.48 <strong>BLEU</strong> from a competitive baseline ( 30.01 BLEU to 30.49 BLEU ) with the Stanford Phrasal MT system .1393 Proceedings of the 2013 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 13931398 , Seattle , Washington , <strong>USA</strong> , 18-21 October 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 21) </strong>	We introduce bilingual <strong>word embeddings</strong> : semantic embeddings associated across two languages in the context of neural <strong>language model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	fem The car goes quickly Figure 1 : Ungrammatical Arabic output of Google Translate for the <strong>English</strong> input The car goes quickly .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 23) </strong>	( 1 ll .
							</p>
							<p>  							</p>
					</div>
</div>
          </div>
        </div>
        <button type="button" id="subcollapsible" class="collapsible">Cluster of Evidences (Trigrams)</button>
        <div class="content">
          <div class="row">
            <div class="column">
                <img src="./evidences_trigrams.png" alt="plot" width="1000" height="1000">
            </div>
            <div class="column" name="trigram_evidences">
						<button type="button" style="background-color:#2AB0E9 !important;" class="collapsible">Sentences in cluster n0</button>
						<div class="content">
							<p> <strong> (Paper 5) </strong>	We show that it is possible to significantly decrease training and test corpus perplexity of the <strong>translation model</strong>s .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	In addition , we perform a rescoring of-Best lists using our maximum entropy model and thereby yield an improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	This additional information could be : Simple context information : information of the words surrounding the<strong> word pair</strong> ; Syntactic information : part-of-speech information , syntactic constituent , sentence Semantic information : disambiguation information ( e.g.from WordNet currentspeech or dialog act .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	In our approach we introduce equivalence classes in order to ignore information not relevant to the translation process .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	Nevertheless , they found that<strong> human mind</strong> is very well capable of deriving dependencies such as morphology , cognates , proper names , spelling variations etc and that this capability was finally at the basis of the better results produced by humans compared to corpus based <strong>machine translation</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We will investigate this in the future .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 4 reports experimental results and Section 5 concludes our work.<strong>English</strong> PeopleEntityCube GeChinese Renlifang GcAbstracting translation as graph mapping Figure 1 : Illustration of entity-relationship graphs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Similarly , we can mine Chinese news articles to obtain the re lationships between t Jli Vi and 1 ' 1 li \ itJli Vi .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Such engine This work was done when the first two authors visited Microsoft Research Asia .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate , Figure 1 demonstrates the query result for Bill Gates , retrieving and visualizing the entity-relationship graph of related people names that frequently co-occur with Bill in <strong>English</strong> corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	While high quality entity translation is essential in <strong>cross-lingual</strong> information access and trans lation , it is non-trivial to achieve , due to the challenge that entity translation , though typically bearing pronunciation similarity , can also be arbitrary , e.g Jackie Chan and fiX : it ( pronounced Cheng Long Existing efforts to address these challenges can be categorized into transliterationand corpusbased approaches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To summarize , we believe that this paper has the<strong> following contribution</strong>s : We abstract entity translation problem as a graph mapping between entity-relationship graphs in two languages.We develop an effective matching algorithm leveraging both pronunciation and cooccurrence similarity .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Qc 2013 Association for Computational Linguistics names in parallel corpora , updating <strong>word segmentation</strong> , <strong>word alignment</strong> and grammar extraction ( Section 3.1 We developed a name-aware MT framework which tightly integrates name tagging and name translation into training and decoding of MT. Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach over a high-quality MT baseline in both overall translation and name translation , especially for formal genres .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We also found the correlation between the <strong>CWS</strong> F-score and <strong>SMT</strong> <strong>BLEU</strong> score was very weak .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Secondly , we investigated the advantages and disadvantages of various <strong>CWS</strong> approaches , both dictionary-based and <strong>CRF</strong>-based , and built CWSs using these approaches to examine their effect on translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We proposed a<strong> new approach</strong> to linear interpolation of translation features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Which approach pro 1 A <strong>CWS</strong> competition organized by the ACL special interest group on Chinese .216 Proceedings of the Third Workshop on Statistical <strong>Machine Translation</strong> , pages 216223 , Columbus , Ohio , <strong>USA</strong> , June 2008 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We grouped all of the <strong>CWS</strong> methods into two classes : the class without out-of-vocabulary ( <strong>OOV</strong> ) recognition and the class with OOV recognition , represented by the dictionary-based CWS and the <strong>CRF</strong>-based CWS , respectively .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	Chinese <strong>word segmentation</strong> ( <strong>CWS</strong> ) is a necessary step in Chinese-<strong>English</strong> statistical <strong>machine translation</strong> ( <strong>SMT</strong> ) and its performance has an impact on the results of SMT .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	On the other hand , the dictionarybased approach that does not support <strong>OOV</strong> recognition produced a lower F-score , but with a relatively weak data spareness problem .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	To obtain the lexical probability conditioned on topic distribution , we first compute the expected count ezn ( e , f of a<strong> word pair</strong> under topic zn : features in the <strong>translation model</strong> , and interpolating them log-linearly with our other features , thus allowezn ( e , f =p ( zn | di ) cj ( e , f 1 ) ing us to discriminatively optimize their weights on di T xj di an arbitrary objective function .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Civera and Juan , 2006 a mixture extension of <strong>IBM</strong> model 2 along with a specific dynamicprogramming decoding algorithm were proposed .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Tiedemann showed that the repetition and consistency are very important when modeling natural language and translation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	First of all , most of parallel corpora lack the annotation of document boundaries ( Tam , 2007 Secondly , although it is easy to incorporate a new feature into the classical log-linear model ( Och , 2003 it is difficult to capture document-level information and model it via some simple features .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In order to resolve this problem , this paper employs a topic model to weaken those noisybilingual phrase pairs by recommending the decoder to choose most likely phrase pairs according to the topic words extracted from the target-side text of similar bilingual document pairs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Section 3 presents our<strong> cache-based approach</strong> to documentlevel <strong>SMT</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Evaluation shows the effectiveness of our<strong> cache-based approach</strong> to document-level translation with the performance improvement of 0.81 in BLUE score over Moses .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	In Section 3 , we present the effect of LM adaptation on <strong>word perplexity</strong> , followed by <strong>SMT</strong> experiments reported in <strong>BLEU</strong> on both speech and text input in Section 3.3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	The key property of the b<strong>LSA</strong> model is that Proceedings of the 45th <strong>Annual Meeting</strong> of the Association of Computational Linguistics , pages 520527 , Prague , Czech <strong>Republic</strong> , June 2007 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	On Chinese to <strong>English</strong> speech and text translation the proposed b<strong>LSA</strong> framework successfully reduced <strong>word perplexity</strong> of the English LM by over 27percent for a unigram LM and up to 13.6 percent for a 4-gram LM .
							</p>
							<p>  							</p>
					</div>
						<button type="button" style="background-color:#D2CA0D !important;" class="collapsible">Sentences in cluster n1</button>
						<div class="content">
							<p> <strong> (Paper 1) </strong>	Specifically , two types of semantic role features are proposed in this paper : a semantic role reordering feature designed to capture the skeletonlevel permutation , and a semantic role deletion fea716 Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 pages 716724 , Beijing , August 2010 ture designed to penalize missing semantic roles in the target sentence .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We believe that by performing a rescoring on translation word graphs we will obtain a more<strong> significant improvement</strong> in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	We have been able to obtain a significant better test corpus perplexity and also a slight improvement in translation quality .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 5) </strong>	Other authors have applied this approach to <strong>language model</strong>ing ( Rosenfeld , 1996 ; Martin et al 1999 ; Peters and Klakow , 1999 A short review of the maximum entropy approach is outlined in Section 3 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	<strong>Experimental</strong> results on the German-<strong>English</strong> Verbmobil Source Language Textmorpho-syntactic AnalysisTransformation f J 1 Global Search : maximize Pr ( e I J e I task are reported .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	We furthermore suggest the use of hierarchical lexicon models .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 6) </strong>	It has been successfully applied to realistic tasks in various national and international research programs .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To highlight the advantage of our proposed approach , we compare our results with commercial machine translators Engkoo3 developed in Microsoft Research Asia and Google Translator4 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Our evaluation results empirically validated the accuracy of our algorithm over real-life datasets , and showed the effectiveness on our proposed perspective
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To achieve this goal , we developed a graph alignment algorithm that iteratively reinforces the matching similarity exploiting relational similarity and then extracts correct matches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	1com430 Proceedings of the 2010 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 430439 , MIT , Massachusetts , <strong>USA</strong> , 9-11 October 2010 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	Section 3 then develops our framework .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 10) </strong>	To illustrate this , an <strong>English</strong> news article mentioning Bill Gates and Melinda Gates evidences a relationship between the two entities , which can be quantified from their co-occurrences in the entire English Web corpus .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Compared to previous methods , the novel contributions of our approach are : 1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Experiments on Chinese-<strong>English</strong> translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and <strong>word alignment</strong> over a high-quality MT baseline1 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	Tightly integrate joint bilingual name tagging into MT training by coordinating tagged604 Proceedings of the 51st <strong>Annual Meeting</strong> of the Association for Computational Linguistics , pages 604614 , Sofia , Bulgaria , August 4-9 2013 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	This need can be addressed in part by <strong>cross-lingual</strong> information access tasks such as entity linking ( McNamee et al 2011 ; Cassidy et al 2012 event extraction ( Hakkani-Tur et al 2007 slot filling ( Snover et al 2011 ) and question answering ( Parton et al 2009 ; Parton and McKeown , 2010 A key bottleneck of highquality cross-lingual information access lies in the performance of <strong>Machine Translation</strong> ( MT Traditional MT approaches focus on the fluency and accuracy of the overall translation but fall short in their ability to translate certain content words including critical information , especially names .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 12) </strong>	We propose a novel Name-aware MT ( NAMT ) approach which can tightly integrate name processing into the training and decoding processes of an end-to-end MT pipeline , and a new name-aware metric to evaluate MT which can assign different weights to different tokens according to their importance values in a document .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	Speech recognition , speech synthesis , and <strong>machine translation</strong> research started about half a century ago .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 14) </strong>	The feasibility of speech-to-speech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We interpolated multiple <strong>translation model</strong>s generated by the <strong>CWS</strong> schemes and found our approaches were very effective in improving the translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We found these approaches were very effective in improving quality of translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We tested dictionarybased and <strong>CRF</strong>-based approaches and found there was no significant difference between the two in the qualty of the resulting translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	This approach produced a<strong> significant improvement</strong> in translation andachieved the best <strong>BLEU</strong> score of all the <strong>CWS</strong>schemes .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We have published a much more detailed paper ( Zhang et al 2008 ) to describe the relations between <strong>CWS</strong> and <strong>SMT</strong>
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	We investigated the effect of <strong>CWS</strong> on <strong>SMT</strong> from two points of view .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 15) </strong>	In this work , we also propose approaches to make use of all the Sighan training data regardless of the specifications .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Qc 2012 Association for Computational Linguisticsdata come from ; and even if we do , subcorpus may not be the most useful notion of domain for better translations .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing <strong>SMT</strong> towards relevant translations , as evidenced by significant performance gains .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in<strong> significant improvement</strong>s of up to 1 <strong>BLEU</strong> and 3 TER on Chinese to <strong>English</strong> translation over a strong baseline .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	In this work , we consider the<strong> underlying latent topic</strong>s of the documents ( Blei et al 2003 Topic modeling has received some use in <strong>SMT</strong> , for instance Bilingual <strong>LSA</strong> adaptation ( Tam et al 2007 and the BiTAM model ( Zhao and Xing , 2006 which uses a bilingual topic model for learning alignment .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 17) </strong>	We induce<strong> unsupervised domain</strong>s from large corpora , and we incorporate soft , probabilistic domain membership into a <strong>translation model</strong> .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	Qc 2007 Association for Computational Linguistics taking advantage of the localization phenomenon of <strong>word alignment</strong> in European languages , and the efficient and exact computation of the E-step and<strong> Viterbi alignment</strong> by using a dynamic-programming approach .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	In ( Zhao and Xing , 2006 three fairly sophisticated bayesian topical <strong>translation model</strong>s , taking <strong>IBM</strong> Model 1 as a baseline model , were presented under the bilingual topic admixture model formalism .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 18) </strong>	3 Mixture of <strong>HMM</strong> alignment models Let us suppose that p ( x has been generated using a T-component mixture of HMM alignment models : T p ( x p ( t p ( x y t = 1 T p ( t p ( x , a y , t ) .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Finally , we conclude this paper in Section 6 We have shown that our<strong> cache-based approach</strong> significantly improves the performance with the help of various caches , such as the dynamic , static and topic caches , although the cache-based approach may introduce some negative impact on<strong>BLEU</strong> scores for certain documents.In the future , we will further explore how to reflect document divergence during training and dynamically adjust cache weights according to different documents.There are many useful components in trainingdocuments , such as named entity , event and coreference .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	Statistical <strong>machine translation</strong> systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time , ignoring document-level information .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	He proposed to employ cache-based language and <strong>translation model</strong>s in a phrase-based <strong>SMT</strong> system for domain909 Proceedings of the 2011 Conference on <strong>Empirical</strong> Methods in Natural <strong>Language Processing</strong> , pages 909919 , Edinburgh , Scotland , UK , July 2731 , 2011 .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 19) </strong>	In particular , three new features are designed to explore various kinds of document-level information in above three kinds of caches .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We evaluated the<strong> adapted</strong> LM on <strong>SMT</strong> and found that the evaluation metrics are crucial to reflect the actual improvement in performance .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We organize the paper as follows : In Section 2 , we introduce the b<strong>LSA</strong> framework including Latent Dirichlet-Tree Allocation ( LDTA Tam and Schultz , 2007 ) as a correlated LSA model , bLSA training and crosslingual LM adaptation .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	Results showed that our approach significantly reduces the <strong>word perplexity</strong> on the target language in both cases using ASR hypotheses and manual transcripts .
							</p>
							<p>  							</p>
							<p> <strong> (Paper 20) </strong>	We will investigate the incorporation of monolingual documents for potentially better bilingual <strong>LSA</strong> modeling
							</p>
							<p>  							</p>
					</div>
</div>
            </div>
          </div>
        </div>
      </div>


      <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
          coll[i].addEventListener("click", function() {
            this.classList.toggle("active");
            var content = this.nextElementSibling;
            if (content.style.display === "block") {
              content.style.display = "none";
            } else {
              content.style.display = "block";
            }
          });
        }
      </script>

    </body>
</html>
